{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file = \"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction/data.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging==23.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (23.2)\n",
      "Requirement already satisfied: setuptools==75.8.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (75.8.0)\n",
      "Requirement already satisfied: wheel in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (0.45.1)\n",
      "Requirement already satisfied: ninja in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (1.11.1.4)\n",
      "Requirement already satisfied: axolotl[deepspeed,flash-attn] in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: bitsandbytes==0.45.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.45.2)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (3.2.0)\n",
      "Requirement already satisfied: autoawq==0.2.7.post3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.2.7.post3)\n",
      "Requirement already satisfied: liger-kernel==0.5.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.5.2)\n",
      "Requirement already satisfied: packaging==23.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (23.2)\n",
      "Requirement already satisfied: peft==0.14.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.14.0)\n",
      "Requirement already satisfied: transformers==4.48.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (4.48.3)\n",
      "Requirement already satisfied: tokenizers>=0.21.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.21.1)\n",
      "Requirement already satisfied: accelerate==1.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.3.0)\n",
      "Requirement already satisfied: datasets==3.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (3.2.0)\n",
      "Requirement already satisfied: trl==0.15.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.15.0)\n",
      "Requirement already satisfied: optimum==1.16.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.16.2)\n",
      "Requirement already satisfied: hf_transfer in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.1.9)\n",
      "Requirement already satisfied: sentencepiece in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.2.0)\n",
      "Requirement already satisfied: gradio==3.50.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (3.50.2)\n",
      "Requirement already satisfied: modal==0.70.5 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.70.5)\n",
      "Requirement already satisfied: pydantic==2.10.6 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2.10.6)\n",
      "Requirement already satisfied: addict in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2.4.0)\n",
      "Requirement already satisfied: fire in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2.32.3)\n",
      "Requirement already satisfied: wandb in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.19.8)\n",
      "Requirement already satisfied: einops in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.8.1)\n",
      "Requirement already satisfied: colorama in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.4.6)\n",
      "Requirement already satisfied: numba in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.61.0)\n",
      "Requirement already satisfied: numpy<=2.0.1,>=1.24.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.26.4)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.4.1)\n",
      "Requirement already satisfied: scipy in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn==1.4.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.4.2)\n",
      "Requirement already satisfied: nvidia-ml-py==12.560.30 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (12.560.30)\n",
      "Requirement already satisfied: art in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (6.4)\n",
      "Requirement already satisfied: tensorboard in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2.19.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.0.1)\n",
      "Requirement already satisfied: s3fs>=2024.5.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2024.9.0)\n",
      "Requirement already satisfied: gcsfs>=2024.5.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2024.9.0.post1)\n",
      "Requirement already satisfied: zstandard==0.22.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.22.0)\n",
      "Requirement already satisfied: fastcore in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.8.0)\n",
      "Requirement already satisfied: lm_eval==0.4.7 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.4.7)\n",
      "Requirement already satisfied: langdetect==1.0.9 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.0.9)\n",
      "Requirement already satisfied: immutabledict==4.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (4.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (4.13.2)\n",
      "Requirement already satisfied: torchao==0.7.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.7.0)\n",
      "Requirement already satisfied: schedulefree==1.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (1.3)\n",
      "Requirement already satisfied: axolotl-contribs-lgpl==0.0.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.0.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (2.6.0+cu118)\n",
      "Requirement already satisfied: xformers==0.0.29.post2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.0.29.post2)\n",
      "Requirement already satisfied: deepspeed==0.16.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.16.1)\n",
      "Requirement already satisfied: deepspeed-kernels in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from axolotl[deepspeed,flash-attn]) (0.0.1.dev1698255861)\n",
      "Collecting flash-attn==2.7.4.post1 (from axolotl[deepspeed,flash-attn])\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from accelerate==1.3.0->axolotl[deepspeed,flash-attn]) (7.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from accelerate==1.3.0->axolotl[deepspeed,flash-attn]) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from accelerate==1.3.0->axolotl[deepspeed,flash-attn]) (0.5.3)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from autoawq==0.2.7.post3->axolotl[deepspeed,flash-attn]) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from datasets==3.2.0->axolotl[deepspeed,flash-attn]) (3.11.14)\n",
      "Requirement already satisfied: hjson in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from deepspeed==0.16.1->axolotl[deepspeed,flash-attn]) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from deepspeed==0.16.1->axolotl[deepspeed,flash-attn]) (1.1.0)\n",
      "Requirement already satisfied: ninja in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from deepspeed==0.16.1->axolotl[deepspeed,flash-attn]) (1.11.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from deepspeed==0.16.1->axolotl[deepspeed,flash-attn]) (9.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from evaluate==0.4.1->axolotl[deepspeed,flash-attn]) (0.18.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.6.1)\n",
      "Requirement already satisfied: httpx in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.28.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (3.1.6)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (3.10.1)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (3.10.16)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (10.4.0)\n",
      "Requirement already satisfied: pydub in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.0.20)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (2.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.34.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gradio==3.50.2->axolotl[deepspeed,flash-attn]) (11.0.3)\n",
      "Requirement already satisfied: six in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from langdetect==1.0.9->axolotl[deepspeed,flash-attn]) (1.17.0)\n",
      "Requirement already satisfied: jsonlines in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (2.10.2)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (2.13.6)\n",
      "Requirement already satisfied: pytablewriter in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.2.1)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (2.5.1)\n",
      "Requirement already satisfied: sqlitedict in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (2.1.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (0.0.11)\n",
      "Requirement already satisfied: word2number in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.1)\n",
      "Requirement already satisfied: more_itertools in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (10.6.0)\n",
      "Requirement already satisfied: certifi in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (2025.1.31)\n",
      "Requirement already satisfied: click>=8.1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (8.1.8)\n",
      "Requirement already satisfied: grpclib==0.4.7 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.4.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<6.0,>=3.19 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (5.29.4)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (13.9.4)\n",
      "Requirement already satisfied: synchronicity~=0.9.8 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.9.11)\n",
      "Requirement already satisfied: toml in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.10.2)\n",
      "Requirement already satisfied: typer>=0.9 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.15.2)\n",
      "Requirement already satisfied: types-certifi in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (2021.10.8.3)\n",
      "Requirement already satisfied: types-toml in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.10.8.20240310)\n",
      "Requirement already satisfied: watchfiles in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from modal==0.70.5->axolotl[deepspeed,flash-attn]) (1.0.4)\n",
      "Requirement already satisfied: coloredlogs in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from optimum==1.16.2->axolotl[deepspeed,flash-attn]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from optimum==1.16.2->axolotl[deepspeed,flash-attn]) (1.13.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pydantic==2.10.6->axolotl[deepspeed,flash-attn]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pydantic==2.10.6->axolotl[deepspeed,flash-attn]) (2.27.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from scikit-learn==1.4.2->axolotl[deepspeed,flash-attn]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from scikit-learn==1.4.2->axolotl[deepspeed,flash-attn]) (3.6.0)\n",
      "Requirement already satisfied: networkx in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from torch==2.6.0->axolotl[deepspeed,flash-attn]) (11.8.86)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from transformers==4.48.3->axolotl[deepspeed,flash-attn]) (2024.11.6)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from grpclib==0.4.7->modal==0.70.5->axolotl[deepspeed,flash-attn]) (4.2.0)\n",
      "Requirement already satisfied: multidict in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from grpclib==0.4.7->modal==0.70.5->axolotl[deepspeed,flash-attn]) (6.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from sympy->optimum==1.16.2->axolotl[deepspeed,flash-attn]) (1.3.0)\n",
      "Requirement already satisfied: decorator>4.1.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (5.2.1)\n",
      "Requirement already satisfied: google-auth>=1.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.2.1)\n",
      "Requirement already satisfied: google-cloud-storage in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from requests->axolotl[deepspeed,flash-attn]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from requests->axolotl[deepspeed,flash-attn]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from requests->axolotl[deepspeed,flash-attn]) (2.3.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.21.1)\n",
      "Requirement already satisfied: cmake>=3.24 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from deepspeed-kernels->axolotl[deepspeed,flash-attn]) (3.31.6)\n",
      "Requirement already satisfied: termcolor in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from fire->axolotl[deepspeed,flash-attn]) (2.5.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from numba->axolotl[deepspeed,flash-attn]) (0.44.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (2.2.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from tensorboard->axolotl[deepspeed,flash-attn]) (3.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from wandb->axolotl[deepspeed,flash-attn]) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from wandb->axolotl[deepspeed,flash-attn]) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from wandb->axolotl[deepspeed,flash-attn]) (4.3.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from wandb->axolotl[deepspeed,flash-attn]) (2.24.1)\n",
      "Requirement already satisfied: setproctitle in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from wandb->axolotl[deepspeed,flash-attn]) (1.3.5)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.2,>=1.37.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.37.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (1.18.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->axolotl[deepspeed,flash-attn]) (4.0.12)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (4.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pandas->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pandas->datasets==3.2.0->axolotl[deepspeed,flash-attn]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from rich>=12.0.0->modal==0.70.5->axolotl[deepspeed,flash-attn]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from rich>=12.0.0->modal==0.70.5->axolotl[deepspeed,flash-attn]) (2.19.1)\n",
      "Requirement already satisfied: nltk in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (3.9.1)\n",
      "Requirement already satisfied: portalocker in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (0.9.0)\n",
      "Requirement already satisfied: lxml in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (5.3.1)\n",
      "Requirement already satisfied: sigtools>=4.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from synchronicity~=0.9.8->modal==0.70.5->axolotl[deepspeed,flash-attn]) (4.0.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from typer>=0.9->modal==0.70.5->axolotl[deepspeed,flash-attn]) (1.5.4)\n",
      "Requirement already satisfied: h11>=0.8 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from coloredlogs->optimum==1.16.2->axolotl[deepspeed,flash-attn]) (10.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from fastapi->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.46.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.0.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.24.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.7.1rc1)\n",
      "Requirement already satisfied: anyio in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from httpx->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from httpx->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.0.7)\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.1.4)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (3.2.3)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (0.1.7)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (1.3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from anyio->httpx->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from anyio->httpx->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->axolotl[deepspeed,flash-attn]) (5.0.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.69.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (1.26.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from h2<5,>=3.1.0->grpclib==0.4.7->modal==0.70.5->axolotl[deepspeed,flash-attn]) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from h2<5,>=3.1.0->grpclib==0.4.7->modal==0.70.5->axolotl[deepspeed,flash-attn]) (4.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed,flash-attn]) (0.23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal==0.70.5->axolotl[deepspeed,flash-attn]) (0.1.2)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.7->axolotl[deepspeed,flash-attn]) (5.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/shuof/.conda/envs/llm2/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2024.5.0->axolotl[deepspeed,flash-attn]) (3.2.2)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=189909262 sha256=52226aec6dcb80b89ccb16e724a42eaaa90ecc08726e7fcd0ec0db38352c4744\n",
      "  Stored in directory: /home/shuof/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U packaging==23.2 setuptools==75.8.0 wheel ninja\n",
    "!pip3 install --no-build-isolation axolotl[flash-attn,deepspeed]\n",
    "\n",
    "# Download example axolotl configs, deepspeed configs\n",
    "#axolotl fetch examples\n",
    "#axolotl fetch deepspeed_configs  # OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data format transform. [link](https://axolotl-ai-cloud.github.io/axolotl/docs/dataset-formats/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已轉換完成，結果儲存在 axolotl_format_llama.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction/data/train.json\"  # paper_id, abstract, introduction\n",
    "output_file = \"axolotl_format_llama.jsonl\"\n",
    "\n",
    "# instruction\n",
    "instruction_text = \"You are a professional researcher writing for a top-tier academic conference. Based on the introduction, write a concise and formal abstract that clearly presents the motivation, methodology, and contributions of the work.\\n\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        introduction = data.get(\"introduction\", \"\").strip()\n",
    "        abstract = data.get(\"abstract\", \"\").strip()\n",
    "\n",
    "        if introduction and abstract:\n",
    "            new_data = {\n",
    "                \"instruction\": \"{instruction_text}\\n\\n\",\n",
    "                \"input\": f\"{introduction}\\n\",\n",
    "                \"output\": abstract\n",
    "            }\n",
    "            json.dump(new_data, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ 已轉換完成，結果儲存在 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已轉換完成，結果儲存在 test_axolotl_format_llama.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction/data/test.json\"  # paper_id, introduction\n",
    "output_file = \"test_axolotl_format_llama.json\"\n",
    "\n",
    "# instruction\n",
    "instruction_text = \"You are a professional researcher writing for a top-tier academic conference. Based on the introduction, write a concise and formal abstract that clearly presents the motivation, methodology, and contributions of the work.\\n\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        introduction = data.get(\"introduction\", \"\").strip()\n",
    "\n",
    "        if introduction:\n",
    "            new_data = {\n",
    "                \"instruction\": \"{instruction_text}\\n\\n\",\n",
    "                \"input\": f\"{introduction}\\n\",\n",
    "            }\n",
    "            json.dump(new_data, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ 已轉換完成，結果儲存在 {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `2`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:16,630] [INFO] [datasets.<module>:54] [PID:770857] PyTorch version 2.6.0+cu118 available.\n",
      "[2025-04-08 09:56:16,630] [INFO] [datasets.<module>:54] [PID:770856] PyTorch version 2.6.0+cu118 available.\n",
      "[2025-04-08 09:56:17,209] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-08 09:56:17,222] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-08 09:56:17,258] [INFO] [root.spawn:60] [PID:770856] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -c /tmp/tmpe8_5y599/test.c -o /tmp/tmpe8_5y599/test.o\n",
      "[2025-04-08 09:56:17,269] [INFO] [root.spawn:60] [PID:770856] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat /tmp/tmpe8_5y599/test.o -laio -o /tmp/tmpe8_5y599/a.out\n",
      "[2025-04-08 09:56:17,270] [INFO] [root.spawn:60] [PID:770857] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -c /tmp/tmp5r2dvbbl/test.c -o /tmp/tmp5r2dvbbl/test.o\n",
      "[2025-04-08 09:56:17,280] [INFO] [root.spawn:60] [PID:770857] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat /tmp/tmp5r2dvbbl/test.o -laio -o /tmp/tmp5r2dvbbl/a.out\n",
      "[2025-04-08 09:56:17,700] [INFO] [root.spawn:60] [PID:770856] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -c /tmp/tmpw08viosa/test.c -o /tmp/tmpw08viosa/test.o\n",
      "[2025-04-08 09:56:17,710] [INFO] [root.spawn:60] [PID:770857] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -O2 -isystem /home/shuof/.conda/envs/llm2/include -fPIC -c /tmp/tmph49aar7z/test.c -o /tmp/tmph49aar7z/test.o\n",
      "[2025-04-08 09:56:17,711] [INFO] [root.spawn:60] [PID:770856] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat /tmp/tmpw08viosa/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpw08viosa/a.out\n",
      "[2025-04-08 09:56:17,720] [INFO] [root.spawn:60] [PID:770857] gcc -pthread -B /home/shuof/.conda/envs/llm2/compiler_compat /tmp/tmph49aar7z/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmph49aar7z/a.out\n",
      "\u001b[33m[2025-04-08 09:56:18,172] [WARNING] [axolotl.utils.config.models.input.hint_trust_remote_code:458] [PID:770856] [RANK:0] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:18,177] [WARNING] [axolotl.utils.config.models.input.hint_trust_remote_code:458] [PID:770857] [RANK:1] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n",
      "[2025-04-08 09:56:18,229] [DEBUG] [axolotl.resolve_dtype:67] [PID:770856] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2025-04-08 09:56:18,234] [DEBUG] [axolotl.resolve_dtype:67] [PID:770857] [RANK:1] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2025-04-08 09:56:18,422] [INFO] [axolotl.normalize_config:236] [PID:770856] [RANK:0] cuda memory usage baseline: 0.000GB (+0.336GB misc)\u001b[39m\n",
      "[2025-04-08 09:56:18,431] [INFO] [axolotl.normalize_config:236] [PID:770857] [RANK:1] cuda memory usage baseline: 0.000GB (+0.409GB misc)\u001b[39m\n",
      "\n",
      "     #@@ #@@      @@# @@#\n",
      "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
      "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
      "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
      "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
      "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
      "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
      "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
      "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
      "    @@@@  @@@@@@@@@@@@@@@@\n",
      "\n",
      "\u001b[33m[2025-04-08 09:56:18,456] [WARNING] [axolotl.cli.checks.check_user_token:47] [PID:770856] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:18,461] [WARNING] [axolotl.cli.checks.check_user_token:47] [PID:770857] [RANK:1] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [DEBUG] [axolotl.load_tokenizer:298] [PID:770856] [RANK:0] EOS: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [DEBUG] [axolotl.load_tokenizer:299] [PID:770856] [RANK:0] BOS: None / None\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [DEBUG] [axolotl.load_tokenizer:300] [PID:770856] [RANK:0] PAD: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [DEBUG] [axolotl.load_tokenizer:301] [PID:770856] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [INFO] [axolotl.load_tokenizer:315] [PID:770856] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:770856] [RANK:0] Unable to find prepared dataset in last_run_prepared/5606787b0f411587f91aaeb73be5d281\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:770856] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:19,050] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:770856] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2025-04-08 09:56:19,050] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:770856] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "[2025-04-08 09:56:19,055] [DEBUG] [axolotl.load_tokenizer:298] [PID:770857] [RANK:1] EOS: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:19,055] [DEBUG] [axolotl.load_tokenizer:299] [PID:770857] [RANK:1] BOS: None / None\u001b[39m\n",
      "[2025-04-08 09:56:19,055] [DEBUG] [axolotl.load_tokenizer:300] [PID:770857] [RANK:1] PAD: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:19,055] [DEBUG] [axolotl.load_tokenizer:301] [PID:770857] [RANK:1] UNK: None / None\u001b[39m\n",
      "[2025-04-08 09:56:19,055] [INFO] [axolotl.load_tokenizer:315] [PID:770857] [RANK:1] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W408 09:56:19.123387369 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:19,735] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:770856] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "[2025-04-08 09:56:19,824] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:176] [PID:770856] [RANK:0] min_input_len: 341\u001b[39m\n",
      "[2025-04-08 09:56:19,825] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:178] [PID:770856] [RANK:0] max_input_len: 2478\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:20,126] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:206] [PID:770856] [RANK:0] Dropped 34 long samples from dataset\u001b[39m\n",
      "[2025-04-08 09:56:20,500] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:332] [PID:770856] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/5606787b0f411587f91aaeb73be5d281\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 374/374 [00:00<00:00, 5372.34 examples/s]\n",
      "[rank0]:[W408 09:56:20.638384669 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:20,906] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:770857] [RANK:1] Unable to find prepared dataset in last_run_prepared/5606787b0f411587f91aaeb73be5d281\u001b[39m\n",
      "[2025-04-08 09:56:20,906] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:770857] [RANK:1] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:20,906] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:770857] [RANK:1] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2025-04-08 09:56:20,906] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:770857] [RANK:1] No seed provided, using default seed of 42\u001b[39m\n",
      "[2025-04-08 09:56:20,911] [DEBUG] [axolotl.calculate_total_num_steps:403] [PID:770856] [RANK:0] total_num_tokens: 519_681\u001b[39m\n",
      "[2025-04-08 09:56:20,914] [DEBUG] [axolotl.calculate_total_num_steps:421] [PID:770856] [RANK:0] `total_supervised_tokens: 115_096`\u001b[39m\n",
      "[2025-04-08 09:56:21,607] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:770857] [RANK:1] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "[2025-04-08 09:56:21,698] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:176] [PID:770857] [RANK:1] min_input_len: 341\u001b[39m\n",
      "[2025-04-08 09:56:21,698] [DEBUG] [axolotl.utils.data.utils.drop_long_seq_in_dataset:178] [PID:770857] [RANK:1] max_input_len: 2478\u001b[39m\n",
      "\u001b[33m[2025-04-08 09:56:21,983] [WARNING] [axolotl.utils.data.utils.drop_long_seq_in_dataset:206] [PID:770857] [RANK:1] Dropped 34 long samples from dataset\u001b[39m\n",
      "[2025-04-08 09:56:25,504] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:203] [PID:770856] [RANK:0] gather_len_batches: [355, 356]\u001b[39m\n",
      "[2025-04-08 09:56:25,505] [DEBUG] [axolotl.calculate_total_num_steps:473] [PID:770856] [RANK:0] data_loader_len: 44\u001b[39m\n",
      "[2025-04-08 09:56:25,507] [INFO] [axolotl.calc_sample_packing_eff_est:479] [PID:770856] [RANK:0] sample_packing_eff_est across ranks: [0.7010892629623413, 0.7023019790649414]\u001b[39m\n",
      "[2025-04-08 09:56:25,507] [DEBUG] [axolotl.calculate_total_num_steps:491] [PID:770856] [RANK:0] sample_packing_eff_est: 0.71\u001b[39m\n",
      "[2025-04-08 09:56:25,508] [DEBUG] [axolotl.calculate_total_num_steps:499] [PID:770856] [RANK:0] total_num_steps: 176\u001b[39m\n",
      "[2025-04-08 09:56:25,508] [DEBUG] [axolotl.train.train:47] [PID:770856] [RANK:0] loading tokenizer... Qwen/Qwen2-7B\u001b[39m\n",
      "[2025-04-08 09:56:26,042] [DEBUG] [axolotl.load_tokenizer:298] [PID:770857] [RANK:1] EOS: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:26,042] [DEBUG] [axolotl.load_tokenizer:299] [PID:770857] [RANK:1] BOS: None / None\u001b[39m\n",
      "[2025-04-08 09:56:26,042] [DEBUG] [axolotl.load_tokenizer:300] [PID:770857] [RANK:1] PAD: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:26,043] [DEBUG] [axolotl.load_tokenizer:301] [PID:770857] [RANK:1] UNK: None / None\u001b[39m\n",
      "[2025-04-08 09:56:26,043] [INFO] [axolotl.load_tokenizer:315] [PID:770857] [RANK:1] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [DEBUG] [axolotl.load_tokenizer:298] [PID:770856] [RANK:0] EOS: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [DEBUG] [axolotl.load_tokenizer:299] [PID:770856] [RANK:0] BOS: None / None\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [DEBUG] [axolotl.load_tokenizer:300] [PID:770856] [RANK:0] PAD: 151643 / <|endoftext|>\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [DEBUG] [axolotl.load_tokenizer:301] [PID:770856] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [INFO] [axolotl.load_tokenizer:315] [PID:770856] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2025-04-08 09:56:26,131] [DEBUG] [axolotl.train.train:82] [PID:770856] [RANK:0] loading model and peft_config...\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:29,780] [INFO] [axolotl.load_lora:1311] [PID:770857] [RANK:1] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "[2025-04-08 09:56:30,226] [INFO] [axolotl.load_model:1182] [PID:770857] [RANK:1] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.29s/it]When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/axolotl/core/trainers/base.py:177: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*_args, **kwargs)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:31,594] [INFO] [axolotl.load_lora:1311] [PID:770856] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
      "trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n",
      "[2025-04-08 09:56:32,187] [INFO] [axolotl.load_model:1182] [PID:770856] [RANK:0] cuda memory usage after adapters: 0.000GB ()\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/axolotl/core/trainers/base.py:177: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*_args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 09:56:32,748] [INFO] [axolotl.train.train:134] [PID:770856] [RANK:0] Pre-saving adapter config to ./model-out\u001b[39m\n",
      "[2025-04-08 09:56:32,920] [INFO] [axolotl.train.train:173] [PID:770856] [RANK:0] Starting trainer...\u001b[39m\n",
      "[2025-04-08 09:56:33,110] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:203] [PID:770856] [RANK:0] gather_len_batches: [357, 357]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/176 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  1%|          | 1/176 [00:23<1:08:31, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9219, 'grad_norm': 0.6243578195571899, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "[2025-04-08 09:57:17,405] [INFO] [axolotl.callbacks.on_step_end:127] [PID:770857] [RANK:1] cuda memory usage while training: 1.180GB (+7.782GB cache, +0.867GB misc)\u001b[39m\n",
      "[2025-04-08 09:57:17,405] [INFO] [axolotl.callbacks.on_step_end:127] [PID:770856] [RANK:0] cuda memory usage while training: 0.118GB (+8.943GB cache, +0.794GB misc)\u001b[39m\n",
      "{'loss': 1.791, 'grad_norm': 0.5720100402832031, 'learning_rate': 4e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/176 [00:57<53:30, 18.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8595, 'grad_norm': 0.5460270047187805, 'learning_rate': 6e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/176 [01:14<51:24, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8087, 'grad_norm': 0.5623361468315125, 'learning_rate': 8e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/176 [01:31<50:05, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.843, 'grad_norm': 0.5271819829940796, 'learning_rate': 0.0001, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/176 [01:48<49:10, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8336, 'grad_norm': 0.5260074734687805, 'learning_rate': 0.00012, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/176 [02:05<48:53, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6402, 'grad_norm': 0.5690122842788696, 'learning_rate': 0.00014, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 8/176 [02:22<48:16, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5574, 'grad_norm': 0.6189325451850891, 'learning_rate': 0.00016, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 9/176 [02:39<47:44, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6149, 'grad_norm': 0.5302111506462097, 'learning_rate': 0.00018, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/176 [02:57<47:37, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5548, 'grad_norm': 0.5901342034339905, 'learning_rate': 0.0002, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 11/176 [03:14<47:11, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3908, 'grad_norm': 0.6847280859947205, 'learning_rate': 0.00019998209226697376, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 12/176 [03:31<46:49, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3896, 'grad_norm': 0.707537829875946, 'learning_rate': 0.00019992837548163316, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 13/176 [03:48<46:30, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6465, 'grad_norm': 0.6909599304199219, 'learning_rate': 0.00019983886888289514, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 14/176 [04:05<46:27, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5604, 'grad_norm': 0.7220121026039124, 'learning_rate': 0.00019971360452796522, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 15/176 [04:22<46:04, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.528, 'grad_norm': 0.6443451046943665, 'learning_rate': 0.0001995526272808559, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 16/176 [04:39<45:43, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6162, 'grad_norm': 0.6108537912368774, 'learning_rate': 0.0001993559947963185, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 17/176 [04:57<45:32, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5802, 'grad_norm': 0.5161197781562805, 'learning_rate': 0.00019912377749919374, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 18/176 [05:14<45:19, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6339, 'grad_norm': 0.5501196384429932, 'learning_rate': 0.00019885605855918885, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 19/176 [05:31<44:54, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6029, 'grad_norm': 0.5440030097961426, 'learning_rate': 0.00019855293386108992, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 20/176 [05:48<44:33, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5966, 'grad_norm': 0.49905410408973694, 'learning_rate': 0.00019821451197042026, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 21/176 [06:06<44:30, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4534, 'grad_norm': 0.4772231876850128, 'learning_rate': 0.00019784091409455728, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 22/176 [06:23<44:07, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4738, 'grad_norm': 0.5136944651603699, 'learning_rate': 0.00019743227403932134, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 23/176 [06:40<43:44, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6948, 'grad_norm': 0.5433662533760071, 'learning_rate': 0.00019698873816105273, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 24/176 [06:57<43:35, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5033, 'grad_norm': 0.508597731590271, 'learning_rate': 0.00019651046531419332, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 25/176 [07:14<43:21, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6157, 'grad_norm': 0.5576367378234863, 'learning_rate': 0.0001959976267943923, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 26/176 [07:31<42:58, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4821, 'grad_norm': 0.543472170829773, 'learning_rate': 0.0001954504062771555, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 27/176 [07:49<42:37, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3572, 'grad_norm': 0.5598989129066467, 'learning_rate': 0.00019486899975206166, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 28/176 [08:06<42:27, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4501, 'grad_norm': 0.5180805325508118, 'learning_rate': 0.00019425361545256727, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 29/176 [08:23<42:04, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5013, 'grad_norm': 0.5772169828414917, 'learning_rate': 0.00019360447378142728, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 30/176 [08:40<41:43, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5846, 'grad_norm': 0.6073312759399414, 'learning_rate': 0.00019292180723175654, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 31/176 [08:57<41:33, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5487, 'grad_norm': 0.5270331501960754, 'learning_rate': 0.00019220586030376134, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 32/176 [09:15<41:18, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5864, 'grad_norm': 0.5399494171142578, 'learning_rate': 0.00019145688941717075, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 33/176 [09:32<40:56, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4491, 'grad_norm': 0.5495076179504395, 'learning_rate': 0.00019067516281939825, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 34/176 [09:49<40:35, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6126, 'grad_norm': 0.5167589783668518, 'learning_rate': 0.00018986096048946824, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 35/176 [10:06<40:33, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4121, 'grad_norm': 0.5515856146812439, 'learning_rate': 0.00018901457403773967, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 36/176 [10:23<40:09, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4096, 'grad_norm': 0.5689404010772705, 'learning_rate': 0.00018813630660146488, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 37/176 [10:41<39:47, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6492, 'grad_norm': 0.5214044451713562, 'learning_rate': 0.0001872264727362194, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 38/176 [10:58<39:27, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4002, 'grad_norm': 0.4831809401512146, 'learning_rate': 0.00018628539830324229, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 39/176 [11:15<39:20, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5102, 'grad_norm': 0.5277357697486877, 'learning_rate': 0.00018531342035272766, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 40/176 [11:32<38:59, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3961, 'grad_norm': 0.5158504843711853, 'learning_rate': 0.00018431088700310844, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 41/176 [11:49<38:37, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4557, 'grad_norm': 0.4905198812484741, 'learning_rate': 0.00018327815731637612, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 42/176 [12:07<38:32, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4012, 'grad_norm': 0.5000771284103394, 'learning_rate': 0.00018221560116948103, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 43/176 [12:24<38:07, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4826, 'grad_norm': 0.5324798822402954, 'learning_rate': 0.00018112359912185924, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 44/176 [12:41<37:47, 17.18s/it]/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5077, 'grad_norm': 0.5111132264137268, 'learning_rate': 0.00018000254227913348, 'epoch': 0.99}\n",
      "[2025-04-08 10:09:35,801] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:89] [PID:770856] Saving model to ./model-out/checkpoint-44/pytorch_model_fsdp.bin\n",
      "[2025-04-08 10:09:38,036] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:91] [PID:770856] Model saved to ./model-out/checkpoint-44/pytorch_model_fsdp.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 10:09:38,614] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:193] [PID:770856] Saving Optimizer state to ./model-out/checkpoint-44/optimizer.bin\n",
      "[2025-04-08 10:09:44,444] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:195] [PID:770856] Optimizer state saved in ./model-out/checkpoint-44/optimizer.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 45/176 [13:15<48:52, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5951, 'grad_norm': 0.7011626362800598, 'learning_rate': 0.0001788528321530366, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 46/176 [13:33<45:14, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3095, 'grad_norm': 0.4819463789463043, 'learning_rate': 0.00017767488051760857, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 47/176 [13:50<42:22, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3645, 'grad_norm': 0.5259706377983093, 'learning_rate': 0.00017646910926171747, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 48/176 [14:07<40:19, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3705, 'grad_norm': 0.47591468691825867, 'learning_rate': 0.00017523595023795813, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 49/176 [14:24<38:49, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3485, 'grad_norm': 0.5108562707901001, 'learning_rate': 0.0001739758451079821, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 50/176 [14:41<37:53, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2013, 'grad_norm': 0.5345377326011658, 'learning_rate': 0.00017268924518431438, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 51/176 [14:58<36:59, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3574, 'grad_norm': 0.5435337424278259, 'learning_rate': 0.0001713766112687139, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 52/176 [15:15<36:16, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3797, 'grad_norm': 0.6755226254463196, 'learning_rate': 0.0001700384134871351, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 53/176 [15:33<35:56, 17.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1606, 'grad_norm': 0.6169378161430359, 'learning_rate': 0.00016867513112135013, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 54/176 [15:50<35:21, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0944, 'grad_norm': 0.6175594925880432, 'learning_rate': 0.0001672872524372919, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 55/176 [16:07<34:53, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2323, 'grad_norm': 0.6302008628845215, 'learning_rate': 0.00016587527451017938, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 56/176 [16:24<34:28, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2547, 'grad_norm': 0.7022792100906372, 'learning_rate': 0.0001644397030464877, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 57/176 [16:42<34:19, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1877, 'grad_norm': 0.7651361227035522, 'learning_rate': 0.00016298105220282713, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 58/176 [16:59<33:54, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2222, 'grad_norm': 0.7665628790855408, 'learning_rate': 0.00016149984440179537, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 59/176 [17:16<33:32, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2624, 'grad_norm': 0.7115821242332458, 'learning_rate': 0.00015999661014486956, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 60/176 [17:33<33:20, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1224, 'grad_norm': 0.7139832973480225, 'learning_rate': 0.0001584718878224047, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 61/176 [17:50<32:58, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2162, 'grad_norm': 0.7407925128936768, 'learning_rate': 0.00015692622352080662, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 62/176 [18:07<32:38, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1362, 'grad_norm': 0.7548317909240723, 'learning_rate': 0.00015536017082694846, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 63/176 [18:24<32:19, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2084, 'grad_norm': 0.7431498765945435, 'learning_rate': 0.00015377429062990122, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 64/176 [18:42<32:11, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2438, 'grad_norm': 0.7927469611167908, 'learning_rate': 0.00015216915092004847, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 65/176 [18:59<31:49, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2051, 'grad_norm': 0.8397870659828186, 'learning_rate': 0.0001505453265856581, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 66/176 [19:16<31:28, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2363, 'grad_norm': 0.7779252529144287, 'learning_rate': 0.00014890339920698334, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 67/176 [19:33<31:16, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1471, 'grad_norm': 0.8191755414009094, 'learning_rate': 0.0001472439568479671, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 68/176 [19:51<31:02, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2463, 'grad_norm': 0.9698572754859924, 'learning_rate': 0.00014556759384562416, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 69/176 [20:08<30:41, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0804, 'grad_norm': 0.8714132308959961, 'learning_rate': 0.00014387491059717652, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 70/176 [20:25<30:20, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1152, 'grad_norm': 0.9071589112281799, 'learning_rate': 0.0001421665133450184, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 71/176 [20:42<30:13, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1651, 'grad_norm': 0.9199504852294922, 'learning_rate': 0.0001404430139595877, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 72/176 [21:00<29:51, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2048, 'grad_norm': 0.8832140564918518, 'learning_rate': 0.00013870502972022173, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 73/176 [21:17<29:29, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2735, 'grad_norm': 0.9679939150810242, 'learning_rate': 0.0001369531830940757, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 74/176 [21:34<29:16, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1141, 'grad_norm': 0.8863211870193481, 'learning_rate': 0.0001351881015131833, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 75/176 [21:51<29:00, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.121, 'grad_norm': 0.8902248740196228, 'learning_rate': 0.000133410417149739, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 76/176 [22:08<28:39, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0412, 'grad_norm': 0.8024545311927795, 'learning_rate': 0.0001316207666896824, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 77/176 [22:25<28:18, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2175, 'grad_norm': 0.9432669281959534, 'learning_rate': 0.00012981979110466654, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 78/176 [22:43<28:09, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.326, 'grad_norm': 0.8467408418655396, 'learning_rate': 0.00012800813542249072, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 79/176 [23:00<27:49, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0764, 'grad_norm': 0.7702096700668335, 'learning_rate': 0.0001261864484960807, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 80/176 [23:17<27:28, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1659, 'grad_norm': 0.882672131061554, 'learning_rate': 0.0001243553827710992, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 81/176 [23:34<27:16, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3029, 'grad_norm': 0.8768044114112854, 'learning_rate': 0.00012251559405226941, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 82/176 [23:52<27:00, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2644, 'grad_norm': 0.8983853459358215, 'learning_rate': 0.00012066774126849529, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 83/176 [24:09<26:39, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2001, 'grad_norm': 0.9084727764129639, 'learning_rate': 0.00011881248623686338, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 84/176 [24:26<26:18, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1589, 'grad_norm': 0.8341889381408691, 'learning_rate': 0.00011695049342560968, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 85/176 [24:43<26:08, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0765, 'grad_norm': 0.8844025731086731, 'learning_rate': 0.00011508242971613741, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 86/176 [25:00<25:47, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2127, 'grad_norm': 0.9135432243347168, 'learning_rate': 0.00011320896416417026, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 87/176 [25:17<25:27, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2697, 'grad_norm': 0.921433687210083, 'learning_rate': 0.000111330767760127, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 88/176 [25:35<25:13, 17.20s/it]/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1383, 'grad_norm': 0.8645327091217041, 'learning_rate': 0.00010944851318880314, 'epoch': 1.97}\n",
      "[2025-04-08 10:22:28,725] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:89] [PID:770856] Saving model to ./model-out/checkpoint-88/pytorch_model_fsdp.bin\n",
      "[2025-04-08 10:22:31,291] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:91] [PID:770856] Model saved to ./model-out/checkpoint-88/pytorch_model_fsdp.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 10:22:31,880] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:193] [PID:770856] Saving Optimizer state to ./model-out/checkpoint-88/optimizer.bin\n",
      "[2025-04-08 10:22:38,917] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:195] [PID:770856] Optimizer state saved in ./model-out/checkpoint-88/optimizer.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 89/176 [26:19<36:30, 25.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0633, 'grad_norm': 0.7762306332588196, 'learning_rate': 0.00010756287458844569, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 90/176 [26:27<28:55, 20.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3978, 'grad_norm': 1.3143119812011719, 'learning_rate': 0.00010567452730930743, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 91/176 [26:44<27:12, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9269, 'grad_norm': 0.8204464912414551, 'learning_rate': 0.00010378414767176705, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 92/176 [27:01<26:01, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8492, 'grad_norm': 0.955324649810791, 'learning_rate': 0.0001018924127241019, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 93/176 [27:18<25:08, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8192, 'grad_norm': 0.8951396346092224, 'learning_rate': 0.0001, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 94/176 [27:35<24:23, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9183, 'grad_norm': 0.8839482069015503, 'learning_rate': 9.810758727589813e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 95/176 [27:53<23:46, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7947, 'grad_norm': 0.9238813519477844, 'learning_rate': 9.621585232823298e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 96/176 [28:10<23:25, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6089, 'grad_norm': 1.1311562061309814, 'learning_rate': 9.432547269069261e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 97/176 [28:27<22:57, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8247, 'grad_norm': 1.772520899772644, 'learning_rate': 9.243712541155436e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 98/176 [28:44<22:32, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7603, 'grad_norm': 1.4388368129730225, 'learning_rate': 9.055148681119688e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 99/176 [29:01<22:13, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.847, 'grad_norm': 1.2991931438446045, 'learning_rate': 8.866923223987302e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 100/176 [29:19<21:55, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.685, 'grad_norm': 0.9900791645050049, 'learning_rate': 8.679103583582979e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 101/176 [29:36<21:34, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7307, 'grad_norm': 1.2003250122070312, 'learning_rate': 8.491757028386263e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 102/176 [29:53<21:14, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7238, 'grad_norm': 1.1682456731796265, 'learning_rate': 8.304950657439033e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 103/176 [30:10<20:59, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7505, 'grad_norm': 1.064987301826477, 'learning_rate': 8.118751376313664e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 104/176 [30:28<20:39, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8096, 'grad_norm': 1.1799596548080444, 'learning_rate': 7.93322587315047e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 105/176 [30:45<20:20, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7427, 'grad_norm': 1.055729866027832, 'learning_rate': 7.74844059477306e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 106/176 [31:02<20:05, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7948, 'grad_norm': 1.1052440404891968, 'learning_rate': 7.564461722890081e-05, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 107/176 [31:19<19:49, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7598, 'grad_norm': 1.072085976600647, 'learning_rate': 7.381355150391933e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 108/176 [31:36<19:29, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8425, 'grad_norm': 1.1153829097747803, 'learning_rate': 7.19918645775093e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 109/176 [31:53<19:09, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6877, 'grad_norm': 1.120301365852356, 'learning_rate': 7.018020889533348e-05, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 110/176 [32:11<18:54, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8208, 'grad_norm': 1.310806393623352, 'learning_rate': 6.83792333103176e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 111/176 [32:28<18:38, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7052, 'grad_norm': 1.1752434968948364, 'learning_rate': 6.658958285026102e-05, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 112/176 [32:45<18:18, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7142, 'grad_norm': 1.2394925355911255, 'learning_rate': 6.48118984868167e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 113/176 [33:02<17:59, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7428, 'grad_norm': 1.193556308746338, 'learning_rate': 6.304681690592431e-05, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 114/176 [33:19<17:48, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7508, 'grad_norm': 1.2150166034698486, 'learning_rate': 6.129497027977829e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 115/176 [33:37<17:28, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8587, 'grad_norm': 1.2972508668899536, 'learning_rate': 5.955698604041231e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 116/176 [33:54<17:08, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9356, 'grad_norm': 1.42965829372406, 'learning_rate': 5.7833486654981606e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 117/176 [34:11<16:53, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6865, 'grad_norm': 1.276843547821045, 'learning_rate': 5.6125089402823485e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 118/176 [34:28<16:37, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6664, 'grad_norm': 1.3249151706695557, 'learning_rate': 5.443240615437586e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 119/176 [34:45<16:17, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7402, 'grad_norm': 1.3040919303894043, 'learning_rate': 5.275604315203293e-05, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 120/176 [35:02<15:58, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8541, 'grad_norm': 1.340408205986023, 'learning_rate': 5.109660079301668e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 121/176 [35:20<15:47, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7058, 'grad_norm': 1.2787551879882812, 'learning_rate': 4.945467341434195e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 122/176 [35:37<15:26, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0309, 'grad_norm': 1.5337650775909424, 'learning_rate': 4.783084907995156e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 123/176 [35:54<15:07, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7422, 'grad_norm': 1.3824148178100586, 'learning_rate': 4.622570937009879e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 124/176 [36:11<14:52, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0079, 'grad_norm': 1.4398808479309082, 'learning_rate': 4.4639829173051554e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 125/176 [36:28<14:36, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8276, 'grad_norm': 1.2737700939178467, 'learning_rate': 4.307377647919343e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 126/176 [36:45<14:17, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7427, 'grad_norm': 1.2956236600875854, 'learning_rate': 4.152811217759529e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 127/176 [37:02<13:59, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.745, 'grad_norm': 1.297504186630249, 'learning_rate': 4.000338985513046e-05, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 128/176 [37:20<13:45, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8127, 'grad_norm': 1.1280875205993652, 'learning_rate': 3.8500155598204644e-05, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 129/176 [37:37<13:26, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7404, 'grad_norm': 1.2223368883132935, 'learning_rate': 3.701894779717286e-05, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 130/176 [37:54<13:09, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7192, 'grad_norm': 1.2007910013198853, 'learning_rate': 3.5560296953512295e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 131/176 [38:11<12:53, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8706, 'grad_norm': 1.2080594301223755, 'learning_rate': 3.4124725489820645e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 132/176 [38:29<12:38, 17.23s/it]/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7113, 'grad_norm': 1.2391043901443481, 'learning_rate': 3.2712747562708115e-05, 'epoch': 2.94}\n",
      "[2025-04-08 10:35:22,890] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:89] [PID:770856] Saving model to ./model-out/checkpoint-132/pytorch_model_fsdp.bin\n",
      "[2025-04-08 10:35:25,563] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:91] [PID:770856] Model saved to ./model-out/checkpoint-132/pytorch_model_fsdp.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 10:35:26,150] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:193] [PID:770856] Saving Optimizer state to ./model-out/checkpoint-132/optimizer.bin\n",
      "[2025-04-08 10:35:34,852] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:195] [PID:770856] Optimizer state saved in ./model-out/checkpoint-132/optimizer.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 133/176 [39:14<18:28, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7375, 'grad_norm': 1.210554838180542, 'learning_rate': 3.132486887864992e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 134/176 [39:31<16:11, 23.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8182, 'grad_norm': 1.2330275774002075, 'learning_rate': 2.9961586512864947e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 135/176 [39:40<12:51, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7653, 'grad_norm': 1.7565335035324097, 'learning_rate': 2.8623388731286093e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 136/176 [39:57<12:13, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5605, 'grad_norm': 1.2535507678985596, 'learning_rate': 2.7310754815685624e-05, 'epoch': 3.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 137/176 [40:14<11:39, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6214, 'grad_norm': 1.3677128553390503, 'learning_rate': 2.6024154892017937e-05, 'epoch': 3.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 138/176 [40:31<11:11, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4828, 'grad_norm': 1.1938384771347046, 'learning_rate': 2.4764049762041874e-05, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 139/176 [40:49<10:50, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4904, 'grad_norm': 1.1888666152954102, 'learning_rate': 2.353089073828255e-05, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 140/176 [41:06<10:27, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5577, 'grad_norm': 1.1722335815429688, 'learning_rate': 2.2325119482391467e-05, 'epoch': 3.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 141/176 [41:23<10:06, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5155, 'grad_norm': 1.301377773284912, 'learning_rate': 2.1147167846963422e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 142/176 [41:40<09:47, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4883, 'grad_norm': 1.2315571308135986, 'learning_rate': 1.999745772086655e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 143/176 [41:57<09:30, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5022, 'grad_norm': 1.299026370048523, 'learning_rate': 1.8876400878140775e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 144/176 [42:14<09:10, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5679, 'grad_norm': 1.2970017194747925, 'learning_rate': 1.7784398830519e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 145/176 [42:31<08:52, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5522, 'grad_norm': 1.3924212455749512, 'learning_rate': 1.672184268362391e-05, 'epoch': 3.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 146/176 [42:49<08:35, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4788, 'grad_norm': 1.328970193862915, 'learning_rate': 1.5689112996891576e-05, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 147/176 [43:06<08:19, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4277, 'grad_norm': 1.1732603311538696, 'learning_rate': 1.4686579647272336e-05, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 148/176 [43:23<08:00, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4428, 'grad_norm': 1.588991403579712, 'learning_rate': 1.3714601696757712e-05, 'epoch': 3.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 149/176 [43:40<07:42, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4749, 'grad_norm': 1.4834729433059692, 'learning_rate': 1.2773527263780626e-05, 'epoch': 3.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 150/176 [43:57<07:27, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4248, 'grad_norm': 1.3817076683044434, 'learning_rate': 1.1863693398535114e-05, 'epoch': 3.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 151/176 [44:14<07:09, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5609, 'grad_norm': 1.4564628601074219, 'learning_rate': 1.0985425962260343e-05, 'epoch': 3.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 152/176 [44:32<06:51, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6124, 'grad_norm': 1.6900793313980103, 'learning_rate': 1.01390395105318e-05, 'epoch': 3.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 153/176 [44:49<06:34, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5216, 'grad_norm': 1.4833459854125977, 'learning_rate': 9.324837180601741e-06, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 154/176 [45:06<06:18, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4786, 'grad_norm': 1.4516291618347168, 'learning_rate': 8.543110582829272e-06, 'epoch': 3.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 155/176 [45:23<06:00, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4273, 'grad_norm': 1.3579373359680176, 'learning_rate': 7.794139696238645e-06, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 156/176 [45:40<05:43, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4948, 'grad_norm': 1.4427732229232788, 'learning_rate': 7.078192768243486e-06, 'epoch': 3.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 157/176 [45:58<05:27, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4579, 'grad_norm': 1.523766279220581, 'learning_rate': 6.395526218572723e-06, 'epoch': 3.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 158/176 [46:15<05:09, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5656, 'grad_norm': 1.6758238077163696, 'learning_rate': 5.746384547432737e-06, 'epoch': 3.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 159/176 [46:32<04:51, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5149, 'grad_norm': 1.6158514022827148, 'learning_rate': 5.131000247938367e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 160/176 [46:49<04:35, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6253, 'grad_norm': 1.6002038717269897, 'learning_rate': 4.549593722844492e-06, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 161/176 [47:06<04:18, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4721, 'grad_norm': 1.424229621887207, 'learning_rate': 4.002373205607723e-06, 'epoch': 3.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 162/176 [47:23<04:00, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5228, 'grad_norm': 1.4248486757278442, 'learning_rate': 3.4895346858066724e-06, 'epoch': 3.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 163/176 [47:41<03:43, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4942, 'grad_norm': 1.3427907228469849, 'learning_rate': 3.011261838947277e-06, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 164/176 [47:58<03:26, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4782, 'grad_norm': 1.3611289262771606, 'learning_rate': 2.5677259606786684e-06, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 165/176 [48:15<03:09, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4534, 'grad_norm': 1.5842920541763306, 'learning_rate': 2.159085905442737e-06, 'epoch': 3.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 166/176 [48:32<02:51, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4023, 'grad_norm': 1.297414779663086, 'learning_rate': 1.7854880295797405e-06, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 167/176 [48:49<02:34, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4933, 'grad_norm': 1.5247983932495117, 'learning_rate': 1.4470661389100804e-06, 'epoch': 3.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 168/176 [49:07<02:17, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3786, 'grad_norm': 1.3487112522125244, 'learning_rate': 1.143941440811147e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 169/176 [49:24<02:00, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4751, 'grad_norm': 1.3351223468780518, 'learning_rate': 8.762225008062674e-07, 'epoch': 3.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 170/176 [49:41<01:42, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4815, 'grad_norm': 1.381123661994934, 'learning_rate': 6.440052036815081e-07, 'epoch': 3.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 171/176 [49:58<01:25, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4487, 'grad_norm': 1.4288486242294312, 'learning_rate': 4.4737271914411236e-07, 'epoch': 3.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 172/176 [50:15<01:08, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4202, 'grad_norm': 1.4863215684890747, 'learning_rate': 2.86395472034795e-07, 'epoch': 3.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 173/176 [50:33<00:51, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5167, 'grad_norm': 1.4859470129013062, 'learning_rate': 1.611311171048735e-07, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 174/176 [50:50<00:34, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.574, 'grad_norm': 1.4528590440750122, 'learning_rate': 7.162451836685291e-08, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 175/176 [51:07<00:17, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4701, 'grad_norm': 1.3968466520309448, 'learning_rate': 1.7907733026223394e-08, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [51:24<00:00, 17.18s/it]/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6465, 'grad_norm': 1.4981250762939453, 'learning_rate': 0.0, 'epoch': 3.92}\n",
      "[2025-04-08 10:48:18,649] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:89] [PID:770856] Saving model to ./model-out/checkpoint-176/pytorch_model_fsdp.bin\n",
      "[2025-04-08 10:48:22,085] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_model:91] [PID:770856] Model saved to ./model-out/checkpoint-176/pytorch_model_fsdp.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:863: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 10:48:22,670] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:193] [PID:770856] Saving Optimizer state to ./model-out/checkpoint-176/optimizer.bin\n",
      "[2025-04-08 10:48:29,984] [INFO] [accelerate.utils.fsdp_utils.save_fsdp_optimizer:195] [PID:770856] Optimizer state saved in ./model-out/checkpoint-176/optimizer.bin\n",
      "{'train_runtime': 3113.0634, 'train_samples_per_second': 0.481, 'train_steps_per_second': 0.057, 'train_loss': 1.0278361282226713, 'epoch': 3.92}\n",
      "[2025-04-08 10:48:30,103] [INFO] [axolotl.train.train:192] [PID:770856] [RANK:0] Training Completed!!! Saving pre-trained model to ./model-out\u001b[39m\n",
      "[2025-04-08 10:48:30,112] [INFO] [axolotl.train.train:204] [PID:770856] [RANK:0] Set FSDP state dict type to FULL_STATE_DICT for saving.\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [51:53<00:00, 17.69s/it]\n",
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "[rank0]:[W408 10:48:41.427339333 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['axolotl', 'train', 'qlora-fsdp.yaml'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# train model\n",
    "subprocess.run([\"axolotl\", \"train\", \"qlora-fsdp.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport yaml\\nimport torch\\nfrom transformers import (\\n    AutoTokenizer,\\n    AutoModelForCausalLM,\\n    BitsAndBytesConfig,\\n    TrainingArguments,\\n    Trainer,\\n    DataCollatorForSeq2Seq\\n)\\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\nfrom datasets import load_dataset\\n\\nwith open(\"qlora-fsdp.yaml\", \"r\") as f:\\n    config = yaml.safe_load(f)\\n\\nbase_model = config.get(\"base_model\", \"Qwen/Qwen2-7B\")\\nlora_cfg = config.get(\"lora_config\", {})\\ntrain_cfg = config.get(\"training\", {})\\ndataset_cfg = config.get(\"datasets\", [{}])[0]\\n\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\"\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    base_model,\\n    quantization_config=bnb_config,\\n    device_map={\"\": torch.cuda.current_device()},\\n    trust_remote_code=True\\n)\\nmodel = prepare_model_for_kbit_training(model)\\n\\nlora_config = LoraConfig(\\n    r=lora_cfg.get(\"r\", 32),\\n    lora_alpha=lora_cfg.get(\"lora_alpha\", 64),\\n    lora_dropout=lora_cfg.get(\"lora_dropout\", 0.05),\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\"\\n)\\nmodel = get_peft_model(model, lora_config)\\nmodel.gradient_checkpointing_enable() \\n\\ndata_path = dataset_cfg.get(\"path\", \"./train.jsonl\")\\ndataset = load_dataset(\"json\", data_files=data_path)[\"train\"]\\n\\ndef formatting_func(example):\\n    prompt = f\"{example[\\'instruction\\']}\\n\\n{example[\\'input\\']}\"\\n    \\n    inputs = tokenizer(prompt, truncation=True, max_length=2048)\\n    outputs = tokenizer(example[\"output\"], truncation=True, max_length=700)\\n\\n    input_ids = inputs[\"input_ids\"]\\n    label_ids = outputs[\"input_ids\"]\\n\\n    return {\\n        \"input_ids\": input_ids,\\n        \"attention_mask\": inputs[\"attention_mask\"],\\n        \"labels\": label_ids\\n    }\\n\\ndataset = dataset.map(formatting_func, remove_columns=dataset.column_names)\\n\\ntraining_args = TrainingArguments(\\n    output_dir=\"./qlora-output\",\\n    per_device_train_batch_size=1,\\n    gradient_accumulation_steps=4,\\n    num_train_epochs=4,\\n    learning_rate=2e-4,\\n    logging_steps=10,\\n    save_strategy=\"epoch\",\\n    bf16=True,\\n    save_total_limit=1,\\n    report_to=\"none\",\\n\\n    # FSDP\\n    fsdp=\"full_shard auto_wrap\",\\n    fsdp_transformer_layer_cls_to_wrap=\"Qwen2DecoderLayer\",\\n    fsdp_config={\\n        \"fsdp_limit_all_gathers\": True,\\n        \"fsdp_sync_module_states\": True,\\n        \"fsdp_offload_params\": True,\\n        \"fsdp_use_orig_params\": False,\\n        \"fsdp_cpu_ram_efficient_loading\": True,\\n        \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\\n        \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\\n        \"fsdp_sharding_strategy\": \"FULL_SHARD\"\\n    }\\n)\\n\\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"pt\")\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=data_collator,\\n    train_dataset=dataset\\n)\\n\\ntrainer.train()\\n\\nmodel.save_pretrained(\"./qlora-output2/final\")\\ntokenizer.save_pretrained(\"./qlora-output2/final\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another training method (server 209 has only one GPU -> OOM)\n",
    "\"\"\"\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "with open(\"qlora-fsdp.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_model = config.get(\"base_model\", \"Qwen/Qwen2-7B\")\n",
    "lora_cfg = config.get(\"lora_config\", {})\n",
    "train_cfg = config.get(\"training\", {})\n",
    "dataset_cfg = config.get(\"datasets\", [{}])[0]\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_cfg.get(\"r\", 32),\n",
    "    lora_alpha=lora_cfg.get(\"lora_alpha\", 64),\n",
    "    lora_dropout=lora_cfg.get(\"lora_dropout\", 0.05),\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable() \n",
    "\n",
    "data_path = dataset_cfg.get(\"path\", \"./train.jsonl\")\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "def formatting_func(example):\n",
    "    prompt = f\"{example['instruction']}\\n\\n{example['input']}\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, truncation=True, max_length=2048)\n",
    "    outputs = tokenizer(example[\"output\"], truncation=True, max_length=700)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    label_ids = outputs[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": label_ids\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(formatting_func, remove_columns=dataset.column_names)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # FSDP\n",
    "    fsdp=\"full_shard auto_wrap\",\n",
    "    fsdp_transformer_layer_cls_to_wrap=\"Qwen2DecoderLayer\",\n",
    "    fsdp_config={\n",
    "        \"fsdp_limit_all_gathers\": True,\n",
    "        \"fsdp_sync_module_states\": True,\n",
    "        \"fsdp_offload_params\": True,\n",
    "        \"fsdp_use_orig_params\": False,\n",
    "        \"fsdp_cpu_ram_efficient_loading\": True,\n",
    "        \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n",
    "        \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n",
    "        \"fsdp_sharding_strategy\": \"FULL_SHARD\"\n",
    "    }\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./qlora-output2/final\")\n",
    "tokenizer.save_pretrained(\"./qlora-output2/final\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuof/.conda/envs/llm2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]\n",
      "🔍 Generating Abstracts:   0%|          | 0/103 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   1%|          | 1/103 [00:11<19:00, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   2%|▏         | 2/103 [00:22<18:39, 11.08s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   3%|▎         | 3/103 [00:33<18:24, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   4%|▍         | 4/103 [00:44<18:10, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   5%|▍         | 5/103 [00:55<17:56, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   6%|▌         | 6/103 [01:06<17:45, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   7%|▋         | 7/103 [01:17<17:35, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   8%|▊         | 8/103 [01:28<17:24, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:   9%|▊         | 9/103 [01:39<17:14, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  10%|▉         | 10/103 [01:50<17:02, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  11%|█         | 11/103 [02:01<16:51, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  12%|█▏        | 12/103 [02:12<16:41, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  13%|█▎        | 13/103 [02:23<16:29, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  14%|█▎        | 14/103 [02:34<16:18, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  15%|█▍        | 15/103 [02:45<16:06, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  16%|█▌        | 16/103 [02:56<15:55, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  17%|█▋        | 17/103 [03:07<15:44, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  17%|█▋        | 18/103 [03:17<15:33, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  18%|█▊        | 19/103 [03:28<15:22, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  19%|█▉        | 20/103 [03:39<15:11, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  20%|██        | 21/103 [03:50<15:00, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  21%|██▏       | 22/103 [04:01<14:49, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  22%|██▏       | 23/103 [04:12<14:38, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  23%|██▎       | 24/103 [04:23<14:27, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  24%|██▍       | 25/103 [04:34<14:16, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  25%|██▌       | 26/103 [04:45<14:05, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  26%|██▌       | 27/103 [04:56<13:54, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  27%|██▋       | 28/103 [05:07<13:44, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  28%|██▊       | 29/103 [05:18<13:33, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  29%|██▉       | 30/103 [05:29<13:21, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  30%|███       | 31/103 [05:40<13:11, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  31%|███       | 32/103 [05:51<13:00, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  32%|███▏      | 33/103 [06:02<12:49, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  33%|███▎      | 34/103 [06:13<12:38, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  34%|███▍      | 35/103 [06:24<12:27, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  35%|███▍      | 36/103 [06:35<12:16, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  36%|███▌      | 37/103 [06:46<12:05, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  37%|███▋      | 38/103 [06:57<11:54, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  38%|███▊      | 39/103 [07:08<11:43, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  39%|███▉      | 40/103 [07:19<11:32, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  40%|███▉      | 41/103 [07:30<11:22, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  41%|████      | 42/103 [07:41<11:11, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  42%|████▏     | 43/103 [07:52<11:01, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  43%|████▎     | 44/103 [08:03<10:49, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  44%|████▎     | 45/103 [08:14<10:37, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  45%|████▍     | 46/103 [08:25<10:26, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  46%|████▌     | 47/103 [08:36<10:17, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  47%|████▋     | 48/103 [08:47<10:06, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  48%|████▊     | 49/103 [08:58<09:54, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  49%|████▊     | 50/103 [09:09<09:43, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  50%|████▉     | 51/103 [09:20<09:32, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  50%|█████     | 52/103 [09:31<09:21, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  51%|█████▏    | 53/103 [09:42<09:09, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  52%|█████▏    | 54/103 [09:53<08:58, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  53%|█████▎    | 55/103 [10:04<08:47, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  54%|█████▍    | 56/103 [10:15<08:36, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  55%|█████▌    | 57/103 [10:26<08:25, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  56%|█████▋    | 58/103 [10:37<08:14, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  57%|█████▋    | 59/103 [10:48<08:03, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  58%|█████▊    | 60/103 [10:59<07:52, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  59%|█████▉    | 61/103 [11:10<07:42, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  60%|██████    | 62/103 [11:21<07:31, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  61%|██████    | 63/103 [11:32<07:20, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  62%|██████▏   | 64/103 [11:43<07:09, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  63%|██████▎   | 65/103 [11:54<06:57, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  64%|██████▍   | 66/103 [12:05<06:46, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  65%|██████▌   | 67/103 [12:16<06:35, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  66%|██████▌   | 68/103 [12:27<06:24, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  67%|██████▋   | 69/103 [12:38<06:13, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  68%|██████▊   | 70/103 [12:49<06:02, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  69%|██████▉   | 71/103 [13:00<05:51, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  70%|██████▉   | 72/103 [13:11<05:40, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  71%|███████   | 73/103 [13:22<05:29, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  72%|███████▏  | 74/103 [13:33<05:18, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  73%|███████▎  | 75/103 [13:44<05:07, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  74%|███████▍  | 76/103 [13:55<04:56, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  75%|███████▍  | 77/103 [14:06<04:45, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  76%|███████▌  | 78/103 [14:17<04:34, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  77%|███████▋  | 79/103 [14:28<04:23, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  78%|███████▊  | 80/103 [14:39<04:12, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  79%|███████▊  | 81/103 [14:50<04:01, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  80%|███████▉  | 82/103 [15:01<03:50, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  81%|████████  | 83/103 [15:12<03:39, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  82%|████████▏ | 84/103 [15:23<03:28, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  83%|████████▎ | 85/103 [15:34<03:17, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  83%|████████▎ | 86/103 [15:45<03:06, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  84%|████████▍ | 87/103 [15:56<02:56, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  85%|████████▌ | 88/103 [16:07<02:45, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  86%|████████▋ | 89/103 [16:18<02:34, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  87%|████████▋ | 90/103 [16:29<02:22, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  88%|████████▊ | 91/103 [16:40<02:12, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  89%|████████▉ | 92/103 [16:51<02:01, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  90%|█████████ | 93/103 [17:02<01:49, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  91%|█████████▏| 94/103 [17:13<01:39, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  92%|█████████▏| 95/103 [17:24<01:27, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  93%|█████████▎| 96/103 [17:35<01:16, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  94%|█████████▍| 97/103 [17:46<01:05, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  95%|█████████▌| 98/103 [17:57<00:54, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  96%|█████████▌| 99/103 [18:08<00:43, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  97%|█████████▋| 100/103 [18:19<00:32, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  98%|█████████▊| 101/103 [18:30<00:21, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts:  99%|█████████▉| 102/103 [18:41<00:10, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "🔍 Generating Abstracts: 100%|██████████| 103/103 [18:52<00:00, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 推論完成，已輸出至：output_QWEN2_2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# path\n",
    "base_model_path = \"Qwen/Qwen2-7B\"\n",
    "adapter_path = \"/mnt/sda1/shuof/HW_LLM/HW2/model-out\"\n",
    "input_file = \"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction/data/test_axolotl_format_llama.json\"\n",
    "output_file = \"output_QWEN2_2.json\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# Instruction prompt\n",
    "instruction = (\n",
    "    \"You are a professional researcher writing for a top-tier academic conference. \"\n",
    "    \"Based on the introduction, write a concise and formal abstract that clearly presents \"\n",
    "    \"the motivation, methodology, and contributions of the work.\"\n",
    ")\n",
    "\n",
    "# tqdm\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in tqdm(lines, desc=\"🔍 Generating Abstracts\"):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        paper_id = str(data.get(\"paper_id\", \"\")).strip()\n",
    "        intro = data.get(\"introduction\", \"\").strip()\n",
    "\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{intro}\\n\\n### Response:\\n\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        response = decoded.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "        json.dump({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"abstract\": response\n",
    "        }, outfile, ensure_ascii=False)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ 推論完成，已輸出至：{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/mnt/sda1/shuof/HW_LLM/HW2/HW2_Introduction/data/train.json\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "    train_data = train_data[:103]\n",
    "\n",
    "refs = [entry[\"abstract\"] for entry in train_data]\n",
    "\n",
    "with open(\"/mnt/sda1/shuof/HW_LLM/HW2/output_QWEN2.json\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "    test_data = test_data\n",
    "\n",
    "preds = [entry[\"abstract\"] for entry in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.3630904045105764), 'rouge2': np.float64(0.06626940457958805), 'rougeL': np.float64(0.1797194956005157), 'rougeLsum': np.float64(0.17948802720259488)}\n",
      "{'precision': [0.8332086801528931, 0.8455277681350708, 0.8418658971786499, 0.8484259247779846, 0.8263354301452637, 0.8333075046539307, 0.8365870118141174, 0.8522847294807434, 0.8509970903396606, 0.8371009230613708, 0.8677625060081482, 0.843564510345459, 0.8210716843605042, 0.8473855257034302, 0.832959771156311, 0.8347581624984741, 0.8392698764801025, 0.8446987271308899, 0.8558298349380493, 0.8426832556724548, 0.8440885543823242, 0.8410561084747314, 0.8371365070343018, 0.8374246954917908, 0.8246350288391113, 0.844743013381958, 0.8330234289169312, 0.8459643125534058, 0.8334879875183105, 0.8387441039085388, 0.8435492515563965, 0.8509434461593628, 0.8429733514785767, 0.8419063091278076, 0.847912073135376, 0.8334373235702515, 0.8438273668289185, 0.8451995849609375, 0.8429703712463379, 0.8497307300567627, 0.8419461250305176, 0.8251309990882874, 0.8258146643638611, 0.8245106339454651, 0.8427125215530396, 0.8338954448699951, 0.8363418579101562, 0.8402749300003052, 0.8531222343444824, 0.8479272127151489, 0.8077174425125122, 0.8394051790237427, 0.8232824206352234, 0.856863260269165, 0.8291091322898865, 0.8560258150100708, 0.842045783996582, 0.8344818353652954, 0.8430097103118896, 0.8434966802597046, 0.8293881416320801, 0.8382731676101685, 0.8332511782646179, 0.8372607827186584, 0.8400305509567261, 0.826683759689331, 0.8411728143692017, 0.8399480581283569, 0.8512871861457825, 0.8423504829406738, 0.8390074968338013, 0.813855767250061, 0.8356399536132812, 0.8496890664100647, 0.857762336730957, 0.8431212306022644, 0.8527268171310425, 0.8549299240112305, 0.8395230174064636, 0.8424860239028931, 0.8450047969818115, 0.7319635152816772, 0.8470324873924255, 0.8385716080665588, 0.8364430665969849, 0.833306074142456, 0.8451288342475891, 0.85847008228302, 0.8360166549682617, 0.8590326905250549, 0.8481977581977844, 0.8390196561813354, 0.8409488201141357, 0.8321517705917358, 0.8512195944786072, 0.8245235085487366, 0.8495506048202515, 0.84248948097229, 0.8424416780471802, 0.8505717515945435, 0.8488756418228149, 0.8524583578109741, 0.8459585309028625], 'recall': [0.8530764579772949, 0.8417487144470215, 0.8312231302261353, 0.8431779742240906, 0.8296727538108826, 0.82828688621521, 0.8438795208930969, 0.8514620065689087, 0.8485665917396545, 0.8472044467926025, 0.8423552513122559, 0.844253420829773, 0.8231974244117737, 0.8489375710487366, 0.8510754108428955, 0.8574957847595215, 0.8450514078140259, 0.84725421667099, 0.8377232551574707, 0.8409972190856934, 0.8339871168136597, 0.8388373851776123, 0.8465728163719177, 0.8562723398208618, 0.8415110111236572, 0.8367942571640015, 0.8451457619667053, 0.8257313370704651, 0.8435918092727661, 0.8429473042488098, 0.8237809538841248, 0.84688800573349, 0.8348407745361328, 0.8311108350753784, 0.831596851348877, 0.8439247608184814, 0.8518494367599487, 0.8391448855400085, 0.8355921506881714, 0.8590698838233948, 0.8279935121536255, 0.8360797166824341, 0.8353346586227417, 0.8476046323776245, 0.8321467041969299, 0.8355065584182739, 0.8509395122528076, 0.8364988565444946, 0.8382945656776428, 0.8394644856452942, 0.8389269709587097, 0.8407460451126099, 0.8190946578979492, 0.8490716218948364, 0.8323900103569031, 0.8484784960746765, 0.8505405187606812, 0.8405452370643616, 0.8474014401435852, 0.843968391418457, 0.8401769399642944, 0.8297193646430969, 0.8223274946212769, 0.8004509806632996, 0.8437309265136719, 0.825932502746582, 0.8493207693099976, 0.8413518071174622, 0.8484187126159668, 0.8553137183189392, 0.8314024209976196, 0.8349961638450623, 0.8462073802947998, 0.8450794219970703, 0.8607815504074097, 0.8367933630943298, 0.8442295789718628, 0.8506629467010498, 0.8344805240631104, 0.8577802777290344, 0.8347791433334351, 0.8375356793403625, 0.8481212258338928, 0.8432941436767578, 0.8641766905784607, 0.853245735168457, 0.8464191555976868, 0.8537153601646423, 0.8569948673248291, 0.8506348133087158, 0.8434914350509644, 0.8232418894767761, 0.850898265838623, 0.8257635831832886, 0.8541384935379028, 0.8411732316017151, 0.8359062671661377, 0.8408640027046204, 0.84566330909729, 0.8491177558898926, 0.8498213291168213, 0.83265221118927, 0.8474830389022827], 'f1': [0.8430255055427551, 0.8436340093612671, 0.8365106582641602, 0.8457937836647034, 0.8280007243156433, 0.8307896256446838, 0.8402174115180969, 0.8518730998039246, 0.8497800230979919, 0.8421223163604736, 0.854870080947876, 0.8439088463783264, 0.8221331834793091, 0.8481608033180237, 0.8419201970100403, 0.8459742069244385, 0.8421507477760315, 0.8459745645523071, 0.8466797471046448, 0.8418393731117249, 0.8390074372291565, 0.8399452567100525, 0.8418282270431519, 0.8467437028884888, 0.8329875469207764, 0.8407498598098755, 0.8390407562255859, 0.8357253670692444, 0.8385094404220581, 0.8408404588699341, 0.8335478901863098, 0.8489108681678772, 0.8388873338699341, 0.8364737629890442, 0.8396751880645752, 0.83864825963974, 0.847819447517395, 0.8421612977981567, 0.8392650485038757, 0.8543748259544373, 0.8349115252494812, 0.8305692672729492, 0.8305473923683167, 0.8358981013298035, 0.8373962640762329, 0.8347002267837524, 0.8435775637626648, 0.8383826613426208, 0.8456434607505798, 0.8436746001243591, 0.8230264782905579, 0.8400750756263733, 0.8211831450462341, 0.8529496788978577, 0.8307463526725769, 0.8522354960441589, 0.846271812915802, 0.8375024795532227, 0.8451998829841614, 0.843732476234436, 0.8347476720809937, 0.833974301815033, 0.8277533054351807, 0.8184422254562378, 0.8418766856193542, 0.8263079524040222, 0.8452271819114685, 0.8406493067741394, 0.8498505353927612, 0.8487825989723206, 0.8351876139640808, 0.8242904543876648, 0.8408904671669006, 0.8473780155181885, 0.8592692613601685, 0.839945375919342, 0.8484569191932678, 0.8527910709381104, 0.8369941711425781, 0.8500642776489258, 0.8398607969284058, 0.7811989188194275, 0.8475764989852905, 0.8409262299537659, 0.8500837683677673, 0.8431580662727356, 0.8457735180854797, 0.8560861945152283, 0.8463757634162903, 0.8548130989074707, 0.8458380103111267, 0.8310559391975403, 0.8458942770957947, 0.8289453387260437, 0.8526765704154968, 0.8327651619911194, 0.8426732420921326, 0.8416759967803955, 0.8440493941307068, 0.8498440980911255, 0.849348247051239, 0.8424388766288757, 0.8467200994491577], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.48.3)'}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load (\"bertscore\")\n",
    "rouge = metric_rouge.compute (predictions=preds, references=refs, use_stemmer=True)\n",
    "bertscore = metric_bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "\n",
    "print(rouge)\n",
    "print(bertscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
