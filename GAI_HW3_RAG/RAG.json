[
  {
    "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
    "answer": "According to the text, the SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles. These articles are equally distributed into:\n\n* Training set: 144 articles\n* Test set: 100 articles",
    "evidence": [
      "dataset BIBREF0 . This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations,",
      "Dataset and Preprocessing",
      "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an",
      "equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators.",
      "keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both",
      "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful",
      "intact, as they are the two most keyphrase dense parts within scientific articles BIBREF4 , and select only the most content bearing sentences from the remaining contents. To do this, sentences are ordered using TextRank BIBREF14 and the less informative",
      "off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were",
      "Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset BIBREF0 . This dataset is composed of documents (scientific articles) that were automatically converted from PDF",
      "level 3 preprocessed documents using an unsupervised summarization technique. More specifically, we keep the title and abstract intact, as they are the two most keyphrase dense parts within scientific articles BIBREF4 , and select only the most content"
    ]
  },
  {
    "title": "Comparative Studies of Detecting Abusive Language on Twitter",
    "answer": "The authors provide two examples to illustrate the difficulties presented by the context-dependent nature of online aggression:\n\n**Example 1:**\nTweet (2): \"I hate it when I'm trying to board a bus and there's already an as**ole on it.\"\nWithout context, this tweet is labeled as abusive due to the use of vulgar",
    "evidence": [
      "The context-dependent nature of online aggression makes annotating large collections of data extremely difficult. Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models. Recently,",
      "However, their approach was limited because the meta-information they focused on—author information, conversation type, use of the same hashtags or emojis—are all highly dependent on data. In order to avoid data dependency, text sequences of context",
      "INLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it.  Similarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice. (3) Survivors of #Syria",
      "our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better",
      "causes aggravation BIBREF0 , BIBREF1 . Abusive language is often reframed as, but not limited to, offensive language BIBREF2 , cyberbullying BIBREF3 , othering language BIBREF4 , and hate speech BIBREF5 . Recently, an increasing number of users have been",
      "to censor offensive language, yet it seems nearly impossible to successfully resolve the issue BIBREF7 , BIBREF8 . The major reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics BIBREF9 .",
      "models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic",
      "been comprehensively studied to its potential. In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for",
      "important in understanding the abusive tweet (4), especially in identifying the target of the malice. (3) Survivors of #Syria Gas Attack Recount `a Cruel Scene'.  INLINEFORM0 (4) Who the HELL is “LIKE\" ING this post? Sick people....  Huang et al.",
      "the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1). (1) I hate when I'm sitting in front of the bus and somebody with a wheelchair get on."
    ]
  },
  {
    "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
    "answer": "According to the text, enforcing agreement between parse trees across different languages (specifically English and Hindi) works through a dual decomposition-based algorithm. Here's a step-by-step breakdown:\n\n1. **Initialization**: Obtain predicted parse trees for the English and Hindi sentences from their respective trained parser models.\n2. **Dual Decomposition (DD",
    "evidence": [
      "a parse tree for English which maximizes the agreement between the English and Hindi parse tree when the Hindi parse tree is fixed, and likewise for the Hindi parse tree. The algorithm converges when the trees no longer change, Let us now look at the",
      "The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and",
      "alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can",
      "between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained",
      "models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two",
      "fixed, and likewise for the Hindi parse tree. The algorithm converges when the trees no longer change, Let us now look at the Project algorithm (Algorithm 2) in detail. It predicts the tree for a sentence in the target language, given the parse tree in",
      "alignments given. We first obtain the predicted parse trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to",
      "trees. The dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints. The problem is formulated as below: In the above formulation, INLINEFORM0 and INLINEFORM1 represent a English and Hindi sentence",
      "tree in one language to correspond to more than one edge, or rather, a path, in the other language parse tree. This has been captured in the examples in figures FIGREF6 (A) and FIGREF6 (B). For an edge in the English parse tree, we term the corresponding",
      "Project algorithm (Algorithm 2) in detail. It predicts the tree for a sentence in the target language, given the parse tree in the source language, and the word alignments between the parallel sentence. Project Algorithm (tree T, sen S) [1] A parse tree T"
    ]
  },
  {
    "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego",
    "answer": "According to the text, NeuronBlocks includes a suite of neural network modules as building blocks, specifically:\n\n1. Embedding (word/character embedding)\n2. CNN (Convolutional Neural Network)\n3. RNN (Recurrent Neural Network)\n4. Transformer\n5. CRF (Conditional Random Field)\n6. LSTM (",
    "evidence": [
      "tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP",
      "components, such as embedding, CNN/RNN, Transformer and so on. Based on the above observations, we developed NeuronBlocks, a DNN toolkit for NLP tasks. The basic idea is to provide two layers of support to the engineers. The upper layer targets common NLP",
      "text matching, sequence labeling, MRC, etc. It further suggested that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on. Based on the above observations, we developed NeuronBlocks,",
      "greatly improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks",
      "layers/modules for the networks. Such design achieves a balance between generality and flexibility. Extensive experiments have verified the effectiveness of this approach. NeuronBlocks has been widely used in a product team of a commercial search engine,",
      "In this paper, we introduce NeuronBlocks, a DNN toolkit for NLP tasks built on PyTorch. NeuronBlocks targets three types of engineers, and provides a two-layer solution to satisfy the requirements from all three types of users. To be more specific, the",
      "The Neuronblocks is built on PyTorch. The overall framework is illustrated in Figure FIGREF16 . It consists of two layers: the Block Zoo and the Model Zoo. In Block Zoo, the most commonly used components of deep neural networks are categorized into",
      "blocks to construct networks with complex architecture. By following the interface guidelines, users can also contribute to this gallery of components with their own modules. The technical contributions of NeuronBlocks are summarized into the following",
      "shared across all users for model architecture design. Moreover, NeuronBlocks has flexible platform support, such as GPU/CPU, GPU management platforms like PAI.",
      "network layers/blocks, such as word/character embedding, CNN/LSTM and CRF. The results suggest that the flexible combination of layers/blocks in NeuronBlocks can easily reproduce the performance of original models, with comparative or slightly better"
    ]
  },
  {
    "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric",
    "answer": "According to the text, they used:\n\n1. Popularity-based algorithms: \n   - INLINEFORM0 \n   - INLINEFORM1 \n   - INLINEFORM2 \n\n2. Similarity-based approaches: \n   - INLINEFORM2 \n   - INLINEFORM3 \n\n3. Hybrid approaches: \n   - A combination of",
    "evidence": [
      "both data sources (i.e., editor tags and Amazon search terms) using a round-robin combination strategy, which ensures an equal weight for both sources. This gives us three additional popularity-based algorithms (= INLINEFORM0 , INLINEFORM1 and INLINEFORM2",
      "Figure FIGREF11 shows the accuracy results of the four hybrid approaches. By combining the best three popularity-based approaches, we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ). On the contrary,",
      "algorithm. Hence, to create a popularity-based hybrid (= INLINEFORM0 ), we combine the best three performing popularity-based approaches from the ones (i) without any contextual signal, (ii) with the author as context, and (iii) with BISAC as context. In",
      "as a hybrid approach that uses the best performing popularity-based and the best performing similarity-based approach (see Figure FIGREF11 in Section SECREF4 for more details about the particular algorithm combinations).",
      "we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ). On the contrary, the combination of the two best performing similarity-based approaches INLINEFORM2 and INLINEFORM3 does not yield better accuracy.",
      "performance. Thus, these results also suggest to investigate beyond-accuracy metrics as done in Section SECREF17 .  Hybrid approaches. Figure FIGREF11 shows the accuracy results of the four hybrid approaches. By combining the best three popularity-based",
      "for both sources. This gives us three additional popularity-based algorithms (= INLINEFORM0 , INLINEFORM1 and INLINEFORM2 ).  Similarity-based approaches. We exploit the textual content of e-books (i.e., description or title) to recommend relevant tags",
      "information), and (iii) hybrid approaches. Due to the lack of personalized tags (i.e., we do not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8 . In total, we evaluate 19",
      "sources and INLINEFORM2 is the similarity score of the corresponding e-book. We again use a round-robin strategy to combine both data sources (= INLINEFORM3 and INLINEFORM4 ).  Hybrid approaches. We use the previously mentioned cross-source algorithm",
      "both data sources (= INLINEFORM3 and INLINEFORM4 ).  Hybrid approaches. We use the previously mentioned cross-source algorithm BIBREF13 to construct four hybrid recommendation approaches. In this case, tags are favored that are recommended by more than"
    ]
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection",
    "answer": "According to the text, the best reported systems are those proposed by:\n\n1. Riloff et al. (2013) - referred to as \"riloff2013sarcasm\"\n2. Joshi et al. (2015) - referred to as \"joshi2015harnessing\"\n\nSpec",
    "evidence": [
      "of multiple classification techniques on our dataset through a stratified 10-fold cross validation. We also compare the classification accuracy of our system and the best available systems proposed by riloff2013sarcasm and joshi2015harnessing on our",
      "other systems. These are: Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive",
      "accuracy of our system and the best available systems proposed by riloff2013sarcasm and joshi2015harnessing on our dataset. Using Weka BIBREF18 and LibSVM BIBREF19 APIs, we implement the following classifiers:",
      "using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system.",
      "The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure FIGREF22 . We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test,",
      "review summarizers, dialog systems, recommendation systems and sentiment analyzers. This makes automatic detection of sarcasm an important problem. However, it has been quite difficult to solve such a problem with traditional NLP tools and techniques.",
      "our classifier with 100%, 90%, 80% and 70% of the training data with our whole feature set, and the feature combination from joshi2015harnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure",
      "joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along with readability and word count features), and Gaze+Sarcasm (the complete set of features). For",
      "feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system. Using cognitive features in an NLP Processing system like ours is the first proposal of its kind. Our general approach may be useful in other NLP",
      "one sentence is displayed to the reader along with the “aspect” with respect to which the annotation has to be provided. While reading, an SR-Research Eyelink-1000 eye-tracker (monocular remote mode, sampling rate 500Hz) records several eye-movement"
    ]
  },
  {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "answer": "The offensive dataset is not biased by topic, dialect, or target because the methodology used to build it does not rely on a seed list of offensive words. This approach allows for a more representative and diverse collection of offensive tweets, unaffected by specific topics, dialects, or targets. \n\nIn other words, the dataset is",
    "evidence": [
      "language does not necessitate the appearance of the vocative particle, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive",
      "In this paper, we focus on building effective Arabic offensive tweet detection. We introduce a method for building an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for",
      "all Arabic tweets are offensive BIBREF5, targeted annotation is essential for efficiently building a large dataset. Since our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we",
      "does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled",
      "In this paper we presented a systematic method for building an Arabic offensive language tweet dataset that does not favor specific dialects, topics, or genres. We developed detailed guidelines for tagging the tweets as clean or offensive, including",
      "dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with",
      "doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific. After inspecting many tweets, we observed that many offensive tweets have the",
      "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a",
      "the largest available Arabic offensive language dataset. We characterized the offensive tweets in the dataset to determine the topics that illicit such language, the dialects that are most often used, the common modes of offensiveness, and the gender",
      "and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require de-biasing it by boosting negative class or random sampling additional"
    ]
  },
  {
    "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
    "answer": "According to the text, an individual model consists of:\n\n* A Bayesian model for a specific language \n* The role inventory for each predicate is divided into Primary and Secondary roles.\n\nIn the context of a multilingual model, an individual model also includes:\n \n* Crosslingual latent variables to incorporate soft role agreement between aligned",
    "evidence": [
      "model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows:  For example, the complete role sequence in a frame could be: INLINEFORM0 INLINEFORM1 , INLINEFORM2 ,",
      "of parallel corpora for this task. Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do",
      "generate samples for the hidden variables (model parameters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:",
      "in the former model where two constituents having the same set of features might get assigned different roles if they appear in different contexts.",
      "Monolingual Model",
      "to the variables in all the training instances except the current one, and INLINEFORM3 refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy. For sampling roles in the multilingual model,",
      "to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:  DISPLAYFORM0   where the subscript INLINEFORM0 refers to all the variables except at position INLINEFORM1 , INLINEFORM2 refers",
      "semantic role induction in multiple languages. The model consists of individual Bayesian models for each language BIBREF3 , and crosslingual latent variables to incorporate soft role agreement between aligned constituents. This latent variable approach",
      "We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary",
      "syntactic features of all the identified arguments (the visible variables). We use a collapsed Gibbs-sampling based approach to generate samples for the hidden variables (model parameters are integrated out). The sample counts and the priors are then used"
    ]
  },
  {
    "title": "Hierarchical Transformers for Long Document Classification",
    "answer": "According to the text, it seems that both RNN (Recurrent LSTM) and Transformer layers can be used on top of BERT for classification tasks. The text mentions two techniques: Recurrence over BERT (RoBERT) which uses an RNN layer, and Transformer over BERT (ToBERT).\n\nThe text does not provide a clear",
    "evidence": [
      "through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after",
      "this work we are using BERT-Base for faster training and experimentation, however, our methods are applicable to BERT-Large as well. BERT-Base and BERT-Large are different in model parameters such as number of transformer blocks, number of self-attention",
      "in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer",
      "not. BERT has been shown to beat the state-of-the-art performance on 11 tasks with no modifications to the model architecture, besides adding a task-specific output layer BIBREF1. We follow same procedure suggested in BIBREF1 for our tasks. Fig. FIGREF8",
      "Method ::: Transformer over BERT",
      "On CSAT, ToBERT performs slightly worse than RoBERT but it is not statistically significant as this dataset is small. Table TABREF27 presents results using fine-tuned BERT predictions instead of the pooled output from final transformer block. For each",
      "besides adding a task-specific output layer BIBREF1. We follow same procedure suggested in BIBREF1 for our tasks. Fig. FIGREF8 shows the BERT model for classification. We obtain two kinds of representation from BERT: pooled output from last transformer",
      "shows the BERT model for classification. We obtain two kinds of representation from BERT: pooled output from last transformer block, denoted by H, and posterior probabilities, denoted by P. There are two variants of BERT - BERT-Base and BERT-Large. In",
      "Because our work builds heavily upon BERT, we provide a brief summary of its features. BERT is built upon the Transformer architecture BIBREF0, which uses self-attention, feed-forward layers, residual connections and layer normalization as the main",
      "we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions. With this approach, we overcome BERT's computational complexity, reducing it to $O(n/k"
    ]
  },
  {
    "title": "Impact of Batch Size on Stopping Active Learning for Text Classification",
    "answer": "None, the provided text does not mention specific downstream tasks being evaluated. The text appears to discuss active learning, a process used to selectively determine which unlabeled samples should be annotated, and the considerations involved in the process, such as batch size and stopping methods.",
    "evidence": [
      "with actual stopping methods can make them experience less degradation in performance when larger batch sizes are used. In particular, an important parameter of stopping methods is the window size of previous iterations to consider. The next subsection",
      "an important parameter of stopping methods is the window size of previous iterations to consider. The next subsection shows how decreasing the window size parameter can help to reduce the degradation in performance that stopping methods experience with",
      "which unlabeled samples should be annotated. The number of samples that are selected for annotation at each iteration of active learning is called the batch size. An important aspect of the active learning process is when to stop the active learning",
      "method over and above the degradation that results from reduced learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window size parameter of how many past iterations of learning are taken into account when",
      "process are when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking for additional labels during each iteration. We found that stopping methods degrade in performance when larger batch sizes are",
      ", BIBREF2 . Active learning sharply increases the performance of iteratively trained machine learning models by selectively determining which unlabeled samples should be annotated. The number of samples that are selected for annotation at each iteration",
      "Active learning has the potential to significantly reduce annotation costs. Two important considerations in the active learning process are when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking",
      "BV2009 needs INLINEFORM1 models to be generated before it begins to check if the average is above the threshold. This does not necessarily mean it stops after INLINEFORM2 models have been generated. Rather, it represents the first point in the active",
      "are used. The degradation in performance is larger than the amount that can be explained due to the degradation in learning efficiency that results from using larger batch sizes. An important parameter used by stopping methods is what window size of",
      "This work was supported in part by The College of New Jersey Support of Scholarly Activities (SOSA) program, by The College of New Jersey Mentored Undergraduate Summer Experience (MUSE) program, and by usage of The College of New Jersey High Performance"
    ]
  },
  {
    "title": "Generating Personalized Recipes from Historical User Preferences",
    "answer": "According to the text, the recipes are scraped from Food.com. Specifically, the authors collect a dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com.",
    "evidence": [
      "model) over 62 sessions. For each pair, users were given a partial recipe specification (name and 3-5 key ingredients), as well as two generated recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one recipe is",
      "Our raw data consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018). See tab:int-stats for dataset summary statistics, and tab:samplegk for sample",
      "from a diverse recipe space: in our training data, the average recipe length is 117 tokens with a maximum of 256. There are 13K unique ingredients across all recipes. Rare words dominate the vocabulary: 95% of words appear $<$100 times, accounting for",
      "recipes, while 10% of users have consumed $>$45 recipes. We order reviews by timestamp, keeping the most recent review for each user as the test set, the second most recent for validation, and the remainder for training (sequential leave-one-out",
      "as two generated recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one recipe is generated by one of our three personalized models (Prior Tech, Prior Name, Prior Recipe). The order of recipe presentation",
      "In the kitchen, we increasingly rely on instructions from cooking websites: recipes. A cook with a predilection for Asian cuisine may wish to prepare chicken curry, but may not know all necessary ingredients apart from a few basics. These users with",
      "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com. Here, we restrict to recipes with at least 3 steps, and at least 4 and no more than 20 ingredients. We discard users with fewer",
      "In this paper, we propose a novel task: to generate personalized recipes from incomplete input specifications and user histories. On a large novel dataset of 180K recipes and 700K reviews, we show that our personalized generative models can generate",
      "than 4 reviews, giving 180K+ recipes and 700K+ reviews, with splits as in tab:recipeixnstats. Our model must learn to generate from a diverse recipe space: in our training data, the average recipe length is 117 tokens with a maximum of 256. There are 13K",
      "Our model's input specification consists of: the recipe name as a sequence of tokens, a partial list of ingredients, and a caloric level (high, medium, low). It outputs the recipe instructions as a token sequence: $\\mathcal {W}_r=\\lbrace w_{r,0}, \\dots ,"
    ]
  },
  {
    "title": "Pyramidal Recurrent Unit for Language Modeling",
    "answer": "They used two widely used datasets: \n\n1. Penn Treebank (PTB) \n2. WikiText2 (WT-2)",
    "evidence": [
      "This research was supported by NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Mandar Joshi, Aniruddha Kembhavi,",
      "Results",
      "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . For both datasets, we follow the same training, validation, and test splits as in BIBREF0 . We extend",
      "for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Mandar Joshi, Aniruddha Kembhavi, and anonymous reviewers for their helpful comments.",
      "of the state-of-the-art methods by large margin on the PTB dataset (e.g. RAN BIBREF7 by 16 points with 4M less parameters, QRNN BIBREF33 by 16 points with 1M more parameters, and NAS BIBREF31 by 1.58 points with 6M less parameters). With advanced",
      "Analysis",
      "use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section",
      "by about 4 points on the PTB dataset and 7 points on the WT-2 dataset. This further improves with finetuning on the PTB (about 2 points) and WT-2 (about 1 point) datasets. For similar number of parameters, the PRU with standard dropout outperforms most of",
      "configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of INLINEFORM1 =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use",
      "are learned weights that linearly map INLINEFORM1 to INLINEFORM2 . To simplify notation, we omit the biases. Motivated by successful applications of sub-sampling in computer vision (e.g., BIBREF22 , BIBREF23 , BIBREF9 , BIBREF24 ), we subsample input"
    ]
  },
  {
    "title": "Emotion Detection in Text: Focusing on Latent Representation",
    "answer": "According to the text, the GRU model captures:\n\n1. **Sequential nature of the text**: The GRU model, specifically a bidirectional GRU network, is designed to analyze the sequential nature of the text, which traditional ML models may not capture effectively.\n2. **Contextual information**: The GRU model creates",
    "evidence": [
      "detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show",
      "nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show",
      "the most important features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole. These partial representations are then were concatenated to create out final hidden",
      "a concatenation of global max-pooling and average-pooling layers (with a window size of two). Then a max-pooling was used to extract the most important features form the GRU output and an average-pooling layer was used to considers all features to create",
      "more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets. For building the emotion classifier, we have decided to use 7",
      "goes to a bidirectional GRU layer selected to capture the entirety of each tweet before passing its output forward. The goal is to create an intermediate representation for the tweets that capture the sequential nature of the data. For the next step, we",
      ", BIBREF32 . These models can capture the complexity an context of the language better not only by keeping the sequential information but also by creating hidden representation for the text as a whole and by learning the important features without any",
      "more than 35 terms, we set the size of the embedding layer to 35 and added padding to shorter tweets. The output of this layer goes to a bidirectional GRU layer selected to capture the entirety of each tweet before passing its output forward. The goal is",
      "specifically a bidirectional GRU network to analyze the tweets. For building the emotion classifier, we have decided to use 7 binary classifiers-one for each emotion- each of which uses the same architecture for detecting a specific emotion. You can see",
      "Model Performances on New Dataset"
    ]
  },
  {
    "title": "ReviewQA: a relational aspect-based opinion reading dataset",
    "answer": "The hotel reviews are from TripAdvisor.",
    "evidence": [
      "of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value, room, location, cleanliness, check-in/front desk, service, business service, can possibly be rated in a review. Figure FIGREF8",
      "we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . In the original data, each",
      "room, location, cleanliness, check-in/front desk, service, business service, can possibly be rated in a review. Figure FIGREF8 displays a review extracted from this dataset.",
      "is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value,",
      "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11 . This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name",
      "we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . In the original data, each review comes with a set of rated aspects among the seventh available: Business service, Check in / Front Desk, Cleanliness, Location,",
      "on the set of tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of",
      "hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of the recent datasets, but selected among a set of candidates that contains all the possible answers to the questions of the",
      "hotels that aim to analyze customer satisfaction regarding products and quality of services. Given a text document, the objective is to predict its overall polarity. Generally, it can be positive, negative or neutral. This analysis gives a quick overview",
      "comes with a set of rated aspects among the seventh available: Business service, Check in / Front Desk, Cleanliness, Location, Room, Sleep Quality, Value and for all the reviews an Overall rating. In this articles we propose to exploit these data to"
    ]
  },
  {
    "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata",
    "answer": "They combine a deep learning model (ELMo) with an expansive knowledge base (Wikidata) to address the issues in fine-grained entity type categorization. Specifically, they use the deep learning model to learn representations for entity mentions based on their context, and then incorporate the rich structure of Wikidata to augment these labels into",
    "evidence": [
      "these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity types based on the hierarchy given from the",
      "many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata).",
      "It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For",
      "entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity.",
      "If a knowledge base has these four matching labels, the distant supervision technique will assign all of them to every mention of “Barack Obama”. Therefore, the training data will also fail to distinguish between mentions of “Barack Obama” in all",
      "training data. Distant supervision is a technique which maps each entity in the corpus to knowledge bases such as Freebase BIBREF13 , DBpedia BIBREF14 , YAGO BIBREF15 and helps with the generation of labeled data. This method will assign the same set of",
      "In this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich",
      "to all mentions of a particular entity in the corpus. For example, “Barack Obama” is a person, politician, lawyer, and author. If a knowledge base has these four matching labels, the distant supervision technique will assign all of them to every mention",
      "on Wiki(gold) so a direct comparison is difficult. While these knowledge bases provide semantically rich and fine-granular classes and relationship types, the task of entity classification often requires associating coarse-grained classes with discovered",
      "and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes. We can see comparisons of our model made on Wiki(gold) in"
    ]
  },
  {
    "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
    "answer": "According to the text, the authors define a concept map as \"a labeled graph showing concepts as nodes and relationships between them as edges.\" \n\nIn other words, a concept map is a graph with:\n\n* Concepts as nodes\n* Relationships between concepts as edges\n* Labels (arbitrary sequences of tokens taken from the documents",
    "evidence": [
      "Concept Maps",
      "Concept Map Construction",
      "content, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the",
      "Concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts",
      "summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a",
      "Villalon.2010 for their (single-document) concept map corpus. Analyzing the graph structure of the maps, we found that all of them are connected. They have on average 7.2 central concepts with more than one relation, while the remaining ones occur in only",
      "we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that",
      "between them as edges (Figure FIGREF2 ). Introduced in 1972 as a teaching tool BIBREF6 , concept maps have found many applications in education BIBREF7 , BIBREF8 , for writing assistance BIBREF9 or to structure information repositories BIBREF10 , BIBREF11",
      "task that we propose is concept-map-based MDS, the summarization of a document cluster in the form of a concept map. In order to develop and evaluate methods for the task, gold-standard corpora are necessary, but no suitable corpus is available. The",
      "labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon.2012 and Valerio.2006, define summarization as their goal"
    ]
  },
  {
    "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus",
    "answer": "The quality of the data is empirically evaluated through several methods, including:\n\n1. **Quality score**: The data is assigned a quality score, which is used to assess its quality.\n2. **Manual inspection**: Samples with low scores are manually inspected and sent back for translation when needed.\n3. **BLEU scores**:",
    "evidence": [
      "quality score as oppose to corpus-level BLEU or unnormalized average of sentence BLEU. And $\\textrm {CoefVar}_{MS}$ is a standardized measure of model stability against different speakers (the lower the better). Table TABREF24 shows the $\\textrm",
      "reading from a bank of donated sentences. Each voice clip was validated by at least two other users. Most of the sentences are covered by multiple speakers, with potentially different genders, age groups or accents. Raw CoVo data contains samples that",
      "the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is",
      "translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and provide empirical evidence of the quality of the data. We also provide initial benchmarks,",
      "embeddings BIBREF17. Samples with low scores were manually inspected and sent back for translation when needed. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and",
      "check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.",
      "We applied this method to these three language pairs only as we are confident about the quality of the corresponding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually",
      "that the results are consistent with what we see from ASR models. For example thanks to abundant training data, French has a decent BLEU score of 29.8/25.4. German doesn't perform well, because of less richness of content (transcripts). The other",
      "Baseline Results ::: Experimental Settings ::: Inference and Evaluation",
      "4 words including punctuations) to reduce the portion of short sentences. This makes the resulting evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality"
    ]
  },
  {
    "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
    "answer": "According to the text, a group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. They assessed the translations based on various metrics, including:\n\n1. Adequacy: rated on a 5-point scale of how much of the meaning is conveyed by",
    "evidence": [
      "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized",
      "The input source and target language sentences used for training were taken and divided into bucketed pairs of sentences of a fixed number of sizes. This relationship was determined by examining the distribution of words in the corpus primarily to",
      "Human Evaluation",
      "To ensure that the increase in BLEU score correlated to actual increase in performance of translation, human evaluation metrics like adequacy, precision and ranking values (between RNNSearch and RNNMorph outputs) were estimated in Table TABREF30 . A group",
      "tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless,",
      "to create the language model for the phrase-based SMT model. This corpus contained 567,772 sentences and was self-collected by combining hundreds of ancient Tamil scriptures, novels and poems by accessing the websites of popular online ebook libraries in",
      "same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in the number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy,",
      "made a distinction between global and local attention by means of AER scores of the attention vectors. A Gaussian distribution and a monotonic lookup were used to facilitate the corresponding local source sentence look-up.",
      "tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention",
      "The corpus selected for this experiment was a combination of different corpora from various domains. The major part of the corpus was made up by the EnTam v2 corpus BIBREF2 . This corpus contained sentences taken from parallel news articles, English and"
    ]
  },
  {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "answer": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset, specifically from 1,268 randomly selected tweets. This dataset is used to create two datasets: \n\n* STAN INLINEFORM0, which consists of 1,108 unique English hashtags \n* STAN INLINEFORM1, which includes all 12",
    "evidence": [
      "as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for",
      "of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset,",
      "their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.",
      "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0 , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along",
      "A hashtag is a keyphrase represented as a sequence of alphanumeric characters plus underscore, preceded by the # symbol. Hashtags play a central role in online communication by providing a tool to categorize the millions of posts generated daily on",
      "unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches",
      "efficient methods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher",
      "sentiment analysis. Feeding the automatically segmented hashtags to a state-of-the-art sentiment analysis method on the SemEval 2017 benchmark dataset results in a 2.6% increase in the official metric for the task.",
      "trends BIBREF2 . Hashtags often carry very important information, such as emotion BIBREF3 , sentiment BIBREF4 , sarcasm BIBREF5 , and named entities BIBREF6 , BIBREF7 . However, inferring the semantics of hashtags is non-trivial since many hashtags",
      "removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, HashtagMaster. BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment BIBREF43 . It"
    ]
  },
  {
    "title": "Extractive Summarization of EHR Discharge Notes",
    "answer": "According to the text, two datasets were mentioned:\n\n1. A dataset of 515 annotated history of present illness notes, which was split into:\n\t* 70% train set\n\t* 15% development set\n\t* 15% test set\n2. A dataset of 55,000 discharge summaries",
    "evidence": [
      "Data",
      "to the CRF. We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25",
      "on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.",
      "present illness notes. We achieve an F1 score of 0.876, which indicates that this model can be employed to create a dataset for evaluation of extractive summarization methods.",
      "upper bound on extractive summarization of discharge notes and develop an LSTM model to sequentially label topics of history of present illness notes. We achieve an F1 score of 0.876, which indicates that this model can be employed to create a dataset for",
      "original text to form the summary BIBREF16 , topic modeling of EHR notes using Latent Dirichlet allocation (LDA) or bayesian networks BIBREF15 , and knowledge based heuristic systems BIBREF17 . To our knowledge, there is no literature to date on",
      "15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25 with an initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for regularization, and each batch",
      "from the Unified Medical Language System (UMLS) were compared in order to assess whether clinically relevant concepts in the discharge summary could be located in the remaining notes BIBREF18 . CUIs were extracted using Apache cTAKES BIBREF19 and filtered",
      "on the dev set. We report performance of the best performing model on the test set. Table TABREF16 compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus",
      "Table TABREF17 shows the precision, recall, F1 score, and support for each of the ten labels, and Figure FIGREF18 shows the confusion matrix illustrating which labels were frequently misclassified. The demographics and patient movement labels achieved the"
    ]
  },
  {
    "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    "answer": "The participants read two types of sentences: \n\n1. 50 sentences of normal reading \n2. 50 sentences of task-specific reading. \n\nThese sentences were presented in blocks of approximately 50 sentences, alternating between the two types of reading tasks. The sentences were displayed on a screen, one at a time, and",
    "evidence": [
      "tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences.",
      "Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an example sentence as it was depicted on the screen during recording. As shown in Figure FIGREF8 (middle), the control condition for this",
      "during the breaks and were encouraged to rest. All sentences were presented at the same position on the screen and could span multiple lines. The sentences were presented in black on a light grey background with font size 20-point Arial, resulting in a",
      "devices, and the personal reading speed of the participants. We recorded 14 blocks of approx. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences",
      "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an",
      "able to read in their own speed, using a control pad to move to the next sentence and to answer the control questions, which allowed for natural reading. Since each subject reads at their own personal pace, the reading speed between varies between",
      "task consisted of single-choice questions about the content of the previous sentence. 12% of randomly selected sentences were followed by such a comprehension question with three answer options on a separate screen.",
      "duration of the recording sessions was between 100 and 180 minutes, depending on the time required to set up and calibrate the devices, and the personal reading speed of the participants. We recorded 14 blocks of approx. 50 sentences, alternating between",
      "dataset statistics are shown in Table TABREF2. Of the 739 sentences, the participants read 349 sentences in a normal reading paradigm, and 390 sentences in a task-specific reading paradigm, in which they had to determine whether a certain relation type",
      "a practice round, which described the relation and was followed by three sample sentences, so that the participants would be familiar with the respective relation type."
    ]
  },
  {
    "title": "Data Collection for Interactive Learning through the Dialog",
    "answer": "The data was collected through conversational dialogs with workers on the crowdsourcing platform CrowdFlower (CF). The CF workers were given tasks to use a chat-like interface to help with the dialogs, which were then annotated by expert annotators. The dialogs were based on questions randomly chosen from training examples of the Simple questions BIB",
    "evidence": [
      "The templates and keywords were manually collected from frequent expressions used by CF workers during preparation runs of the dataset collection process (google it, check wikipedia, I would need... $\\rightarrow $ Negate).",
      "to answer. However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. A CF worker gets a task instructing them to use our chat-like interface to help the",
      "Dataset Collection Process",
      "we collected a dataset from conversational dialogs with workers on crowdsourcing platform CrowdFlower. Those dialogs were annotated with expert annotators and published under Creative Commons 4.0 BY-SA license on lindat. We also provide evaluation scripts",
      "We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase",
      "were not requested by the policy. We collected the dataset (see Section \"Dataset Collection Process\" ) that enables simulation where the policy can decide how much extra information to the question it requests. If the question is clear to the system it",
      "information requested by the system. Parsing of the dialog acts is made by hand written rules using templates and keyword spotting. The templates and keywords were manually collected from frequent expressions used by CF workers during preparation runs of",
      "are available. To fill this gap, we collected a dataset which enables to train interactively learned policies through a simulated interaction with users.",
      "simulated question answering dialogs. Second, it measures accuracy on answer hints understanding. For purposes of evaluation we collected a dataset from conversational dialogs with workers on crowdsourcing platform CrowdFlower. Those dialogs were",
      "stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071)."
    ]
  },
  {
    "title": "Distilling Translations with Visual Awareness",
    "answer": "The English-German dataset. \n\nHowever, it is also mentioned that the authors only experiment with German and French, which suggests that state of the art results are also achieved on a French dataset, but the specific dataset is not mentioned. \n\nA specific dataset mentioned is the Flickr Entities dataset, but it is used for a",
    "evidence": [
      "leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",
      "multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across",
      "dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test sets (average INLINEFORM0 , INLINEFORM1 ). Transformer-based models enriched with image information (base+sum, base+att and",
      "dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official",
      "Results\nIn this section we present results of our experiments, first in the original dataset without any source degradation (Section SECREF18 ) and then in the setup with various source degradation strategies (Section SECREF25 ).",
      "Data",
      "use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing",
      "datasets. In this setting (PERS), we use the Flickr Entities dataset BIBREF28 to identify all the words that were annotated by humans as corresponding to the category person. We then replace such source words with the BLANK token. The statistics of the",
      "with image information, as well as to the deliberation models, with or without image information. We first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German",
      "The authors thank the anonymous reviewers for their useful feedback. This work was supported by the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund Institutional Links Grant, ID 352343575) projects. We also thank the annotators for"
    ]
  },
  {
    "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
    "answer": "They used two parallel corpora: \n\n1. Asian Scientific Paper Excerpt Corpus (ASPEC) \n2. NTCIR PatentMT Parallel Corpus. \n\nSpecifically, for ASPEC, they used only the first 1 million sentences sorted by sentence-alignment similarity.",
    "evidence": [
      "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity.",
      "of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor. Our experiments show that their method offers significant improvement in BLEU scores and alleviates the",
      "sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus. Note that sentences with more than 40",
      "that jointly trained forward translation and back-translation without pre-training. The RNN used in the experiments had 512 hidden units, 512 embedding units, 30,000 vocabulary size and 64 batch size. We used Adagrad (initial learning rate 0.01) for",
      "by a new decoder to enforce agreement between source and target sentences. In order to confirm the language independence of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using",
      "We used the attention-based NMT BIBREF2 as a baseline-NMT, the encoder-decoder-reconstructor BIBREF3 and the encoder-decoder-reconstructor that jointly trained forward translation and back-translation without pre-training. The RNN used in the experiments",
      "hidden units, 512 embedding units, 30,000 vocabulary size and 64 batch size. We used Adagrad (initial learning rate 0.01) for optimizing model parameters. We trained our model on GeForce GTX TITAN X GPU. Note that we set the hyper-parameter INLINEFORM0 on",
      "of Moses. Table TABREF14 shows the numbers of the sentences in each corpus. Note that sentences with more than 40 words were excluded from the training data.",
      "the original source sentences. In their method, after training the forward translation in a manner similar to the conventional attention-based NMT, they train a back-translation model from the hidden state of the decoder into the source sequence by a new",
      "Related Works"
    ]
  },
  {
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "answer": "According to the text, the biases found in the dataset (specifically Flickr30K and Flickr8K) include:\n\n1. **Racial bias**: The dataset appears to have a disproportionate number of white babies (504) compared to Asian (66) and black babies (36), suggesting that the dataset may be",
    "evidence": [
      "marking between the groups. We can take one step further and look at all the 697 pictures with the word `baby' in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased. I have manually",
      "out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased. I have manually categorized each of the baby images. There are 504 white, 66 asian, and 36 black babies. 73 images do not contain a baby, and 18",
      "Linguistic bias",
      "some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven",
      "are many ways in which a phrase can be biased, it is difficult to automatically detect bias from the data. So how should we deal with stereotype-driven descriptions?",
      "other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased? We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is",
      "dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",
      "dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K): “By asking people to describe the people, objects, scenes and activities that are shown in a picture",
      "for the data to be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions. This paper aims to give an overview of linguistic bias and",
      "problem is not with the lack of consistency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as"
    ]
  },
  {
    "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    "answer": "The size of the dataset is 23,700 queries, which includes:\n\n* 22,500 in-scope queries\n* 1,200 out-of-scope queries\n\nIt covers 150 intents, which can be grouped into 10 general domains.",
    "evidence": [
      "Dataset",
      "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the",
      "has seen little attention in analyses and evaluations of intent classification systems. This paper fills this gap by analyzing intent classification performance with a focus on out-of-scope handling. To do so, we constructed a new dataset with 23,700",
      "Dataset ::: In-Scope Data Collection",
      "Dataset ::: Out-of-Scope Data Collection",
      "intent classification performance with a focus on out-of-scope handling. To do so, we constructed a new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems. The queries cover 150",
      "of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases). BIBREF11 created datasets with constrained training data, but with very few intents, presenting a very different type of challenge. We also",
      "be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.",
      "performance drops slightly. The dataset presented in BIBREF10 has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases). BIBREF11",
      "improve on out-of-scope performance, yet such data is expensive and difficult to generate. We believe our analysis and dataset will lead to developing better, more robust dialog systems. All datasets introduced in this paper can be found at"
    ]
  },
  {
    "title": "Automatic Judgment Prediction via Legal Reading Comprehension",
    "answer": "According to the text, a real-world civil case dataset for Legal Reading Comprehension (LRC) is constructed for experiments. Specifically, the dataset is collected from China Judgments Online, and it consists of:\n\n* INLINEFORM0 cases in total\n* INLINEFORM1 cases for training\n* INLINEFORM2 cases each",
    "evidence": [
      "real-world dataset for experiments, and plan to publish it for further research. (3) Besides baselines from previous works, we also carry out comprehensive experiments comparing different existing deep neural network methods on our dataset. Supported by",
      "Experiments",
      "results on this dataset demonstrate that our model achieves significant improvement over state-of-the-art models. We will publish all source codes and datasets of this work on \\urlgithub.com for further research.",
      "also carry out comprehensive experiments comparing different existing deep neural network methods on our dataset. Supported by these experiments, improvements achieved by LRC prove to be robust.",
      "Dataset Construction for Evaluation",
      "semantic interactions among facts, pleas, and laws. In experiments, we construct a real-world civil case dataset for LRC. Experimental results on this dataset demonstrate that our model achieves significant improvement over state-of-the-art models. We",
      "Since none of the datasets from previous works have been published, we decide to build a new one. We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and",
      "re-formalize judgment prediction as Legal Reading Comprehension to better model the complementary inputs. (2) We construct a real-world dataset for experiments, and plan to publish it for further research. (3) Besides baselines from previous works, we",
      "Results and Analysis",
      ", INLINEFORM5 to INLINEFORM6 , INLINEFORM7 to INLINEFORM8 , batch size to 64. We employ precision, recall, F1 and accuracy for evaluation metrics. We repeat all the experiments for 10 times, and report the average results."
    ]
  },
  {
    "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
    "answer": "Based on the provided context, the core component for KBQA (Knowledge Base Question Answering) systems appears to be:\n\n**Relation Detection**\n\nThe text mentions that the proposed relation detection model plays a key role in the KBQA system, and that it helps achieve state-of-the-art accuracy for both single-relation and multi",
    "evidence": [
      "KBQA Enhanced by Relation Detection",
      "KBQA End-Task Results",
      "making the whole system easy to build. Following previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”). Then we generate the KB",
      ", BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2)",
      "but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",
      "the state-of-the-art. The third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the re-ranking step relies on the relation",
      "Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance,",
      "task, we also propose a simple KBQA implementation composed of two-step relation detection. Given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role",
      "does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art. The third block of the table details two ablation tests for the proposed components in our KBQA systems:",
      "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . For an input question, these systems typically generate a KB query, which can be executed"
    ]
  },
  {
    "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
    "answer": "The datasets used to evaluate this paper are:\n\n1. WN18 (a subset of WordNet)\n2. FB15K (a subset of Freebase)\n\nThese datasets are used for two benchmark tasks: \n\n1. Triplet Classification\n2. Link Prediction",
    "evidence": [
      "Datasets",
      "to integrate representations of structure and text into a unified architecture. Experiments show that our models outperform baseline by margin on link prediction and triplet classification tasks. Source codes of this paper will be available on Github.",
      "knowledge base relations to the same vector space and obtained substantial improvements. While releasing the current paper we discovered a paper by BIBREF30 proposing a similar model with attention mechanism which is evaluated on link prediction and",
      "Triplet Classification",
      "We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1",
      "WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.",
      "Experiment\nIn this section, we study the empirical performance of our proposed models on two benchmark tasks: triplet classification and link prediction.",
      "we discovered a paper by BIBREF30 proposing a similar model with attention mechanism which is evaluated on link prediction and triplet classification. However, our work encodes text description as a whole without explicit segmentation of sentences, which",
      "The evaluation results are reported under these two settings. We select the margin $\\gamma $ among $\\lbrace 1, 2\\rbrace $ , the embedding dimension $d$ among $\\lbrace 20, 50, 100\\rbrace $ , the regularization $\\eta $ among $\\lbrace 0, 1E{-5},",
      "and triplet classification show that our joint models can handle the sparsity problem well and outperform the baseline method on all metrics with a large margin. Our contributions in this paper are summarized as follows."
    ]
  },
  {
    "title": "State-of-the-Art Vietnamese Word Segmentation",
    "answer": "Several approaches have been applied to solve word segmentation in Vietnamese, including:\n\n1. **Weighted Finite State Transducer (WFST) approach**: Used by Dinh et al. to start the task of Vietnamese word segmentation.\n2. **Neural Network approach**: Also used by Dinh et al. for Vietnamese word segmentation.\n",
    "evidence": [
      "techniques to improve the accuracy for Vietnamese word segmentation. According to our observation, this study also reports a few of achivements and limitations in existing Vietnamese word segmentation systems.",
      "of survey of word segmentation studies on Asian languages and Vietnamese as well. This paper aims reviewing state-of-the-art word segmentation approaches and systems applying for Vietnamese. This study will be a foundation for studies on Vietnamese word",
      "and systems for word segmentation in Vietnamese. To have an overview of all stages from building corpora to developing toolkits, we discuss building the corpus stage, approaches applied to solve the word segmentation and existing toolkits to segment words",
      "several studies about the Vietnamese word segmentation task over the last decade. Dinh et al. started this task with Weighted Finite State Transducer (WFST) approach and Neural Network approach BIBREF9 . In addition, machine learning approaches are",
      "Word segmentation is the first step of any tasks in Vietnamese language processing. This paper reviews stateof-the-art approaches and systems for word segmentation in Vietnamese. To have an overview of all stages from building corpora to developing",
      "This study reviewed state-of-the-art approaches and systems of Vietnamese word segmentation. The review pointed out common features and methods used in Vietnamese word segmentation studies. This study also had an evaluation of the existing Vietnamese word",
      "three tags (B-IO). This technique is applied for Vietnamese word segmentation in several studies BIBREF7 , BIBREF24 . Nguyen et al. applied on a segmented corpus of 8,000 sentences and got the result at 94.05% while Ngo et al. used it with 45,531",
      "labels. Many researchers applied Hidden Markov model to solve Vietnamese word segmentation such as in BIBREF8 , BIBREF20 and so on. N-gram language modeling applied to estimate probabilities for each word segmentation solution BIBREF21 . The result of",
      "segmentation and other following Vietnamese tasks as well, such as part-of-speech tagger, chunker, or parser systems. There are several studies about the Vietnamese word segmentation task over the last decade. Dinh et al. started this task with Weighted",
      "al. listed out several Vietnamese word segmentation inconsistencies in the corpus based on POS information and n-gram sequences BIBREF14 . Currently, there are at least three available word segmentation corpus used in Vietnamese word segmentation studies"
    ]
  },
  {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "answer": "The baselines mentioned in the text are:\n\n1. **Transformer**: a state-of-the-art NMT (Neural Machine Translation) model proposed in BIBREF8, which achieves good results for sentence pairs in some languages.\n2. **Strong NMT baseline**: a baseline NMT model that is improved by 16%",
    "evidence": [
      "from the test set of Italian queries, respectively. It is evident that our model achieves better balance compared to baseline transformer, except for a very few cases. Given a query INLINEFORM0 , consider INLINEFORM1 as the set of terms from human",
      "queries, respectively. Figure FIGREF16 shows the balance values for transformer and our model for a random sample of 20 queries from the test set of Italian queries, respectively. It is evident that our model achieves better balance compared to baseline",
      "likely in TC?. We show that training a strong baseline NMT with RAT roughly achieves 16% improvement over the baseline. Using a qualitative analysis, we further show that RAT works as a regularizer and prohibits NMT to overfit to TC vocabulary.",
      "We train NMT with RAT to achieve better query translations. We improve a recently proposed NMT baseline, Transformer, that achieves state-of-the-art results for sentence pairs in some languages BIBREF8 . We discuss Transformer, RAT, and our multi-task",
      "architecture that achieves 16% improvement over a strong NMT baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization",
      "question: In the decoding stage, can we restrict an NMT model so that it does not only generate terms that are highly likely in TC?. We show that training a strong baseline NMT with RAT roughly achieves 16% improvement over the baseline. Using a",
      "is drawn from a context window following BIBREF6 . In the figure, INLINEFORM0 and INLINEFORM1 represents the top document retrieved against INLINEFORM2 . The shuffler component shuffles INLINEFORM3 and INLINEFORM4 and creates (context, pivot) pairs. After",
      "data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem with our multi-task learning architecture that achieves 16% improvement over a strong NMT baseline on Italian-English query-document dataset. We show using",
      "a batch of sentence pairs, INLINEFORM0 . We also use a similar loss function to train word embedding over a set of context ( INLINEFORM1 ) and pivot ( INLINEFORM2 ) pairs formed using INLINEFORM3 as query to retrieve INLINEFORM4 using Query Likelihood",
      "or recurrent neural network based models: (a) reduced computational complexity of each layer, (b) parallel computation, and (c) path length between long-range dependencies."
    ]
  },
  {
    "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
    "answer": "The invertibility condition is that the neural projector must satisfy two requirements: \n\n1. INLINEFORM0 \n2. INLINEFORM1 exists.\n\nIn other words, the neural projector must be invertible, meaning that it has an inverse transformation, and this is achieved by ensuring that the Jacobian determinant of the transformation is non-zero (",
    "evidence": [
      "Learning with Invertibility",
      "In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible",
      "by BIBREF16 , which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property). From Eq. ( EQREF22 ) we know that only INLINEFORM0 is required for",
      "cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments",
      "Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep",
      ": INLINEFORM3  By using the change of variable rule to the integration, which allows the integration variable INLINEFORM0 to be replaced by INLINEFORM1 , the marginal emission factor can be computed in closed-form when the invertibility condition is",
      "learning may be difficult. In sec:opt, we address this issue by constraining INLINEFORM3 to be invertible, and show that this constraint enables tractable exact inference and marginal likelihood computation.",
      "determinant is equal to zero, which means the projection is non-invertible and thus information is being lost through the projection. Such “information preserving” regularization is crucial during optimization, otherwise the trivial solution of always",
      "Jacobian determinant (and thus the invertibility property). From Eq. ( EQREF22 ) we know that only INLINEFORM0 is required for accomplishing learning and inference; we never need to explicitly construct INLINEFORM1 . Thus, we directly define the",
      "behavior of the projection. Maximizing it can be thought of as preventing information loss. In the extreme case, the Jacobian determinant is equal to zero, which means the projection is non-invertible and thus information is being lost through the"
    ]
  },
  {
    "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
    "answer": "They compare lexicons by:\n\n1. Comparing crowd evaluations to those of expert linguists to assess the overall lexicon quality and the evaluation capabilities of the crowd.\n2. Evaluating existing annotations to determine the lexicon quality.\n3. Comparing evaluations from linguistic experts and the crowd itself to highlight the crowd's performance on evaluation.\n\n",
    "evidence": [
      "and doesn't require experts or gold standards. We also compare crowd and expert evaluations of the lexicon, to assess the overall lexicon quality, and the evaluation capabilities of the crowd.",
      "existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.",
      "that go through a filtering process. If the answers are considered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks,",
      "evaluate the whole corpus. However, the uniformity of expert judgement is replaced with the diversity and mass of contributors. The corpus may be limiting the term groups in the lexicon to specific domain-specific subjects. Comparisons with existing",
      "evaluation wise) lexicon acquisition process. Therefore, to highlight crowd's performance on evaluation, we compare evaluations from linguistic experts and the crowd itself.",
      "Lexicon Analysis",
      "lexicon acquisition could be improved on an individual basis. Spell check can include spelling recommendations, filtering could incorporate rewarding and penalties, evaluation process can include experts and so on. Crowd diversity in the annotation and",
      "created by experts, such as linguists and behavioural scientists, with strict rules. Lexicon evaluation was also performed by experts or gold standards. In our paper, we propose a crowdsourcing method for lexicon acquisition, which is scalable,",
      "doesn't require gold standards. Our goal is to maintain an end to end automated work-flow for a crowdsourced (annotation and evaluation wise) lexicon acquisition process. Therefore, to highlight crowd's performance on evaluation, we compare evaluations",
      "by experts or gold standards. In our paper, we propose a crowdsourcing method for lexicon acquisition, which is scalable, cost-effective, and doesn't require experts or gold standards. We also compare crowd and expert evaluations of the lexicon, to assess"
    ]
  },
  {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "answer": "KAR (Knowledge Aided Reader) is an end-to-end MRC (Machine Reading Comprehension) model.",
    "evidence": [
      "As shown in Figure FIGREF7 , KAR is an end-to-end MRC model consisting of five layers: Lexicon Embedding Layer. This layer maps the words to the lexicon embeddings. The lexicon embedding of each word is composed of its word embedding and character",
      "only a subset of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",
      "MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models,",
      "mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset ( INLINEFORM0 – INLINEFORM1 ) of the training examples are",
      "mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms",
      "named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and",
      "which explicitly uses the general knowledge to assist its attention mechanisms. Experimental results show that KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data",
      "In this section, we elaborate our MRC model: Knowledge Aided Reader (KAR). The key components of most existing MRC models are their attention mechanisms BIBREF13 , which are aimed at fusing the associated representations of each given passage-question",
      "of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state-of-the-art MRC models by a large margin. That is to say, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise",
      "According to the experimental results, KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. The reasons for these achievements, we"
    ]
  },
  {
    "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "answer": "According to the text, semantically related words take larger values along a **specified dimension** or an **associated dimension** ( INLINEFORM5 ). \n\nMore specifically, the text states that the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word",
    "evidence": [
      "dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic",
      "that exist in the corresponding word-group obtained from Roget's Thesaurus (and are thus explicitly forced to achieve increased dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence",
      "dimensions of word embeddings. Specifically, the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word-group under consideration. This",
      "the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are",
      "list of words belonging to the concept word-group for that dimension. We also present the list of words with the greatest dimension value for the dimensions 11, 13, 16, 31, 36, 39, 41, 43 and 79 in Table TABREF11 . These dimensions are aligned/imparted",
      "to achieve deliberately increased values along an associated dimension INLINEFORM5 . The relative weight of the second term is controlled by the parameter INLINEFORM6 . The simultaneous minimization of both objectives ensures that words that are similar",
      "dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts. Therefore, we impart interpretability to the word embedding by",
      "markers for the original and the proposed algorithms, respectively. Additionally, the top-50 words that achieve the greatest 32nd dimension values among the considered 1000 words are emphasized with enlarged markers, along with text annotations. In the",
      "their word embedding vectors coming from the original GloVe algorithm. The dimension values are denoted with blue and red/green markers for the original and the proposed algorithms, respectively. Additionally, the top-50 words that achieve the greatest",
      "increased interpretability and preservation of the intrinsic semantic structure. An important point was that, while it is expected for words that are already included in the concept word-groups to be aligned together since their dimensions are directly"
    ]
  },
  {
    "title": "Stance Detection in Turkish Tweets",
    "answer": "The targets are:\n\n1. Galatasaray (Target-1)\n2. Fenerbahçe (Target-2)\n\nBoth are popular football clubs in Turkey.",
    "evidence": [
      "are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the",
      "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely",
      "Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the",
      "the corresponding annotations are made publicly available. The domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included. We also provide the evaluation results of SVM classifiers (for each target) on",
      "between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of",
      "specified in the text or not. In this study, we present a stance-annotated tweet data set in Turkish where the targets of the annotated stances are two popular sports clubs in Turkey. The corresponding annotations are made publicly-available for research",
      "finer-grained classes can be considered in further annotation studies. We should also note that in a few tweets, the target of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised.",
      "clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering",
      "the text author, the stance expressed for a particular target is investigated in stance detection. In this paper, we present a stance detection tweet data set for Turkish comprising stance annotations of these tweets for two popular sports clubs as",
      "a stance detection tweet data set for Turkish comprising stance annotations of these tweets for two popular sports clubs as targets. Additionally, we provide the evaluation results of SVM classifiers for each target on this data set, where the classifiers"
    ]
  },
  {
    "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts",
    "answer": "The EEG data comes from 14 participants.",
    "evidence": [
      "pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only",
      "scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded",
      "which mostly deal with subject-dependent classification of EEG into discrete vowel or word labels, this work investigates a subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator",
      "intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is",
      "subject-invariant mapping of EEG data with different phonological categories, varying widely in terms of underlying articulator motions (eg: involvement or non-involvement of lips and velum, variation of tongue movements etc). Our model takes an advantage",
      "for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute",
      "Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space. In fact, our",
      "We performed two sets of experiments with the single-trial EEG data. In PHASE-ONE, our goals was to identify the best architectures and hyperparameters for our networks with a reasonable number of runs. For PHASE-ONE, we randomly shuffled and divided the",
      "that the network is able to find better discriminative information from the EEG data beyond random decisions. The maximum above-chance accuracy (75.92%) is recorded for presence/absence of the vowel task and the minimum (49.14%) is recorded for the",
      "To this end, we develop our mixed neural network architecture composed of three supervised and a single unsupervised learning step, discussed in the next subsections and shown in Fig. FIGREF1 . We formulate the problem of categorizing EEG data based on"
    ]
  },
  {
    "title": "Towards Understanding Neural Machine Translation with Word Importance",
    "answer": "According to the text, they test their word importance approach on two representative model architectures. No specific names of the architectures are mentioned, but it is stated that the approach consistently outperforms its attention counterpart across model architectures and language pairs.",
    "evidence": [
      "of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures,",
      "experiments are conducted on the test dataset, and we estimate the input word importance using the model generated hypotheses. In the following experiments, we compare IG (Attribution) with several black-box methods (i.e., Content, Frequency, Attention)",
      "we exploit the intermediate gradients to better estimate word importance, which consistently outperforms its attention counterpart across model architectures and language pairs.",
      "to the NMT generations. We categorize the methods of word importance estimation into two types: black-box methods without the knowledge of the model and white-box methods that have access to the model internal information (e.g., parameters and gradients).",
      "estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve NMT models. First, we leverage the word importance to identify input words that are under-translated by NMT models. Experimental",
      "of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve",
      "methods in estimating the word importance. Further analyses show that important words are of distinct syntactic categories on different language pairs, which might support the viewpoint that essential inductive bias should be introduced into the model",
      "estimating word importance. We find that word importance is useful for understanding NMT by identifying under-translated words. We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language",
      "models. First, we leverage the word importance to identify input words that are under-translated by NMT models. Experimental results show that the gradient-based approach outperforms both the best black-box method and other comparative methods. Second, we",
      "In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section SECREF33) and design better architectures for specific languages (Section SECREF37). Due to the space limitation, we only analyze the"
    ]
  },
  {
    "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
    "answer": "By taking the first principal component (PC) of its contextualized representations in a given layer.",
    "evidence": [
      "As noted earlier, we can create static embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table TABREF34, we plot the performance of these PC static embeddings on several",
      "for each word, a notable problem with static word embeddings is that all senses of a polysemous word must share a single vector.",
      "We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding. This means that even in the best-case scenario, in all layers of all",
      "representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of",
      "component. It gives us an upper bound on how well a static embedding could replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\\textit {MEV}_\\ell (w) = 1$,",
      "what extent they can be replaced with static word embeddings, if at all. Our work in this paper is thus markedly different from most dissections of contextualized representations. It is more similar to BIBREF13, which studied the geometry of static word",
      "Findings ::: Static vs. Contextualized ::: On average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding.",
      "implicitly factorize a word-context matrix containing a co-occurrence statistic BIBREF7, BIBREF8. Because they create a single representation for each word, a notable problem with static word embeddings is that all senses of a polysemous word must share a",
      "static embeddings would be a poor replacement for contextualized ones. Still, static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector",
      "The application of deep learning methods to NLP is made possible by representing words as vectors in a low-dimensional continuous space. Traditionally, these word embeddings were static: each word had a single vector, regardless of context BIBREF0,"
    ]
  },
  {
    "title": "Paraphrase-Supervised Models of Compositionality",
    "answer": "They compare against:\n\n1. A state-of-the-art phrase-based MT system (§ SECREF38 )\n2. A baseline that marks translation rules with indicator functions but without the proposed features.\n\nNo specific names of commercial or well-known translation systems are mentioned, but rather a research-oriented phrase-based MT system and a baseline variant.",
    "evidence": [
      "Machine Translation",
      "are also evaluated for their impact on a machine translation system where we show improvements in translation quality, demonstrating that compositionality in interpretation correlates with compositionality in translation.",
      "it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and",
      "the translation rule are unigrams), and correspondingly the second feature indicates if the translation rule has been scored. Therefore, an appropriate additional baseline would be to mark translation rules with these indicator functions but without the",
      "the translations produced by a state-of-the-art phrase-based MT system (§ SECREF38 ). The word vectors used in all of our experiments were produced by word2vec using the skip-gram model with 20 negative samples, a context window size of 10, a minimum",
      "important. To this end, we consider the problem of machine translation, operating under the hypothesis that sentences which express their meaning non-compositionally should also translate non-compositionally. Modern phrase-based translation systems are",
      "Phrasal Similarity",
      "to the translation system that a phrase is non-compositional, it should be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT",
      "can be used to learn partial semantic supervision that matches previous techniques in intrinsic evaluation tasks. Our approaches are also evaluated for their impact on a machine translation system where we show improvements in translation quality,",
      "scores in translation. Thus, our work is more similar to Xiong2010, who propose maximum entropy classifiers that mark positions between words in a sentence as being a phrase boundary or not, and integrate these scores as additional features in an MT"
    ]
  },
  {
    "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    "answer": "According to the text, the models mentioned use the following matrix factorization methods:\n\n1. LexVec: stochastic gradient descent to factorize the PPMI-weighted word-context co-occurrence matrix.\n2. PPMI-SVD: SVD ( Singular Value Decomposition) to factorize the PPMI matrix.\n3. GloVe:",
    "evidence": [
      "Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, BIBREF26 and BIBREF27 retrofit",
      "morphological segmentation in learning word representations BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and",
      "The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.  $$PPMI_{wc} = max(0, \\log \\frac{M_{wc} \\; M_{**}}{ M_{w*} \\; M_{*c} })$$   (Eq. 3)  where $M$ is the word-context co-occurrence",
      "matrix BIBREF1 . The most well-known predictive model, which has become eponymous with word embedding, is word2vec BIBREF2 . Popular counting models include PPMI-SVD BIBREF3 , GloVe BIBREF4 , and LexVec BIBREF5 . These models all learn word-level",
      "the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations BIBREF15 , BIBREF16 ,",
      "matrix constructed by sliding a window of fixed size centered over every target word  $w$ in the subsampled BIBREF2 training corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair). LexVec",
      "four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is",
      "subword information into counting models using a strategy similar to fastText. We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , but the",
      "corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair). LexVec adjusts the PPMI matrix using context distribution smoothing BIBREF3 . With the PPMI matrix calculated, the sliding window",
      "which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix BIBREF1 . The most well-known predictive model, which has become eponymous with word embedding, is word2vec BIBREF2 ."
    ]
  },
  {
    "title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis",
    "answer": "The recent abstractive summarization method mentioned in this paper is:\n\nPointer Networks with Coverage Mechanism (PG-net) \n\nThis is mentioned in the text as: \"a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0.\"",
    "evidence": [
      "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive",
      "summarization using the proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.",
      "domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art",
      "We explored improving the performance of neural abstractive summarizers when applied to the low resource domain of student reflections using three approaches: domain transfer, data synthesis and the combination of both. For domain transfer, state of the",
      "Abstractive Summarization. Abstractive summarization aims to generate coherent summaries with high readability, and has seen increasing interest and improved performance due to the emergence of seq2seq models BIBREF8 and attention mechanisms BIBREF9. For",
      "using three approaches: domain transfer, data synthesis and the combination of both. For domain transfer, state of the art abstractive summarization model was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student",
      "a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0. To experiment with domain transfer, the model was pretrained using the CNN/DM dataset, then fine tuned using the student reflection dataset (see the",
      "art abstractive summarization model was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student reflections). The process of tuning improved ROUGE scores on the student reflection data, and at the same time produced more",
      "for student reflections. To improve performance in low resource domains, we explore three directions. First, we explore domain transfer for abstractive summarization. While domain transfer is not new, compared to prior summarization studies BIBREF6,",
      "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large"
    ]
  },
  {
    "title": "Revisiting Summarization Evaluation for Scientific Articles",
    "answer": "The common belief that this paper refutes is that ROUGE (a metric for evaluating summarization systems) is a reliable metric for evaluating scientific summaries. Specifically, the paper argues that ROUGE is not as effective in evaluating scientific summaries as it is in evaluating summaries in other domains, such as newswire data, because scientific",
    "evidence": [
      "metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual",
      "from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating",
      "dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between",
      "terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an",
      "ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is",
      "the small sample size. These results confirm our initial hypothesis that Rouge is not accurate estimator of the quality of the summary in scientific summarization. We attribute this to the differences of scientific summarization with general domain",
      "also show sub-optimal correlations. Note that INLINEFORM2 and INLINEFORM3 correlations are more reliable in our setup due to the small sample size. These results confirm our initial hypothesis that Rouge is not accurate estimator of the quality of the",
      "summaries. When humans summarize a relatively long research paper, they might use different terminology and paraphrasing. Therefore, Rouge which only relies on term matching between a candidate and a gold summary, is not accurate in quantifying the",
      "answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak,",
      "data and are intrinsically very different than other summarization tasks such as summarization of scientific papers. We argue that Rouge is not the best metric for all summarization tasks and we propose an alternative metric for evaluation of scientific"
    ]
  },
  {
    "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
    "answer": "The baseline systems are systems trained only on parallel data, without synthetic data. Specifically, they are:\n\n* For English-Czech, a model trained on the complete WMT16 parallel training set (including CzEng1.6) until convergence on the heldout set (newstest2014).\n* For other language pairs",
    "evidence": [
      "Baseline System",
      "BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.",
      "of parallel and synthetic data, compared to the baseline that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3–1.7 Bleu). Our submitted system includes reranking of the 50-best output of the",
      "and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8",
      "that the final system (+r2l reranking) improves by 0.7–1.0 Bleu on the ensemble of 4, and 4.3–4.9 on the baseline. For Czech INLINEFORM0 English the training process was similar to the above, except that we created the synthetic training data",
      "For English INLINEFORM0 Czech, we trained our baseline model on the complete WMT16 parallel training set (including CzEng 1.6pre BIBREF9 ), until we observed convergence on our heldout set (newstest2014). This took approximately 1M minibatches, or 3",
      "Table TABREF13 shows results for English INLINEFORM0 German. We observe improvements of 3.4–5.7 Bleu from training with a mix of parallel and synthetic data, compared to the baseline that is only trained on parallel data. Using an ensemble of the last 4",
      "INLINEFORM0 Czech, English INLINEFORM1 German, English INLINEFORM2 Romanian and English INLINEFORM3 Russian. Our systems are based on an attentional encoder-decoder BIBREF0 , using BPE subword segmentation for open-vocabulary translation with a fixed",
      "Czech INLINEFORM2 English was an ensemble of 8 systems – the last 4 save-points of the 10M synthetic data run, and the last 4 save-points of the 7.5M run. We show this as ensemble8 in Table TABREF15 , and the +synthetic results are on the last (i.e. 10M)",
      "of 4.1–5.1 Bleu. for English INLINEFORM0 Romanian, we found that the best single system outperformed the ensemble of the last 4 checkpoints on dev, and we thus submitted the best single system as primary system."
    ]
  },
  {
    "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
    "answer": "According to the text, they show there is space for further improvement by:\n\n* Conducting their own human study, where they test humans on a random subset of validation questions that their model (psr ensemble) could not answer.\n* Comparing their model's performance to human performance, implying that if a system can be designed to",
    "evidence": [
      "Possible Directions for Improvements",
      "However we hypothesized that the system is still not achieving its full potential so we decided to examine the room for improvement in our own small human study.",
      "architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.",
      "After adding more data we have the performance on the CBT validation and test datasets soaring. However is there still potential for much further growth beyond the results we have observed? We decided to explore the remaining space for improvement on the",
      ". However in the final section we show in our own human study that there is still room for improvement on the CBT beyond the performance of our model.",
      "for improvement on the algorithmic side. The other possibility to improve performance is simply to use more training data. The importance of training data was highlighted by the frequently quoted Mercer's statement that “There is no data like more data.”",
      "for much further growth beyond the results we have observed? We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could",
      "system actually capable of helping humans, we want the model to use all available resources as efficiently as possible. Given that we believe that if the community is striving to bring the performance as far as possible, it should move its work to larger",
      "also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement. A system that would answer correctly every time when either our ensemble or human answered correctly would achieve",
      "in this field, most research was performed on unnecessarily small datasets. As a gentle reminder to the community we have shown that simply infusing a model with more data can yield performance improvements of up to INLINEFORM0 where several attempts to"
    ]
  },
  {
    "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
    "answer": "According to the text, the keyword-specific expectation is elicited from the crowd through an iterative process where:\n\n1. Crowd workers estimate keyword-specific expectations.\n2. The model is trained using expectation regularization.\n3. The model is used to select keyword-related microposts where the model's prediction disagrees with the crowd's expectation",
    "evidence": [
      "leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the",
      "and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More",
      "loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model",
      "expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation",
      "keyword-specific expectations, which can then be generalized to help classify microposts containing the same keyword, thus amplifying the utility of the crowd. Our work is further connected to the topic of interpretability and transparency of machine",
      "at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's",
      "expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus",
      "To learn the keyword-specific expectation $e^{(t)}$ and the crowd worker reliability $\\pi ^{(n)}$ ($1\\le n\\le N$), we model the likelihood of the crowd-contributed labels $\\mathbf {A}$ as a function of these parameters. In this context, we view the",
      "detection models. Our approach takes advantage of the disagreement between the crowd and the model to discover informative keywords and leverages the joint power of the crowd and the model in expectation inference. We evaluated our approach on real-world",
      "The E-step and the crowd parameter update in the M-step are similar to the Dawid-Skene model BIBREF14. The keyword expectation is inferred by taking into account both the crowd-contributed labels and the model prediction: The parameter of the target model"
    ]
  },
  {
    "title": "ARAML: A Stable Adversarial Training Framework for Text Generation",
    "answer": "The GAN models used as baselines to compare against are:\n\n1. SeqGAN\n2. LeakGAN\n3. MaliGAN\n4. DialogGAN\n5. DPGAN \n\nThese models were compared against the ARAML model. Note that LeakGAN has a different generator architecture, consisting of a hierarchical LSTM unit",
    "evidence": [
      "in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the",
      "the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original",
      "as input, we chose the following baselines respectively: MLE: a RNN model trained with MLE objective BIBREF4 . Its extension, Seq2Seq, can work on the dialogue dataset BIBREF2 . SeqGAN: The first text GAN model that updates the generator with policy",
      "https://github.com/kepei1106/ARAML. As for the details of the baselines, the generators of all the baselines except LeakGAN are the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation",
      "setting as the generators, and is pre-trained on the training set of each dataset. The codes and the datasets are available at https://github.com/kepei1106/ARAML. As for the details of the baselines, the generators of all the baselines except LeakGAN are",
      "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively: MLE: a RNN model trained with MLE objective BIBREF4 . Its extension,",
      "based discriminator and regards cross-entropy as rewards BIBREF13 . Note that MLE, SeqGAN, LeakGAN, MaliGAN and IRL are the baselines on COCO and EMNLP2017 WMT, while MLE, RAML, DialogGAN, and DPGAN on WeiboDial. The original codes are used to test the",
      "IRL, MaliGAN and ARAML, respectively. Then, we presented the forward/reverse perplexity in the training process in Figure FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics.",
      "reverse perplexity, our model ARAML beats other baselines, showing that our model can fit the data distribution better. Other GANs, particularly LeakGAN, obtain high reverse perplexity due to mode collapse BIBREF12 , thus they only capture limited fluent",
      "FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics. Although LeakGAN reaches the best forward perplexity, its standard deviation is extremely large and it performs badly in reverse"
    ]
  },
  {
    "title": "Predicting the Industry of Users on Social Media",
    "answer": "Based on the text, it appears that the researchers looked at:\n\n1. Twitter (mentioned in the context of Hecht et al. predicting Twitter users' location)\n2. Sina Weibo (mentioned as a platform where users have been verified, introducing a potential bias in the resulting dataset)\n3. Blog",
    "evidence": [
      "number of researchers sought to predict the social media users' age and/or gender BIBREF14 , BIBREF15 , BIBREF16 , while others targeted and analyzed the ethnicity, nationality, and race of the users BIBREF17 , BIBREF18 , BIBREF19 . One of the profile",
      "social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the expressed emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13 . At the same",
      "emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13 . At the same time, a number of researchers sought to predict the social media users' age and/or gender BIBREF14 , BIBREF15 , BIBREF16 , while others",
      "In this paper, we examined the task of predicting a social media user's industry. We introduced an annotated dataset of over 20,000 blog users and applied a content-based classifier in conjunction with two feature selection methods for an overall",
      "targeted and analyzed the ethnicity, nationality, and race of the users BIBREF17 , BIBREF18 , BIBREF19 . One of the profile fields that has drawn a great deal of attention is the location of a user. Among others, Hecht et al. Hecht11 predicted Twitter",
      "and refine models of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the",
      "occupational classes highlighting the effect of homophily in user interactions. This work examined only users that have been verified by the Sina Weibo platform, introducing a potential bias in the resulting dataset. Finally, Preotiuc-Pietro et al.",
      ", and recommender systems BIBREF2 . However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3 . To uncloak this information, a number of studies have utilized social media users' footprints to",
      "Alongside the wide adoption of social media by the public, researchers have been leveraging the newly available data to create and refine models of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile",
      "deliberately omitted BIBREF3 . To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles. This paper explores the potential of predicting a user's industry –the aggregate of enterprises in"
    ]
  },
  {
    "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
    "answer": "The model, AEM, overcomes the assumption that all words in a document are generated from a single event by not making this assumption. Unlike previous models, such as LEM and DPEMM, AEM does not assume that a document is generated from a single event. Instead, it can handle documents that describe multiple events",
    "evidence": [
      "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short",
      "approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical",
      "(1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which",
      "Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple",
      "approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al.",
      "significantly better than LEM and DPEMM. It improves upon LEM by 15.5% and upon DPEMM by more than 30% in F-measure. This is because: (1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable",
      "applications, none of these approaches tackles open-domain event extraction from online texts. We propose a novel GAN-based event extraction model called AEM. Compared with the previous models, AEM has the following differences: (1) Unlike most GAN-based",
      "(1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable for long text such as news articles; (2) DPEMM generates too many irrelevant events which leads to a very low precision score. Overall,",
      "that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the",
      "and the document-level event-related word distributions (entity, location, keyword and date). In other words, with an event distribution INLINEFORM1 as input, INLINEFORM2 could generate the corresponding entity distribution, location distribution, keyword"
    ]
  },
  {
    "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
    "answer": "MSD prediction refers to the task of predicting the Morpho-Syntactic Description (MSD) tag of a target form, which is a sequence of components that describe the grammatical properties of a word, such as part of speech, case, number, and person.\n\nIn the context of the provided text, MSD prediction",
    "evidence": [
      "MSD prediction",
      "the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are generated with an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components,",
      "Figure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above. Accuracy here is generally higher than on the main task, with the multilingual finetuned setup for Spanish and the monolingual setup for",
      "from multilingual training, while MSD prediction suffers greatly. Notice that the magnitude of change is not always the same, however, even when the general direction matches: for ru, for example, multilingual training benefits inflection much more than",
      "meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are",
      "the high results we obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for four languages (de, en, es, and sv) the effect is positive, while for the rest it is negative. We consider this to be an issue of",
      "overall accuracy of 49.87 on Task 2 by introducing three augmentations to the provided baseline system: (1) We use an LSTM to encode the entire available context; (2) We employ a multi-task learning approach with the auxiliary objective of MSD prediction;",
      "encoding the full context with an LSTM highly enhances the performance of the model, by 11.15% on average. This observation explains the high results we obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for",
      "in benefits MSD prediction, even though the MSD decoder is the only component that is actually shared between languages. This observation illustrates the two-fold effect of multi-task training: an auxiliary task can either inform the main task through the",
      "(1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion."
    ]
  },
  {
    "title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion",
    "answer": "The dataset used to build their model consists of 1,160 physician logs of Medical ICU admission requests at a tertiary care center affiliated to Mount Sanai, collected prospectively over one year. Additionally, they also utilize task-oriented resources such as:\n\n* Wikipedia articles of candidates\n* Medical scientific papers\n* A large biomedical",
    "evidence": [
      "Baseline Models",
      "available. Alternatively, we use the following baselines to compare with our approach. Rating: This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base. Raw Input embeddings: We trained word embeddings only",
      "training data. And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. Rating: This baseline model chooses",
      "Data and Evaluation Metrics",
      "from the Raw Input embeddings baseline, we use the embedding trained from a large biomedical data collection that includes knowledge bases like PubMed and PMC and a Wikipedia dump of biomedical related articles BIBREF8 for semantic similarity computation.",
      "information largely benefits the abbreviation expansion model. Compared with our approach, embeddings trained only from the ICU texts do not significantly contribute to the performance over the rating baseline. The reason is that the size of data for",
      "from the 1,160 raw ICU texts and we choose the most semantically related candidate as the answer. General embeddings: Different from the Raw Input embeddings baseline, we use the embedding trained from a large biomedical data collection that includes",
      "abundant and easily-accessible task-oriented resources to enrich our dataset for training embeddings. To the best of our knowledge, we are the first to apply word embeddings to this task. Experimental results show that the embeddings trained on the",
      "from domain-specific knowledge base as candidates. We train word embeddings using the clinical notes data with task-oriented resources such as Wikipedia articles of candidates and medical scientific papers and compute the semantic similarity between an",
      "The clinical notes we used for the experiment are provided by domain experts, consisting of 1,160 physician logs of Medical ICU admission requests at a tertiary care center affiliated to Mount Sanai. Prospectively collected over one year, these"
    ]
  },
  {
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "answer": "The nine types of user reactions are:\n\n1. Answer\n2. Elaboration\n3. Question\n4. Agreement\n5. Appreciation\n6. Disagreement\n7. Humor\n8. Negative reaction \n9. (Implicitly) None of the given labels or \"other\"",
    "evidence": [
      "In the current work, we have presented a content-based model that classifies user reactions into one of nine types, such as answer, elaboration, and question, etc., and a large-scale analysis of Twitter posts and Reddit comments in response to content",
      "In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call",
      "across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction",
      "into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant",
      "the initial reactions of the users. Other work has focused on the language of misinformation in social media BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 to detect types of deceptive news. As an alternative to studying newsworthy events one",
      "Using these labels, we examine how often response types occur when users react to each type of news source. For clarity, we report the five most frequently occurring reaction types (expressed in at least 5% of reactions within each source type) and",
      "we report the five most frequently occurring reaction types (expressed in at least 5% of reactions within each source type) and compare the distributions of reaction types for each type of news source. To examine whether users react to content from",
      "For both Twitter and Reddit datasets, we found that the primary reaction types were answer, appreciation, elaboration, question, or “other” (no label was predicted). Figure FIGREF13 illustrates the distribution of reaction types among Reddit comments (top",
      ", BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 to detect types of deceptive news. As an alternative to studying newsworthy events one at a time BIBREF10 , the current work applies linguistically-infused models to predict user reactions to deceptive and trusted",
      "of the given type (i.e., trusted, all deceptive, and deceptive excluding disinformation sources). For Twitter, we report clear differences in user reactions to trusted vs. deceptive sources. Deceptive (including disinformation) sources have a much higher"
    ]
  },
  {
    "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
    "answer": "According to the text, there are 2 sources of error signals mentioned that are for supervised tasks, and it is stated that the model has \"3 sources of error signals - one for each task\" and that \"each successive task is supervised on the next layer\". However, one of the tasks is an unsupervised auxiliary task",
    "evidence": [
      "we update both representations to maintain a coherent view of the world. By incorporating an unsupervised auxiliary task (e.g. plank2016multilingual) as the most senior layer we can use this structure for semi-supervised learning - the error on the",
      "layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F β=1 for Chunking compared to a plausible baseline.",
      "data to see whether our semi-supervised approach makes our models more robust (Tables 3 and 4 ). Here, we find variable but consistent improvement in the performance of our tasks even at 1 % of the original training data.",
      "Supervision of Multiple Tasks\nOur model has 3 sources of error signals - one for each task. Since each task is categorical we use the discrete cross entropy to calculate the loss for each task: $",
      "unsupervised tasks penalises junior tasks when their representations and predictions are not consistent. It is the aim of this paper to demonstrate that organising a network in such a way can improve performance. To that end, although we do not achieve",
      "(e.g. plank2016multilingual) as the most senior layer we can use this structure for semi-supervised learning - the error on the unsupervised tasks penalises junior tasks when their representations and predictions are not consistent. It is the aim of this",
      "for junior downstream tasks, a form of semi-supervised learning. Arguably, there is a two-way relationship between each pair of tasks. Following work such as sogaard2016deep, that exploits such hierarchies in a fully supervised setting, we represent this",
      "of incorporating junior tasks in prediction. Our neural network has one hidden layer, after which each successive task is supervised on the next layer. In addition, we add skip connections from the hidden layer to the senior supervised layers to allow",
      "receive new information from one task that is inconsistent with with our internal representation of a junior task we update both representations to maintain a coherent view of the world. By incorporating an unsupervised auxiliary task (e.g.",
      "by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows"
    ]
  },
  {
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "answer": "Based on the provided text, some interesting correlations observed are:\n\n1. **Yoga-Veganism**: The data shows a noticeable correlation between yoga and veganism, suggesting that people who practice yoga are more likely to follow a vegan diet.\n2. **Women-Yoga**: The data also suggests a correlation between women and",
    "evidence": [
      "interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that",
      "have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring",
      "comparison with ground truth 2nd dominant topic might be considered. In future, we will extract more tweets and train the model and observe the model behavior on test data. As we found misleading and unrelated topic in test cases, it is important to",
      "with ground truth, employed different visualizations after information integration and discovered interesting correlation (Yoga-Veganism) in data. In future, we will incorporate Local Interpretable model-agnostic Explanation (LIME) method to understand",
      "data, inferred topic using topic modeling (i.e. LSA, NMF, LDA), observed model behavior on new tweets, compared train/test accuracy with ground truth, employed different visualizations after information integration and discovered interesting correlation",
      "usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'.",
      "correlations, find out the dominant topic in data and understand people's interest through the discussions. In this work, we explore Twitter data related to health. We extract the popular topics under different categories (e.g. diet, exercise) discussed",
      "able to find some interesting correlation of data. Several works have been done on prediction of social media content BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . Prieto et al. proposed a method to extract a set of tweets to estimate and track the",
      "Twitter data related to health. We extract the popular topics under different categories (e.g. diet, exercise) discussed in Twitter via topic modeling, observe model behavior on new tweets, discover interesting correlation (i.e. Yoga-Veganism). We",
      "we picked k = 2, 4, and 4 with the highest coherence value for LSA, NMF, and LDA correspondingly (Fig. FIGREF8 ). Table TABREF13 shows the topics and top-10 keywords of the corresponding topic. We get more informative and understandable topics using LDA"
    ]
  },
  {
    "title": "Learning Relational Dependency Networks for Relation Extraction",
    "answer": "They learn relations such as parents, spouse, siblings, etc. jointly. These relations are inter-related, and learning them jointly improves performance. Specifically, they learn relations within the person category jointly, and joint learning with gold standard outperforms individual learning counterparts.",
    "evidence": [
      "Joint learning",
      "category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance.",
      "are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that",
      "some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations.",
      "advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines. However, some surprising observations include the fact that weak supervision is not as",
      "learning perspective, but also critical to the NLP community. As we show empirically, some of the results such as human advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among",
      "relations jointly within the RDN, displayed in Table TABREF22 . Recall and F1 are omitted for conciseness – the conclusions are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person",
      "comparison of settings such as joint learning vs learning of individual relations, use of weak supervision vs gold standard labels, using expert advice vs only learning from data, etc. These questions are extremely interesting from a general machine",
      "relation extraction such as joint learning, weak supervision, word2vec features, and human advice, among others. We provide a comprehensive comparison of settings such as joint learning vs learning of individual relations, use of weak supervision vs gold",
      "Table TABREF26 shows the results of experiments that test the use of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning. The key impact of advice can be mostly seen in the"
    ]
  },
  {
    "title": "Simultaneous Neural Machine Translation using Connectionist Temporal Classification",
    "answer": "According to the text, the metrics used to evaluate simultaneous translation are:\n\n1. BLEU (to measure translation accuracy)\n2. RIBES (to measure translation accuracy)\n3. Token-level delay (to measure latency)\n\nAdditionally, they use Kytea as a tokenization method for evaluating Japanese translations.",
    "evidence": [
      "Simultaneous machine translation by Wait-k model",
      "simultaneous translation with the proposed and baseline methods and show the proposed method achieves a good translation performance with relatively small latency. The proposed method can determine when to wait or translate in an adaptive manner and is",
      "for simultaneous translation. ma2018stacl proposed a very simple incremental method called Wait-k, where the decoder starts to generate output tokens after the encoder reads k tokens and then works token-by-token. Here, some required inputs may not be",
      "Simultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency. We have to determine when we start the translation",
      "Simultaneous translation is a translation task where the translation process starts before the end of an input. It helps real-time spoken language communications such as human conversations and public talks. A usual machine translation system works in the",
      "Experiments\nWe conducted simultaneous translation experiments from English to Japanese and discussed accuracy, latency, and issues for translation results.",
      "with the proposed approach. The use of specialized data for simultaneous translation would be important in practice, such as monotonic translations like simultaneous translation.",
      "with relatively small latency. The proposed method can determine when to wait or translate in an adaptive manner and is useful in simultaneous translation tasks.",
      "used along with the latency penalty and a standard word prediction loss. The experimental results suggest the proposed method determines when to translate or when to wait in an adaptive manner. Future work includes further analyses on translation accuracy",
      "source sentences (called Full Sentence). For evaluation metrics, we used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese"
    ]
  },
  {
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "answer": "According to the text, traditional classification techniques are used, but the specific type of classifier used for emergency detection is not explicitly mentioned. However, it is mentioned that a multi-class classifier is trained on manually labeled data with classes such as \"fire\", \"accident\", \"earthquake\", etc.\n\nIt can be inferred that",
    "evidence": [
      "detects emergency related events, and classifies them into appropriate categories like\"fire\",\"accident\",\"earthquake\", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting",
      "<emergency>and <non-emergency>labels, and later classified manually to provide labels according to the type of emergency they indicate. We use the manually labeled data for training our classifiers. We use traditional classification techniques such as",
      "depending on the type of emergencies they indicate. This multi-class classifier is trained on data manually labeled with classes. We tokenize the training data using “NgramTokenizer” and then, apply a filter to create word vectors of strings before",
      "Our system detects whether a tweet, which contains a keyword from a pre-decided list, is related to an actual emergency or not. It also classifies the event into its appropriate category, and visualizes the possible location of the emergency event on the",
      "media sites. We would like to detect such events in case of an emergency. Some previous studies BIBREF0 investigate the use of features such as keywords in the tweet, number of words, and context to devise a classifier for event detection. BIBREF1",
      "We propose a software architecture for Emergency detection and visualization as shown in figure FIGREF9 . We collect data using Twitter API, and perform language pre-processing before applying a classification model. Tweets are labelled manually with",
      "system, which indicates whether a tweet is related to an emergency or not, and if it is, then what category of emergency it belongs to. We display such positively classified tweets along with their type and location on a Google map, and notify our users",
      "users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique",
      "We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like\"fire\",\"accident\",\"earthquake\", etc. We",
      "in figure FIGREF24 . It shows that our classifier model is trained on appropriate words, which are very close to the emergency situations viz. “fire”, “earthquake”, “accident”, “break” (Unigram representation here, but possibly occurs in a bigram phrase"
    ]
  },
  {
    "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "answer": "The authors, who are researchers in either NLP or computer security, annotated the data. Specifically, three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets, and additional team members (five for Darkode, one for Hack Forums) also participated in the",
    "evidence": [
      "security experts. The data annotated during this process is not included in Table TABREF3 . Once we had defined the annotation standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 . Three people",
      "Dataset and Annotation",
      "this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.",
      "standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 . Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were",
      "Annotation Process",
      "produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit BIBREF17 . Note that many",
      "by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. We",
      "Type-level Annotation",
      "Token-level Annotation",
      "applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the"
    ]
  },
  {
    "title": "Deep Health Care Text Classification",
    "answer": "The text doesn't specifically mention a particular type of RNN, but it does mention a variant of RNN, which is:\n\n1. **Long Short-Term Memory (LSTM)**: This is a type of RNN that is designed to handle the vanishing gradient problem and is well-suited for modeling long-term dependencies in sequential data.\n\n",
    "evidence": [
      "Recurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 . The input sequences ${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t -",
      "Recurrent neural network (RNN) and it’s variant",
      "neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for",
      "Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the",
      "neural network for adverse drug event detection. Though the data sets of task 1 and task 2 are limited, this paper proposes RNN and LSTM based embedding method.",
      "${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden state vector $h{i_{t - 1}}$ are calculated based on the transition function $tf$ of present input",
      "presents RNN and LSTM based embedding system for social media health text classification. Due to limited number of tweets, the performance of the proposed method is very less. However, the obtained results are considerable and open the way in future to",
      "The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the",
      "512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature",
      "media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different"
    ]
  },
  {
    "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    "answer": "The baselines mentioned in the text are:\n\n1. A BiLSTM model with randomly initialized word embeddings, following BIBREF30, which is a strong baseline sentence encoding model.\n2. A classic deep neural network MTL model (BIBREF31) that shares its parameters across tasks and has task-specific softmax output",
    "evidence": [
      "The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30 . We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings",
      "INLINEFORM1 is given at training time with a target task INLINEFORM2 that is of particular interest. The training dataset for task INLINEFORM3 consists of INLINEFORM4 examples INLINEFORM5 and their labels INLINEFORM6 . The base model is a classic deep",
      "following BIBREF30 . We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings that work particularly well for this dataset, to showcase the dataset. We would expect pre-trained contextual encoding models, e.g.",
      "So far, we have ignored the issue of how to obtain claim representation, as the base model described in the previous section is agnostic to how instances are encoded. A very simple approach, which we report as a baseline, is to encode claim texts only.",
      "task INLINEFORM3 consists of INLINEFORM4 examples INLINEFORM5 and their labels INLINEFORM6 . The base model is a classic deep neural network MTL model BIBREF31 that shares its parameters across tasks and has task-specific softmax output layers that output",
      "is agnostic to how instances are encoded. A very simple approach, which we report as a baseline, is to encode claim texts only. Such a model ignores evidence for and against a claim, and ends up guessing the veracity based on surface patterns observed in",
      "For each domain, we compute the Micro as well as Macro F1, then mean average results over all domains. Core results with all vs. no metadata are shown in Table TABREF30 . We first experiment with different base model variants and find that label",
      "a fairly small number of claims, and those that do not, often lack evidence documents. An exception is BIBREF2 , who create a Wikipedia-based fact checking dataset. While a good testbed for developing deep neural architectures, their dataset is",
      "Conclusions",
      "Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are"
    ]
  },
  {
    "title": "Retrieval-based Goal-Oriented Dialogue Generation",
    "answer": "They compared their retrieval-based model with:\n\n1. A transformer model (BIBREF31) that uses dialogue context to condition the decoder.\n2. A LSTM model that uses dialogue context and incorporates belief state and KB results as well.\n3. A semantically conditioned generation model using Hierarchical Disentangled Self-Attention (",
    "evidence": [
      "Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not",
      "architecture leads to significant improvements, including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act",
      "given the conversation context. This was confirmed by the human evaluations and also the Inform/Request metrics. When comparing the performance of the exemplar-based model to models that do not use information about past acts to condition the decoder, we",
      "our proposed models versus the baseline. We looked at the responses generated by our proposed models that had the highest score of these metrics and compared to the response generated by the baseline for that same dialogue. Overall we found that the",
      "one and condition our decoder on these. We find that this method not only improves over multiple strong baseline models on word overlap metrics, it also performs better than the state-of-the-art on BLEU and achieves competitive performance for",
      "exemplar-based model. In addition, for the BLEU metric we include the results of a transformer modelBIBREF31 that uses dialogue context to condition the decoder as well as a LSTM that uses dialogue context and incorporates belief state and KB results as",
      "model as the more appropriate one given the context, for 59 percent of the evaluated dialogs. The baseline beat the proposed model only 14 percent of the time. These results can also be found on table TABREF7",
      "this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges",
      "both or none were picked. For appropriateness we see a similar pattern. Evaluators perceived the response produced by the exemplar model as the more appropriate one given the context, for 59 percent of the evaluated dialogs. The baseline beat the proposed",
      "acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention"
    ]
  },
  {
    "title": "User Generated Data: Achilles' heel of BERT",
    "answer": "The reason behind the drop in performance using BERT for some popular tasks is the introduction of noise (spelling mistakes) in the input text data. Specifically, the problem lies with how misspelt words are tokenized by the WordPiece tokenizer used in BERT. As the noise increases, the quality of the output of the",
    "evidence": [
      "It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of",
      "drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 %",
      "of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data.",
      "of BERT. We demonstrated that as the noise increases, BERT’s performance drops drastically. We further investigated the BERT system to understand the reason for this drop in performance. We show that the problem lies with how misspelt words are tokenized",
      "To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to",
      "is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie",
      "to have much more noise in the data as compared to benchmark datasets. In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on",
      "In this work we systematically studied the effect of noise (spelling mistakes) in user generated text data on the performance of BERT. We demonstrated that as the noise increases, BERT’s performance drops drastically. We further investigated the BERT",
      "is BERTBIBREF0. The popularity of BERT stems from its ability to be fine-tuned for a variety of downstream NLP tasks such as text classification, regression, named-entity recognition, question answeringBIBREF0, machine translationBIBREF6 etc. BERT has",
      "performance of BERT model. Hence, as we systematically introduce more errors, the quality of output of the tokenizer degrades further, resulting in the overall performance drop. Our results and analysis shows that one cannot apply BERT blindly to solve"
    ]
  },
  {
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "answer": "The analysis provides general insights into the cohesion of political groups in the Eighth European Parliament (2014-2019), both overall and across different policy areas. Specifically, it:\n\n1. Reveals the most cohesive political groups in the European Parliament.\n2. Shows that coalitions are formed in similar ways across different policy areas",
    "evidence": [
      "insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. (ii) We explore whether coalitions are formed in the same way for different policy areas. (iii) We explore to what degree the",
      "Cohesion of political groups",
      "We study cohesion and coalitions in the Eighth European Parliament by analyzing, on one hand, MEPs' co-voting tendencies and, on the other, their retweeting behavior. We reveal that the most cohesive political group in the European Parliament, when it",
      "We study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014–2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making",
      "We study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making",
      "on different assumptions, they provide consistent results. The main contributions of this paper are as follows: (i) We give general insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy",
      "Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the",
      "Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the",
      "stock-price movements BIBREF2 , BIBREF3 . In this paper we study the cohesion and coalitions exhibited by political groups in the Eighth European Parliament (2014–2019). We analyze two entirely different aspects of how the Members of the European",
      "European Parliament over all policy areas are shown in Fig FIGREF42 G. It comes as no surprise that the strongest cooperations are within the groups (in the diagonal). Moreover, we again observe GUE-NGL, S&D, Greens-EFA, ALDE, and EPP as the most cohesive"
    ]
  },
  {
    "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
    "answer": "Based on the provided context, here are the reasons why MS-MARCO is different from SQuAD:\n\n1. **Multiple passages**: MS-MARCO contains multiple passages for a question, while SQuAD only has one passage for a question.\n2. **Answer type**: In SQuAD, the answer",
    "evidence": [
      "dataset. Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset.",
      "the answer to be an exact sub-span in the passage, while words in the answer are not necessary in the passages in the MS-MARCO dataset. Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages.",
      "In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question",
      "in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO BIBREF1 is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by",
      "We propose a multi-task learning framework for evidence extraction. Unlike the SQuAD dataset, which only has one passage given a question, there are several related passages for each question in the MS-MARCO dataset. In addition to annotating the answer,",
      "Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer in the passage. However, as defined in the",
      "existing works on the MS-MARCO dataset follow their methods on the SQuAD. BIBREF7 combine match-LSTM and pointer networks to produce the boundary of the answer. BIBREF8 and BIBREF9 employ variant co-attention mechanism to match the question and passage",
      "Different from above datasets, the SQuAD dataset BIBREF0 whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO BIBREF1 is the",
      "Stanford Question Answering Dataset (SQuAD) BIBREF0 and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO) BIBREF1 provides the large-scale manually created datasets for model training and testing of machine learning (especially deep learning)",
      "achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset may also be applied on the MS-MARCO dataset BIBREF11 , BIBREF12 , BIBREF13 . The sequence-to-sequence model is widely-used in many tasks such as"
    ]
  },
  {
    "title": "Neural Machine Translation with Supervised Attention",
    "answer": "According to the text, they use several datasets:\n\n1. For the large-scale task:\n\t* Training data: 1.8M sentence pairs from the NIST2008 Open Machine Translation Campaign.\n\t* Development set: nist02 (878 sentences).\n\t* Test sets: nist05 (1082 sentences",
    "evidence": [
      "system BIBREF15 ;  NMT1: an attention based NMT BIBREF0 system at https://github.com/lisa-groundhog/GroundHog;  NMT2: another implementation of BIBREF0 at https://github.com/nyu-dl/dl4mt-tutorial.  We developed the proposed approach based on NMT2, and",
      "We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and",
      "behaviors on the development set, we employed the GIZA++ to generate the alignment on the training set prior to the training SA-NMT. In Table TABREF21 , we can see that MUL is better than MSE. Furthermore, CE performs best among all losses, and thus we",
      "For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets,",
      "systems, we employed the same settings as those in the large scale task, except that vocabulary size is 6000, batch size is 16, and the hyper-parameter INLINEFORM0 for SA-NMT. Table TABREF32 reports the final results. Firstly, we can see that both",
      "based NMT, which is the focus of this paper, is attracting increasing interests in the community BIBREF0 , BIBREF2 . One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing",
      "the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines:  Moses: a phrase-based machine",
      "alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in BIBREF5 as the test set. Following BIBREF17 , we force-decode both the bilingual sentences including source and reference sentences to obtain the",
      "Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in BIBREF0 for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch",
      "All experiments were run with the default settings. To train NMT1, NMT2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set"
    ]
  },
  {
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "answer": "The measure of semantic similarity used is the cosine similarity between the sparse sentence vector and the sparse synset vector, and also the semantic similarity between the context and the candidate senses. \n\nMore specifically, the system uses:\n\n* Cosine similarity between the sparse sentence vector and the sparse synset vector to retrieve the synset containing the",
    "evidence": [
      "estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different",
      "primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large",
      "that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure. In order to",
      "We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 . In the gold standard, each word is provided with a set of instances, i.e., the sentences containing the word. Each",
      "JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 . Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings),",
      "the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space",
      "system estimates the sense labels for these ambiguous words, which can be viewed as a clustering of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation.",
      "The original SemEval 2010 Task 14 used the V-Measure external clustering measure BIBREF5 . However, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system",
      "each word to disambiguate, we retrieve the synset containing this word that maximizes the cosine similarity between the sparse sentence vector and the sparse synset vector. Let INLINEFORM3 be the maximal number of synsets containing a word and INLINEFORM4",
      "We compare the evaluation results obtained for the Sparse and Dense approaches with three baselines: the AdaGram model (AdaGram), the same sense for all the instances per lemma (One) and one sense per instance (Singletons). The evaluation results are"
    ]
  },
  {
    "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
    "answer": "The three datasets used are:\n\n1. DSTC2 (Dialogue State Tracking Competition 2) dataset\n2. M2M-sim-M (dataset in the movie domain)\n3. M2M-sim-R (dataset in the restaurant domain)\n\nThese datasets are also referred to as being open-sourced by Google Research,",
    "evidence": [
      "different datasets in order to make sure the results are not corpus-biased. Table TABREF12 shows the statistics of these three datasets which we will use to train and evaluate the models. The M2M dataset has more diversity in both language and dialogue",
      "We use three different datasets for training the models. We use the Dialogue State Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots. We also used two other datasets recently",
      "as neural machine translation, but their usage has not been explored for the task of training end-to-end task-oriented dialogue generation systems yet. In this study, we apply these models on the three different datasets for training task-oriented",
      "metrics on three different datasets. We provide insight into how effective are self-attentional models for this task and benchmark the time performance of these models against the recurrence-based sequence modelling methods. We try to quantify the",
      "common recurrence-based models used in sequence modelling tasks and compare and analyze the results using different evaluation metrics on three different datasets. We provide insight into how effective are self-attentional models for this task and",
      "BIBREF27 which is the most widely used dataset for research on task-oriented chatbots. We also used two other datasets recently open-sourced by Google Research BIBREF28 which are M2M-sim-M (dataset in movie domain) and M2M-sim-R (dataset in restaurant",
      "Experiments ::: Datasets",
      "generation systems yet. In this study, we apply these models on the three different datasets for training task-oriented chatbots. Our finding shows that self-attentional models can be exploited to create end-to-end task-oriented chatbots which not only",
      "datasets which we will use to train and evaluate the models. The M2M dataset has more diversity in both language and dialogue flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating task-oriented chatbots.",
      "which stores the knowledge base in the form of triplets (which is (subject,relation,object) such as (yoga,time,3pm)) in the key-value memory network and then the model tries to select the most relevant entity from the memory and create a relevant"
    ]
  },
  {
    "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
    "answer": "Based on the provided context, it seems that the authors are proposing a novel taxonomy of fine-grained dialogue acts tailored for the customer service domain on Twitter. Since they are focusing on Twitter conversations, the dialogue acts they propose are likely to be different from those used in stand-alone tweets or other domains.\n\nThe authors aim to develop",
    "evidence": [
      "to develop actionable rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can",
      "Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts. Core and Allen present the Dialogue Act Marking in Several Layers (DAMSL), a standard for",
      "good/bad practices with respect to conversation outcomes. We design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often",
      "on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can provide meaningful recommendations about good communicative practices, based on real data. Our methodology pipeline is shown in",
      "system to detect stories on social media BIBREF11 . In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to",
      "domain on Twitter. Our goal is to offer useful analytics to improve outcome-oriented conversational systems. We first expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for customer service, and",
      "domain presents its own interesting characteristics that are worth exploring further. The most related previous work has explored speech and dialogue act modeling in customer service, however, no previous work has focused on Twitter as a data source. In",
      "for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically overlapping, and conduct multi-label supervised learning experiments to predict multiple appropriate dialogue act",
      "acts\" for each tweet by finding any acts that have received majority-vote labels (at least 3 out of 5 judgements). It is important to note at this point that we make an important choice as to how we will handle dialogue act tagging for each turn. We note",
      "from many disciplines, including philosophy, linguistics, computer science, and sociology. In this section, we describe relevant previous work on speech and dialogue act modeling, general conversation modeling on Twitter, and speech and dialogue act"
    ]
  },
  {
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "answer": "The lexicon of trafficking flags can be expanded through the following approaches:\n\n1. **Word embeddings and t-SNE**: By using word embeddings and t-SNE (t-distributed Stochastic Neighbor Embedding), new terms, acronyms, and emojis can be linked to existing trafficking flags.\n2. **Emoji map updates**:",
    "evidence": [
      "dataset of escort ads. Additionally, because traffickers use acronyms, deliberate typographical errors, and emojis to replace explicit keywords, we demonstrate how to expand the lexicon of trafficking flags through word embeddings and t-SNE.",
      "model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also",
      "emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.",
      "all used in similar contexts and perhaps should all be flags for underaged victims in the updated lexicon. If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link",
      "victims, but instead use acronyms, intentional typos, and emojis BIBREF9 . Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but",
      "been learned as desired. The emoji map can assist anti-trafficking experts in expanding the existing lexicon of trafficking flags. For example, according to the lexicon we obtained from Global Emancipation Network, the cherry emoji and the lollipop emoji",
      "the lexicon of trafficking-related emojis can be subsequently expanded. In Section SECREF2 , we discuss related work on human trafficking detection and ordinal regression. In Section SECREF3 , we present our proposed model and detail its components. In",
      "The fight against human traffickers is adversarial and dynamic. Traffickers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis BIBREF9 . Law enforcement maintains a lexicon of trafficking",
      "also conducted an emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of trafficking flags. Since our experiments, there have been considerable advancements in language representation models, such as",
      "trafficking flags. Since our experiments, there have been considerable advancements in language representation models, such as BERT BIBREF30 . The new language representation models can be combined with our ordinal regression layer, replacing the"
    ]
  },
  {
    "title": "MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs",
    "answer": "The datasets used to evaluate the model are:\n\n1. WN18\n2. FB15k \n\nThese datasets are extracted from:\n\n1. Wordnet \n2. Freebase",
    "evidence": [
      ", we generated one negative example per positive example for all the datasets. We used Adadelta BIBREF30 as the optimizer and fine-tuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding",
      "Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train",
      "and fine-tuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, batch size 100, 150, iterations 1000, 1500, 2500, 3600. We set the initial learning rate on all datasets",
      "use 1 negative per positive, for MDE. To show the effect of dataset construction we compare this models with an experiment of TransE and RotatE on FB15k-237 that applies 256 negative samples per one positive sample BIBREF11 . Although these models perform",
      "lower than other methods. Comparison of MDE and TransE and other distance based models confirms the improved ability of MDE in learning different patterns. Negative Sampling and Data Augmentation Models: Currently, the training datasets for link",
      "the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 . Evaluation Settings: We evaluate the link",
      "Modeling Relational Patterns",
      "sample when training on WN18RR). ComplEx and SimplE also generate 10 negative samples per one positive sample on FB15K. Here, we use 1 negative per positive, for MDE. To show the effect of dataset construction we compare this models with an experiment of",
      "While machine learning methods conventionally model functions given sample inputs and outputs, a subset of statistical relational learning(SRL) BIBREF0 , BIBREF1 approaches specifically aim to model “things” (entities) and relations between them. These",
      "are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 . Evaluation Settings: We evaluate the link prediction performance by ranking the score of each test triple against its versions with replaced head, and once for tail. Then"
    ]
  },
  {
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "answer": "According to the text, users' backstory queries about Gunrock are positively correlated to user satisfaction. Specifically:\n\n* The number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction (log: β = 0.10, SE = 0.002",
    "evidence": [
      "users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.",
      "questions about Gunrocks's backstory positively correlate with user experience. Additionally, we find evidence for interleaved dialog flow, where combining factual information with personal opinions and stories improve user satisfaction. Overall, this",
      "of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3). This suggests that maintaining a consistent personality — and having enough responses to questions the users are interested in — may improve user satisfaction.",
      "longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that",
      "favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized",
      "Overall, the number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3). This suggests that maintaining a consistent",
      "that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses. Overall, the number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of",
      "We assessed the user's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., “What's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by",
      "active in a conversation. Analysis shows that users' speech behavior reflects these capabilities. Longer sentences and more questions about Gunrocks's backstory positively correlate with user experience. Additionally, we find evidence for interleaved",
      "the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her"
    ]
  },
  {
    "title": "Image Captioning: Transforming Objects into Words",
    "answer": "The common captioning metrics mentioned in the text are:\n\n1. BLEU\n2. ROUGE\n3. SPICE (which has sub-metrics, including Count)\n\nAdditionally, BIBREF23 and BIBREF22 are mentioned as other metrics, but their specific names are not provided. \n\nIt's also mentioned that the",
    "evidence": [
      "the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics.",
      "BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned",
      "Image captioning models typically follow an encoder-decoder architecture which uses abstract image feature vectors as input to the encoder. One of the most successful algorithms uses feature vectors extracted from the region proposals obtained from an",
      "results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.",
      "sub-metrics of the SPICE captioning metric. We also present qualitative examples of how incorporating this information can yield captioning results demonstrating better spatial awareness. At present, our model only takes into account geometric information",
      "in more accurate image caption generation. Inspired by neural machine translation, most conventional image captioning systems utilize an encoder-decoder framework, in which an input image is encoded into an intermediate representation of the information",
      "information about the spatial relationship between input detected objects through geometric attention. Quantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common",
      "in the Count subcategory of SPICE, from 11.30 to 17.51. Image captioning methods in general show a large deficit in Count scores when compared with humans BIBREF23 , while we're able to show a significant improvement by adding explicit positional",
      "model in determining the correct relationships between objects. In addition, it is interesting to see a significant improvement in the Count subcategory of SPICE, from 11.30 to 17.51. Image captioning methods in general show a large deficit in Count",
      "and geometry features from all the detected objects in the image. Thereafter we use the Object Relation Transformer to generate the caption text. Section SECREF7 describes how we use the Transformer architecture BIBREF4 in general for image captioning."
    ]
  },
  {
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "answer": "The dataset, RC-QED$^{\\rm E}$, was annotated using a crowdsourcing framework. Specifically, the authors randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances, resulting in 36,000 annotation tasks, and annotated them with",
    "evidence": [
      "There exists few RC datasets annotated with explanations (Table TABREF50). The most similar work to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes. By developing the scalable",
      "Experiments ::: Settings ::: Dataset",
      "Data collection for RC-QED@!START@$^{\\rm E}$@!END@ ::: Dataset",
      "Related work ::: RC datasets with explanations",
      "justification explanations to provide new challenges for NLU. Several datasets are annotated with introspective explanations, ranging from textual entailments BIBREF8 to argumentative texts BIBREF26, BIBREF27, BIBREF33. All these datasets offer the",
      "reasoning annotations and baseline systems publicly available at https://naoya-i.github.io/rc-qed/. One immediate future work is to expand the annotation to non-entity-based multi-hop QA datasets such as HotpotQA BIBREF2. For modeling, we plan to",
      "large-scale corpus of NLDs. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Our experiments using two simple baseline models have demonstrated that RC-QED$^{\\rm E}$ is a non-trivial task, and",
      "a large corpus consisting of 12,000 QA pairs and natural language derivations. The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Through an experiment using two baseline models, we highlight",
      "work is to expand the annotation to non-entity-based multi-hop QA datasets such as HotpotQA BIBREF2. For modeling, we plan to incorporate a generative mechanism based on recent advances in conditional language modeling.",
      "Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks"
    ]
  },
  {
    "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
    "answer": "According to the text, the ground truth of gang membership is established through a data collection process involving:\n\n1. Searching for seed words (location-neutral keywords used by gang members)\n2. Expanded search of retweet, friends, and follower networks of the gang member profiles found\n\nThis process led to identifying 400 authentic gang",
    "evidence": [
      "of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user",
      "share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study",
      "in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover",
      "of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests",
      "filtering method used, which is based on the availability of features in the training dataset as described in BIBREF9 . The baseline Model(1) uses all profiles in the dataset and has a INLINEFORM0 -score of 0.7364 for `gang' class and 0.9690 for",
      "for identifying gang members in Twitter. Both baseline models are built from a random forest classifier trained over term frequencies for unigrams in tweet text, emoji, profile data, YouTube video data and image tags. Baseline Model(1) considers all 3,285",
      "the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed",
      "set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of",
      "This paper presented a word embeddings-based approach to address the problem of automatically identifying gang member profiles on Twitter. Using a Twitter user dataset that consist of 400 gang member and 2,865 non gang member profiles, we trained word",
      "embeddings. We compare our results with the two best performing systems reported in BIBREF9 which are the two state-of-the-art models for identifying gang members in Twitter. Both baseline models are built from a random forest classifier trained over term"
    ]
  },
  {
    "title": "Recurrently Controlled Recurrent Networks",
    "answer": "According to the text, RCRN has approximately equal parameterization to 3L-BiLSTM, and both RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM. However, it is also mentioned that 3L-BiLSTMs were overall better than Bi",
    "evidence": [
      ". INLINEFORM2 are the parameters of the model where INLINEFORM3 and INLINEFORM4 . INLINEFORM5 is the sigmoid function and INLINEFORM6 is the tanh nonlinearity. INLINEFORM7 is the Hadamard product. The controller RNN has two cell states denoted as",
      "model architecture. The dimensionality of the model is set to 200 with a fixed batch size of 32. Finally, we also benchmark a CUDA optimized adaptation of RCRN which has been described earlier (Section SECREF4 ). Table TABREF32 reports training/inference",
      "model which can be considered to be very competitive baselines. RCRN also outperforms ablative baselines BiLSTM ( INLINEFORM1 ) and 3L-BiLSTM ( INLINEFORM2 ). Our results on the TREC question classification dataset (Table TABREF25 ) is also promising.",
      "which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc. We achieve (close",
      "multiple benchmarks (e.g., SST, TREC, SNLI, MultiNLI) do not outperform (or even approach) the state-of-the-art, most of which are models that still heavily rely on bidirectional LSTMs BIBREF27 , BIBREF20 , BIBREF5 , BIBREF10 . While self-attentive",
      "all other components constant, 3L-BiLSTM has equal parameters to RCRN while RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM.",
      "RCRN achieves INLINEFORM0 accuracy, which is competitive to Gumbel LSTM. However, RCRN outperforms a wide range of baselines, including self-attention based models as multi-head BIBREF24 and DiSAN BIBREF25 . There is also performance gain of INLINEFORM1",
      "to utilize multiple recurrent layers. Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of",
      "can lead to considerable improvements. This leads to an improvement of INLINEFORM0 on all four metrics. Note that our model only uses a single layered RCRN while R-NET uses 3 layered BiGRUs. This empirical evidence might suggest that RCRN is a better way",
      "the controller. An alternative interpretation to our model architecture is that it is essentially a `recurrent-over-recurrent' model. Clearly, the formulation we have used above uses BiLSTMs as the atomic building block for RCRN. Hence, we note that it is"
    ]
  },
  {
    "title": "Contextual Recurrent Units for Cloze-style Reading Comprehension",
    "answer": "The datasets used for testing sentiment classification and reading comprehension are:\n\nFor Sentiment Classification:\n1. MR Movie reviews (with one sentence each, classified into positive or negative)\n2. IMDB Movie reviews (from the IMDB website)\n\nFor Reading Comprehension:\n1. CBT NE/CN (specific details not provided but",
    "evidence": [
      "In the sentiment classification task, we tried our model on the following public datasets. [leftmargin=*] MR Movie reviews with one sentence each. Each review is classified into positive or negative BIBREF18. IMDB Movie reviews from IMDB website, where",
      "have tested our CRU model on the cloze-style reading comprehension task and sentiment classification task. Experimental results show that our model could give substantial improvements over various state-of-the-art systems and set up new records on the",
      "NLP tasks: sentiment classification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit",
      "the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some",
      "modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model. To further demonstrate the effectiveness of our model, we also tested our CRU in reading comprehension tasks with a",
      "Neural Network (CNN). RNNs are powerful models in various NLP tasks, such as machine translation BIBREF0, sentiment classification BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, reading comprehension BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11,",
      "Besides the sentiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task. In this paper, we strengthened the recent AoA Reader BIBREF10 and applied our CRU model to see if we could",
      "CRU model could give improvements in both sentence-level modeling and document-level modeling tasks, in this paper, we applied the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification",
      "the CNN into GRU, which aims to better model the local context information via CNN before recurrently modeling the sequence. We have tested our CRU model on the cloze-style reading comprehension task and sentiment classification task. Experimental results",
      "We also tested our CRU model in the cloze-style reading comprehension task. We carried out experiments on the public datasets: CBT NE/CN BIBREF25. The CRU model used in these experiments is the deep-enhanced type with the convolutional filter length of 3."
    ]
  },
  {
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "answer": "They compared their results to the baseline model from See et al. (2017), which was trained on the CNN/Daily Mail dataset. Specifically, they reproduced the results from See et al. (2017) and then applied LRP (Layer-wise Relevance Propagation) to it. \n\nIn other words, the",
    "evidence": [
      "The Task and the Model\nWe present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it.",
      "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is",
      "summary: they are almost all identical and seem uncorrelated with the attention distribution. We then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps. We obtain a ranking of the word from",
      "features for each element of the output sequence. We observe that all the saliency maps for a text are nearly identical and decorrelated with the attention distribution. We propose a way to check their validity by creating what could be seen as a",
      "Validating the Attributions",
      "Obtained Summaries",
      "First Observations",
      "Experimental results",
      "and decorrelated with the attention distribution. We propose a way to check their validity by creating what could be seen as a counterfactual experiment from a synthesis of the saliency maps, using the same technique as in Arras et al. Arras2017. We show",
      "We train the 21 350 992 parameters of the network for about 60 epochs until we achieve results that are qualitatively equivalent to the results of See et al. See2017. We obtain summaries that are broadly relevant to the text but do not match the target"
    ]
  },
  {
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "answer": "They select answer candidates by masking the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers.",
    "evidence": [
      "and QQP are similarity and paraphrase tasks, and MNLI, QNLI, RTE and WNLI are natural language inference tasks. To investigate whether our multi-choice QA based pre-training approach degenerates the performance on common sentence classification tasks, we",
      "about the world, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs for commonsense in QA modeling can help model choose correct answers. For example, as shown in the part B of Table 1",
      "the same concept $_2$ /concept $_1$ and the relation with the correct answers. We denote the resulting model from this dataset BERT_CS_random. Instead of pre-training BERT with a multi-choice QA task that chooses the correct answer from several candidate",
      "special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks. We use the same loss function as BIBREF22 , that is, if c $_1$ is correct and c $_2$ is not, the loss",
      "pre-training task for the target multi-choice QA task. We argue that, for the masked LM task, BERT_CS is required to predict each masked wordpieces (in concepts) independently and for the multi-choice QA task, BERT is required to model the whole candidate",
      "we call as AMS method. Each sample in the dataset consists of a question and several candidate answers, which has the same form as the CommonsenseQA dataset. An example of constructing one training sample by masking concept $_2$ is shown in Table 2 .",
      "on multi-choice QA tasks, e.g., CommonsenseQA and Winograd Schema Challenge (see section 5.3), we fine-tune all parameters in BERT_CS, including the last softmax layer from the token [CLS]; whereas, for other tasks, we randomly initialize the classifier",
      "masked wordpieces (in concepts) independently and for the multi-choice QA task, BERT is required to model the whole candidate phrases. In this way, BERT is able to model the whole concepts instead of paying much attention to the single wordpieces in the",
      "BERT_CS_random. Instead of pre-training BERT with a multi-choice QA task that chooses the correct answer from several candidate answers, we mask concept $_1$ and concept $_2$ and pre-train BERT with a masked language model (MLM) task. We denote the",
      "We transform the pronoun disambiguation problem into a multi-choice question answering problem. We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures"
    ]
  },
  {
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "answer": "The sizes of the two datasets are:\n\n* Europarl: 103,871 sentences\n* SoNaR: 1,269,091 sentences",
    "evidence": [
      "Dataset",
      "test sets consist of 103,871 (Europarl) and 1,269,091 (SoNaR) sentences. The SoNaR dataset, on the other hand, has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled. Considering the imbalance in both",
      "of the second prediction task and/or the split in sentence and context encoder. Firstly, the data is divided into batch sizes of 512 instead of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batch size",
      "size is 512 or 128. Therefore, it can be suggested that an increased batch size has no directly positive influence on model performance. Secondly, the input data is transformed to 200-dimensional word embeddings instead of 100-dimensional word embeddings.",
      "dat. The Europarl dataset, on the one hand, contains 70,057 dat-labeled and 33,814 die-labeled sentences. The resulting train and test sets consist of 103,871 (Europarl) and 1,269,091 (SoNaR) sentences. The SoNaR dataset, on the other hand, has more than",
      "corpora individually and experiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance. There are three settings: full in which the datasets contain full sentences, windowed in which",
      "prediction of these two pronouns. Two separate datasets are built with sentences obtained from, respectively, the Dutch Europarl corpus (Koehn 2015) - which contains the proceedings of the European Parliament from 1996 to the present - and the SoNaR",
      "of 512 instead of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batch size is 512 or 128. Therefore, it can be suggested that an increased batch size has no directly positive influence on model",
      "the binary classification model are used: full in which the datasets contain full sentences, windowed in which sentences are windowed around the unique prediction token without exceeding sentence boundaries (five tokens before and after the token,",
      "dat than die instances in training, validation and test datasets extracted from the Europarl and SoNaR corpus. Secondly, when the dataset is more balanced, as in the SoNaR corpus, the difference in performance between die and dat labels decreases as"
    ]
  },
  {
    "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims",
    "answer": "The challenges highlighted in the text are:\n\n1. **Identifying and formulating key NLP challenges**: The authors aim to identify and formulate the key NLP challenges underlying the task of substantiated perspective discovery.\n2. **Developing a dataset for systematic study**: The authors want to develop a dataset that would allow a systematic study of the",
    "evidence": [
      "understanding challenges, and show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art NLP techniques. This poses a challenge and opportunity for the NLP community to address.",
      "is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges. For example, for the claim in Figure FIGREF1 , multiple (non-redundant) perspectives should be",
      "BIBREF44 , BIBREF1 as separate aspects of problem. We hope that some of these challenges and limitations will be addressed in future work.",
      "In this section we provide a closer look into the challenge and propose a collection of tasks that move us closer to substantiated perspective discovery. To clarify our description we use to following notation. Let INLINEFORM0 indicate a target claim of",
      "that would facilitate discovering diverse perspectives and their supporting evidence with respect to a given claim. Our goal is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic",
      "between claims, perspectives, and evidence, we leave out important issues such as their degree of factuality BIBREF8 or trustworthiness BIBREF44 , BIBREF1 as separate aspects of problem. We hope that some of these challenges and limitations will be",
      "The importance of this work is three-fold; we define the problem of substantiated perspective discovery and characterize language understanding tasks necessary to address this problem. We combine online resources, web data and crowdsourcing and create a",
      "is supported, relative to INLINEFORM6 , by at least one evidence paragraph INLINEFORM7 , denoted INLINEFORM8 . Creating systems that would address our challenge in its full glory requires solving the following interdependent tasks: Determination of",
      "Design Principles and Challenges",
      "problem. Our hope is that this dataset would bring more attention to this important problem and would speed up the progress in this direction. There are two aspects that we defer to future work. First, the systems designed here assumed that the input are"
    ]
  },
  {
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "answer": "The news comment dataset used was collected from Tencent News, a popular Chinese website for news and opinion articles. Specifically, the dataset is BIBREF0, which consists of 198,112 news articles, each with a title, content, and a list of user comments. A human-annotated test set of 1",
    "evidence": [
      "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion",
      "dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the relevance score between articles and comments. The relevance score ranges from 1 to 5, and we find that only 6.8% of the pairs have an average",
      "paired commenting dataset is loosely paired. Qin et al. QinEA2018 were the first to propose the article commenting task and an article-comment dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the",
      "our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a",
      "our model on a news comment dataset. Experiments show that our proposed topic-based approach significantly outperforms previous lexicon-based models. The model also profits from paired corpora and achieves state-of-the-art performance under",
      "filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a",
      "based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the",
      "the plausible comments. Popular: The 50 most popular comments from the dataset. We count the frequency of each comments in the training set, and select the 50 most frequent comments to form the popular comment set. The popular comments are the general and",
      "articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba,",
      "The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the"
    ]
  },
  {
    "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
    "answer": "Mandarin and English. \n\nThe text mentions that the experiment involves a conversational Mandarin-English code-switching speech corpus called SEAME Phase II, and the languages explored are specifically these two.",
    "evidence": [
      "Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and",
      "In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by",
      "Language Modeling",
      "we generate 1-best and 3-best using beam search with a beam size of 5. For the input, we build a parallel monolingual corpus by translating the mixed language sequence using Google NMT to English ( INLINEFORM0 ) and Mandarin ( INLINEFORM1 ) sequences.",
      "Introduction",
      "In this section, we present the experimental settings for pointer-generator network and language model. Our experiment, our pointer-generator model has 500-dimensional hidden states and word embeddings. We use 50k words as our vocabulary for source and",
      "constrained by code-switching texts. The pointer network copies words from both languages and pastes them into the output, generating code switching sentences in matrix language to embedded language and vice versa. The attention mechanism helps the",
      "Related Work",
      "approach such as equivalent constraint BIBREF5 , BIBREF6 is not restrictive to languages with distinctive grammar structure. In this paper, we propose a novel language-agnostic method to learn how to generate code-switching sentences by using a",
      "Then, we concatenate the translated English and Mandarin sequences and assign code-switching sequences as the labels ( INLINEFORM2 ). The baseline language model is trained using RNNLM BIBREF23 . Then, we train our 2-layer LSTM models with a hidden size"
    ]
  },
  {
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "answer": "They add language IDs in the CS point of transcription. In other words, they directly incorporate language identities into the transcription, specifically at the code-switching points, rather than using an additional language identification (LID) model.",
    "evidence": [
      "language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our",
      "to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the proposed method can achieve 16.2% and 12.9%",
      "To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on CTC or attention-based encoder-decoder models or the combination of both. However, previous",
      "to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the",
      "Recently, language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes",
      "state-of-art model in some tasks BIBREF13. However, the lack of CS training data poses serious problem to end-to-end methods. To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4,",
      "language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. In the figure, we use the arrangements of different geometric icons to represent the CS",
      "paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of",
      "the model with standard decode process. This suggests that the predicted language IDs can effectively guide the model decoding. Because the model assigns language IDs to the recognized words directly, the language IDs error rate is hard to compute. This",
      "Because the model assigns language IDs to the recognized words directly, the language IDs error rate is hard to compute. This result may imply that the prediction accuracy of our method is high enough to guide decoding. Meanwhile, We also find that the"
    ]
  },
  {
    "title": "Fully Automated Fact Checking Using External Sources",
    "answer": "The data used to build the task-specific embeddings includes:\n\n1. The claim text\n2. Pieces of potentially-relevant text fragments from the Web (snippets and related sentences from Web pages)\n3. The source reliability information\n\nThese data are encoded using:\n\n1. Long Short-Term Memory networks (LSTMs)",
    "evidence": [
      "described above. All these vectors are concatenated and fully connected to a much more compact hidden layer that captures the task-specific embeddings. This layer is connected to a softmax output unit to classify the claim as true or false. The bottom of",
      "in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this time we use the bi-LSTM embeddings as an input to the SVM. Ultimately, this yields a combination of deep learning and task-specific embeddings with RBF kernels.",
      "We also use a task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN.",
      "to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this time we use the bi-LSTM embeddings as",
      "the words in the text, and also (ii) using LSTM encodings, which we train for the task as part of a deep neural network (NN). We also use a task-specific embedding of the claim together with all the above evidence about it, which comes from the last",
      "snippets and of related sentences from the Web pages, which we automatically train for the task using Long Short-Term Memory networks (LSTMs). We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together",
      "performance of the SVM with and without task-specific embeddings from the NN, when training on Web pages vs. snippets, returned by Google vs. Bing vs. both. The NN embeddings consistently help the SVM in all cases. Moreover, while the baseline SVM using",
      "networks (LSTMs). We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence. We feed all these representations as features, together with pairwise similarities, into a Support Vector",
      "Our framework uses a deep neural network with LSTM text encoding to combine semantic kernels with task-specific embeddings that encode a claim together with pieces of potentially-relevant text fragments from the Web, taking the source reliability into",
      "this time are calculated by averaging rather than using a bi-LSTM. Finally, we combine the SVM with the NN by augmenting the input to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example,"
    ]
  },
  {
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "answer": "According to the text, the framework outperforms state-of-the-art methods by:\n\n* 5.0%, 1.6%, 1.4%, and 1.3% absolute gains on four benchmark datasets ( INLINEFORM0, INLINEFORM1, INLINEFORM2, and INLINEFORM3 respectively) compared to",
    "evidence": [
      "and tagging schema constraints to upgrade the aspect prediction. Experimental results over four benchmark datasets clearly demonstrate that our framework can outperform all state-of-the-art methods.",
      "(CRFs) and Maximum Entropy (ME), to detect aspects. Besides heavy feature engineering, they also ignored the consideration of opinions. Recently, neural network based models, such as LSTM-based BIBREF6 and CNN-based BIBREF31 methods, become the mainstream",
      "with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively. Our framework can outperform RNCRF, a state-of-the-art model based on",
      "i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.",
      ", INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively. Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and",
      "to transform the initial opinion representations and apply attention over the transformed representations to generate the opinion summary. Experimental results show that our framework can outperform state-of-the-art methods.",
      "Recently, neural network based models, such as LSTM-based BIBREF6 and CNN-based BIBREF31 methods, become the mainstream approach. Later on, some neural models jointly extracting aspect and opinion were proposed. BIBREF8 performs the two task in a single",
      "embeddings achieves much better results than CRF-1 on all datasets. WDEmb, which is also an enhanced CRF-based method using additional dependency context embeddings, obtains superior performances than CRF-2. Therefore, the above comparison shows that word",
      "rely on dependency parsing, instead, they employ attention mechanism to distill opinion information to help aspect extraction. Our framework consistently performs better than them. The gains presumably come from two perspectives: (1) In our model, the",
      "it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention,"
    ]
  },
  {
    "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
    "answer": "The work is evaluated over the MS-COCO dataset.",
    "evidence": [
      "Conclusion and Future Work",
      "Evaluation",
      "Related Work",
      "Proposed Method",
      "Attention Mechanism at Work",
      "augment the state-of-the-art sentence representations with those produced by our model and conduct a series of experiments on transfer tasks to test the quality of sentence representations. Through detailed analysis, we confirm our hypothesis that",
      "In this paper, we proposed a novel encoder that exploits self-attention mechanism. We trained the model using MS-COCO dataset and evaluated sentence representations produced by our model (combined with universal sentence representations) on several",
      "representations. As future work, we intend to explore cross-modal attention mechanism to further intertwine language and visual information for the purpose of improving sentence representation quality.",
      "on transfer tasks to test the quality of sentence representations. Through detailed analysis, we confirm our hypothesis that self-attention help our model produce more feature-rich visually grounded sentence representations.",
      "examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images (Figure FIGREF13 ). For example, given the sentence “man in black shirt is playing guitar”, our model identifies words that have association with"
    ]
  },
  {
    "title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction",
    "answer": "According to the text, the variables in the ablation study were:\n\n1. The presence of NER-specific BiRNN layers (either 0 or more layers)\n2. The presence of RE-specific BiRNN layers (either 0 or more layers)\n\nIn other words, the study investigated the effect of removing or",
    "evidence": [
      "Experiments ::: Ablation Study",
      "an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers,",
      "consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table TABREF26 contains the results from the ablation study. These results show",
      "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same",
      "trials with random weight initializations. Table TABREF26 contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much",
      "parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and",
      "achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers",
      "importance of the NER and RE tasks during training. Final training for both datasets used a value of 5 for $\\lambda ^r$. For the ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov",
      "ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of $5\\times 10^{-4}$, During training,",
      "optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of $5\\times 10^{-4}$, During training, dropout was applied before each BiRNN layer, other than the character BiRNN layer, and before the RE scoring layer."
    ]
  },
  {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "answer": "The evaluation datasets are two standard datasets for:\n\n1. Named Entity Recognition (NER)\n2. Chunking \n\nSpecifically, one of the datasets mentioned is CoNLL2003. Additionally, the text also mentions that the datasets are similar in domain to the 1 Billion Word Benchmark, which is derived from news articles",
    "evidence": [
      "We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data",
      "for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve",
      "the additional parameters in TagLM are slightly hurting performance. One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the",
      "from scientific publications across three diverse fields (computer science, material sciences, and physics) and defines three broad entity types (Task, Material and Process). For this task, TagLM increased INLINEFORM0 on the development set by 4.12% (from",
      "experiment. A priori, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small. To test this hypothesis, we replicated the setup from BIBREF3 that samples 1% of the CoNLL 2003 training set",
      "two popular datasets for NER and Chunking. Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different",
      "of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",
      "Units (GRU) BIBREF9 or Long Short-Term Memory units (LSTM) BIBREF10 depending on the task. Finally, the output of the final RNN layer INLINEFORM0 is used to predict a score for each possible tag using a single dense layer. Due to the dependencies between",
      "both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles. To test the sensitivity to the LM training domain, we also applied TagLM with a",
      "adding language models help because they learn composition functions (i.e., the RNN parameters in the language model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data. To"
    ]
  },
  {
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "answer": "The performance results are as follows:\n\n* The BERT-ADA Joint model performs especially strong when measured by the Macro-F1 metric.\n* The standard deviation in accuracy is about $1\\%$, which justifies averaging over 9 runs to measure differences in model performance reliably.\n* For the in-domain training case, the models",
    "evidence": [
      "Experiments ::: Results Analysis",
      "In our experiments we aim to answer the following research questions (RQs): RQ1: How does the number of training iterations in the BERT language model finetuning stage influence the ATSC end-task performance? At what point does performance start to",
      "performance if the domains do not confuse each other. Interesting for the joint-training case is that the BERT-ADA Joint model performs especially strong when measured by the Macro-F1 metric. A reason for this might be that the SemEval 2014 datasets are",
      "performance significantly. In addition, we find that different runs have a high variance, the standard deviation amounts to about $1\\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably. To answer RQ2,",
      "in the BERT language model finetuning stage influence the ATSC end-task performance? At what point does performance start to improve, when does it converge? RQ2: When trained in-domain, what ATSC end-task performance can be reached through fully exploitet",
      "$1\\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably. To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models",
      "model finetuning has a substantial effect on ATSC end-task performance. Secondly, we see that in the laptops domain the performance starts to increase at about 10 Mio. finetuned sentences. This is an interesting insight as one would expect a relation",
      "performs especially strong when measured by the Macro-F1 metric. A reason for this might be that the SemEval 2014 datasets are imbalanced due to dominance of positive label. It seems like through finetuning the language model on both domains the model",
      "evaluated on a test set that comes from the same distribution as the training set. In real-world applications this setting is not always valid, as the trained model is used to predict previously unseen data. In order to evaluate the performance of a",
      "by BIBREF9. As an extension to their paper we investigate the limits of language model finetuning in terms of how end-task performance is dependent on the amount of training steps. The training input representation for language model finetuning consists"
    ]
  },
  {
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "answer": "The existing models that NumNet was compared with include:\n\n1. Semantic parsing-based models:\n\t* Syn Dep \n\t* KDG (neural semantic parsing model with Stanford dependencies based sentence representations)\n\t* OpenIE\n2. Traditional MRC models: \n\t* (not specified)\n3. Numerical MRC",
    "evidence": [
      "our NumNet model can make full use of the numerical comparison information over numbers in both question and passage via the proposed NumGNN module. (2) Our implemented NAQANet+ has a much better performance compared to the original version of NAQANet. It",
      "to semantic parsing-based models, traditional MRC models and even numerical MRC models NAQANet and NAQANet+. The reason is that our NumNet model can make full use of the numerical comparison information over numbers in both question and passage via the",
      "which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing",
      "Methodology ::: Framework ::: Comparison with NAQANet",
      "samples, $9,536$ development samples and $9,622$ testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following BIBREF6. The numerically-focused F1 is set to",
      "To tackle this problem, we introduce a novel model NumNet that integrates numerical reasoning into existing MRC models. A key problem to answer questions requiring numerical reasoning is how to perform numerical comparison in MRC systems, which is crucial",
      "For comparison, we select several public models as baselines including semantic parsing models: [topsep=2pt, itemsep=0pt] Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations; OpenIE",
      "are not adopted respectively. As shown in Table TABREF51, our proposed NumGNN leads to statistically significant improvements compared to traditional GNN on both EM and F1 scores especially for comparing questions. It indicates that considering the",
      "compared to traditional GNN on both EM and F1 scores especially for comparing questions. It indicates that considering the comparing information over numbers could effectively help the numerical reasoning for comparing questions. Moreover, we find that",
      "MRC models: [topsep=2pt, itemsep=0pt] NAQANet BIBREF6, a numerical version of QANet model. NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data"
    ]
  },
  {
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "answer": "Based on the provided text, it seems that the languages being considered are not explicitly stated, but there are some hints:\n\n* Japanese text is mentioned as a high-resource language.\n* Uganda is mentioned as an example, which suggests that languages spoken in Uganda might be considered. \n* The primary source of news in Uganda and",
    "evidence": [
      "languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision",
      "non-exact but relevant keyword matches are retrieved BIBREF20, BIBREF21, BIBREF22. In all these studies, the query and search languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword",
      "BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic classification for speech has",
      "amount of supervision goes a long way. A slightly different approach to quickly analysing speech is the established task of Keyword spotting BIBREF16, BIBREF17, which simply asks whether any of a specific set of keywords appears in each segment. Recent",
      "of individuals, and to populate a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world,",
      "i.e., we ask whether the topic inferred from our predicted translation (ST) agrees with one inferred from a gold translation (human).",
      "topics just from the 20 hours of translations, but in future work, we could use a larger text corpus in the high-resource language to learn a more general topic model covering a wider set of topics, and/or combine it with keyword lists curated for",
      "of the speech segments. Cross-lingual topic modeling may be useful when the target language is high-resource. Here, we learned target topics just from the 20 hours of translations, but in future work, we could use a larger text corpus in the high-resource",
      "it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world, the primary source of news is local radio stations, which broadcast in many languages. A pilot study from the United Nations",
      "spotting BIBREF16, BIBREF17, which simply asks whether any of a specific set of keywords appears in each segment. Recent studies have extended the early work to end-to-end keyword spotting BIBREF18, BIBREF19 and to semantic keyword retrieval, where"
    ]
  },
  {
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "answer": "According to the text, the following information is mentioned:\n\n* Domain-Specific Risk-Taking (DOSPERT) is mentioned as a popular and well-accepted clinically validated PTSD assessment tool.\n* Dryhootch surveys are also mentioned as a clinically validated survey tool.\n\nAdditionally, the text mentions that the authors investigate \"clinically",
    "evidence": [
      "provide trust of clinicians due to its lack of interpretability in clinical terms. In this paper, we develop LAXARY model where first we start investigating clinically validated survey tools which are trustworthy methods of PTSD assessment among",
      "where first we start investigating clinically validated survey tools which are trustworthy methods of PTSD assessment among clinicians, build our category sets based on the survey questions and use these as dictionary words in terms of first person",
      "Demographics of Clinically Validated PTSD Assessment Tools",
      "There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT)",
      "fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey",
      "in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment. Finally, we evaluate the accuracy of LAXARY model performance and reliability-validity of generated PTSD",
      "Dryhootch surveys. We use the output to fill up each survey, estimate the prevalence of PTSD and its intensity based on each tool's respective evaluation metric.",
      "due to the lack of their explainability. In the context of the above research problem, we aim to answer the following research questions Given clinicians have trust on clinically validated PTSD assessment surveys, can we fill out PTSD assessment surveys",
      "questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample",
      "findings about the representation of a blackbox model into clinically trusted tools, using only 210 users' data is not enough to come up with a trustworthy model. Moreover, more clinical validation must be done in future with real clinicians to firmly"
    ]
  },
  {
    "title": "Reference-less Quality Estimation of Text Simplification Systems",
    "answer": "According to the text, the following approaches are compared:\n\n1. Elementary metrics, including:\n\t* Commonly used metrics such as BLEU\n\t* Basic metrics based on a single low-level feature such as sentence length\n\t* MT metrics to compare the output and a reference\n\t* Comparison methods using pre-trained",
    "evidence": [
      "Comparing elementary metrics",
      "Using MT metrics to compare the output and a reference",
      ", comparison methods using pre-trained fastText word embeddings BIBREF35 or Skip-thought sentence embeddings BIBREF36 . TABLE TABREF30 lists 30 of the elementary metrics that we compared, which are those that we found to correlate the most with human",
      "and meaning preservation are best evaluated using INLINEFORM1 -gram-based comparison metrics might bias the TS models towards copying the source sentence and applying fewer modifications. Syntactic parsing or language modelling might capture more",
      "Methodology",
      "Our goal is to compare a large number of ways to perform TS evaluation without a reference. To this end, we use the dataset provided in the QATS shared task. We first compare the behaviour of elementary metrics, which range from commonly used metrics such",
      "these elementary metrics into more complex ones and compare our results with the state of the art, based on the QATS shared task data and results.",
      "be made between the simplified text and its original version. In this paper, we compare multiple approaches to reference-less quality estimation of sentence-level text simplification systems, based on the dataset used for the QATS 2016 shared task. We",
      "in the QATS shared task. We first compare the behaviour of elementary metrics, which range from commonly used metrics such as BLEU to basic metrics based on a single low-level feature such as sentence length. We then compare the effect of aggregating",
      "percentage of words in common between source and output also rank high. Surprisingly, word embedding comparison methods do not perform as well for meaning preservation, even when using word alignment. Methods that give the best results are the most"
    ]
  },
  {
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "answer": "The text doesn't explicitly mention the three Twitter sentiment classification datasets used for experiments. However, it does mention the following datasets:\n\n1. Stanford Twitter Sentiment Corpus\n2. Sanders - Twitter Sentiment Corpus\n\nIt is likely that the third dataset is not explicitly mentioned, but based on the context, it could be one of",
    "evidence": [
      "Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.",
      "for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 . In addition, hybrid approaches combining lexicon-based and",
      "Twitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features",
      "has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct",
      "(Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of",
      ". BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset",
      "For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary",
      "presented in tweets and the classifiers only use these non-emoticon features to predict the sentiment of tweets. However, there is a problem is that if the test set contains emoticons, they do not influence the classifiers because emoticon features do not",
      "analysis BIBREF5 BIBREF6 BIBREF7 . Some researchers used Convolutional Neural Network (CNN) for sentiment classification. CNN models have been shown to be effective for NLP. For example, BIBREF6 proposed various kinds of CNN to learn sentiment-bearing",
      "are extracted during training progress. The use of deep learning methods becomes to achieve remarkable results for sentiment analysis BIBREF5 BIBREF6 BIBREF7 . Some researchers used Convolutional Neural Network (CNN) for sentiment classification. CNN"
    ]
  },
  {
    "title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
    "answer": "Based on the text, the evaluation metrics used are:\n\n1. **Accuracy**: reported as average accuracy over single-language models (Avg) and accuracy obtained when training on the concatenation of all languages but the target one.\n2. **Fleiss kappa ( INLINEFORM0 )**: used to measure inter-annotator agreement",
    "evidence": [
      "Human Evaluation",
      "performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual embeddings model we use the mean embedding representation from the system of BIBREF27 and add max, std and coverage features.",
      "We would like to thank the three anonymous reviewers and our colleagues for their useful feedback on earlier versions of this paper. Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part. We would like to thank all",
      "source language data and test on all target language data. For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual",
      "the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users. Systems were tested under two different conditions regarding the number of tweets per user for the target",
      "six judges, balanced for gender, and obtained three annotations per target user. Inter-annotator agreement for the tasks was measured via Fleiss kappa ( INLINEFORM0 ), and was higher for the in-language experiment ( INLINEFORM1 ) than for the",
      "finding that INLINEFORM1 performs best. When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one",
      "to be the most robust across languages. We tuned the INLINEFORM0 -gram size of this model through in-language cross-validation, finding that INLINEFORM1 performs best. When testing across languages, we report accuracy for two setups: average accuracy over",
      "also be observed in language-to-language experiments, where scores for ES INLINEFORM0 PT and PT INLINEFORM1 ES are the highest. Results for the lexical models are generally lower on English, which might be due to smaller amounts of data (see first column",
      "(Section SECREF14 ). First of all, our results indicate that in-language performance of humans is 70.5%, which is quite in line with the findings of BIBREF6 , who report an accuracy of 75% on English. Within language, lexicalized models are superior to"
    ]
  },
  {
    "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
    "answer": "According to the text, they try to predict a wide range of **62 demographic attributes**.",
    "evidence": [
      "works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter",
      "the prediction results for a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we display two terms with the highest coefficient common between the majority of the folds. Attributes are divided into categories",
      "We investigate how well the demographic attributes can be predicted by using using Yahoo! Ansewrs and Twitter data. We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their",
      "a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter are semantically related to these attributes and provide examples of sociocultural profiles of neighbourhoods through the",
      "Answers sometimes match the name of the attribute (i.e. “asian” and “Jewish”). In the Appendix, in Tables 6 and 7 , we show the prediction results for a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we",
      "neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas BIBREF4 . In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in",
      "real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes",
      "Yahoo! Answers and Twitter, which can be used to predict the population demographics. In this section, we observe whether the correlations between terms and attributes are semantically meaningful. Due to the limited space, we select three attributes and",
      "attributes of the population of neighbourhoods can be predicted using both Yahoo! Answers and Twitter with high performances of $0.54$ and $0.53$ respectively. While Yahoo! Answers outperforms Twitter in predicting attributes related to Ethnicity and",
      "work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include"
    ]
  },
  {
    "title": "Bias in Semantic and Discourse Interpretation",
    "answer": "According to the text, the paper analyzes and provides a model of interpretive bias in general, but it does not specifically mention particular types of interpretative biases that are analyzed. However, it mentions that the framework developed provides tools for understanding and analyzing the range of interpretive biases and the factors that contribute to them.\n\nIt",
    "evidence": [
      "The model of interpretive bias",
      "Some examples of bias",
      "Our paper is organized as follows. Section SECREF2 introduces our model of interpretive bias. Section SECREF3 looks forward towards some consequences of our model for learning and interpretation. We then draw some conclusions in Section SECREF4 . A",
      "oriented bias for a set of facts, and if so, under what conditions. Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that",
      "can be used to pervert or subvert the truth. The framework we develop here provides tools for understanding and analyzing the range of interpretive biases and the factors that contribute to them.",
      "towards some consequences of our model for learning and interpretation. We then draw some conclusions in Section SECREF4 . A detailed and formal analysis of interpretive bias has important social implications. Questions of bias are not only timely but",
      "will argue that it is the biases of the two Juries that create these different interpretations. And these biases are revealed at least implicitly in how they interpret the story: Jury 1 is at the outset at least guarded, if not skeptical, in its appraisal",
      "As mentioned in Section SECREF1 , understanding interpretive bias requires two ingredients. First, we need to know what it is to interpret a text or to build a history over a set of facts. Our answer comes from analyzing discourse structure and",
      "accurately extracting sentiments and opinions from text BIBREF15 , BIBREF16 , BIBREF17 , and our examples show that this is the case for interpretive bias as well.",
      "In this paper, we have put forward the foundations of a formal model of interpretive bias. Our approach differs from philosophical and AI work on dialogue that links dialogue understanding to the recovery of speaker intentions and beliefs BIBREF56 ,"
    ]
  },
  {
    "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
    "answer": "They use an unsupervised word segmentation method called \"latticelm\" that can directly segment words from the lattices of the speech recognition results of the uttered sentences.",
    "evidence": [
      "recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable",
      "the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences",
      "We compared the performance of three types of word segmentation methods for all the considered uttered sentences. It was difficult to weigh the ambiguous syllable recognition and the unsupervised word segmentation separately. Therefore, this experiment",
      "of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more",
      "methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple",
      "segmentation is performed by using the lattices of the speech recognition results. The robot learns spatial concepts from words obtained by word segmentation and robot positions obtained by self-localization for all teaching times. The details of the",
      "on the robot is SHURE's PG27-USB. Further, an unsupervised morphological analyzer, a latticelm 0.4, is implemented BIBREF22 . In the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the",
      "The details of self-localization are provided in SECREF35 . The proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices",
      "segment a sentence into individual words, e.g., “this”, “iz”, “an”, and “aporu”. In addition, it is necessary for the robot to recognize words referring to the same referent, e.g., the fruit apple, from among the many segmented results that contain",
      "location information, irrespective of the word segmentation method used. The experimental results showed that it is possible to improve the estimation accuracy of spatial concepts and vocabulary by performing word clustering that considered location"
    ]
  },
  {
    "title": "Localization of Fake News Detection via Multitask Transfer Learning",
    "answer": "The size of the dataset is 3,206 news articles, with a perfect 50/50 split between real and fake articles, specifically 1,603 real and 1,603 fake articles.",
    "evidence": [
      "We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. Fake articles were sourced from online sites that were tagged as fake news sites by the",
      "train models from scratch. Both BIBREF1 and BIBREF3 use the Fake News Challenge dataset, with 49,972 labeled stances for each headline-body pairs. BIBREF2, on the other hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as",
      "dataset. We mainly focus on three domains: political news, opinion articles, and entertainment/gossip articles. Articles used for testing are sourced from the same websites that the training dataset was taken from.",
      "a batch size of 32, and a learning rate of 3e-5. For ULMFiT, we perform language model finetuning on the fake news dataset (appending no extra classification heads yet) for a total of 10 epochs, using a learning rate of 1e-2, a batch size of 80, and",
      "news dataset, with the corpus only being lightly preprocessed and tokenized using Byte-Pair Encoding. Corpus statistics for the pretraining corpora are shown on table TABREF17.",
      "in each hidden layer) and 12 attention heads for a total of 110M parameters. Its larger sibling, BERT-Large, has 24 layers (1024 units in each hidden layer) and 16 attention heads for a total of 340M parameters.",
      "a batch size of 128, and a weight decay of 0.1. We use the Adam optimizer and use slanted triangular learning rate schedules BIBREF5. We train the model on a machine with one NVIDIA Tesla V100 GPU for a total of 11 hours. For each pretraining scheme, we",
      "pairs. BIBREF2, on the other hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels. This requirement for large datasets to effectively train fake news detection models from scratch",
      "Experimental Setup ::: Fake News Dataset",
      "and a batch size of 256. We train the model for 1,000,000 steps with 10,000 steps of learning rate warmup for 157 hours on a Google Cloud Tensor processing Unit (TPU) v3-8. For GPT-2, we pretrain a GPT-2 Transformer model on our prepared text corpora"
    ]
  },
  {
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "answer": "The experimental results that suggest using less than 50% of the available training examples might result in overfitting are:\n\n1. **Consistent increase in validation loss**: After about 15 epochs, the validation loss consistently increases when using less than 50% of the available training data, as indicated by the dashed lines in",
    "evidence": [
      "and gold-standard data. Results show that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a",
      "and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a",
      "overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data",
      "the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to",
      "is a preliminary study of trying to find parameters for training word embeddings from Twitter and adequate evaluation tests and gold-standard data. Results show that using less than 50% of the available training examples for each vocabulary size might",
      "the impact of using different amounts of training data, for each size of vocabulary to be embedded INLINEFORM0 we will run experiments training the models using 25%, 50%, 75% and 100% of the data available.",
      "we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better",
      "Despite already providing interesting practical clues for our goal of trying to embed a larger vocabulary using more of the training data we have available, these results also revealed that the intrinsic evaluation metrics we are using are overly",
      "result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relatively small size. Nevertheless, results exhibit",
      "of the fraction of training data used (25%, 50%, 75% and 100%). For all possible configurations, we recorded the values of the training and validation loss (cross entropy) after each epoch. Tracking these metrics serves as a minimalistic sanity check: if"
    ]
  }
]